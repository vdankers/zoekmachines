{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment  week 5 MRS Chap 21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook made by   (If not filled in correctly: 0 pts for assignment)\n",
    "\n",
    "__Name(s)__: Adriaan de Vries, Verna Dankers\n",
    "\n",
    "__Student id(s)__ : 10795227, 10761225\n",
    "\n",
    "### Pledge (taken from [Coursera's Honor Code](https://www.coursera.org/about/terms/honorcode) )\n",
    "\n",
    "\n",
    "\n",
    "Put here a selfie with your photo where you hold a signed paper with the following text: (if this is team work, put two selfies here). The link must be to some place on the web, not to a local file. **Assignments without the selfies will not be graded and receive 0 points.**\n",
    "\n",
    "> My answers to homework, quizzes and exams will be my own work (except for assignments that explicitly permit collaboration).\n",
    "\n",
    ">I will not make solutions to homework, quizzes or exams available to anyone else. This includes both solutions written by me, as well as any official solutions provided by the course staff.\n",
    "\n",
    ">I will not engage in any other activities that will dishonestly improve my results or dishonestly improve/hurt the results of others.\n",
    "\n",
    "<img width=30% src='http://oi63.tinypic.com/2q3srh3.jpg'/>\n",
    "<img width=30% src=\"http://i63.tinypic.com/2itj1ic.jpg\" border=\"0\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Link Analysis\n",
    "\n",
    "\n",
    "\n",
    "#### Q1 Hits\n",
    "1. Create the network consisting of the page <http://en.wikipedia.org/wiki/PageRank>, all pages that link to that page (you get those via <http://en.wikipedia.org/wiki/Special:WhatLinksHere/PageRank>), and all outgoing links from <http://en.wikipedia.org/wiki/PageRank>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from bs4 import BeautifulSoup\n",
    "import matplotlib.pyplot as plt\n",
    "START_NODE = 'PAGERANKSTARTNODE' \n",
    "\n",
    "G = nx.Graph()\n",
    "G.add_node(START_NODE)\n",
    "\n",
    "f = open('.\\Pagerank\\outlinks.htm', 'rb')\n",
    "text = f.read().decode('utf8', 'ignore')\n",
    "soup = BeautifulSoup(text)\n",
    "for elem in soup.find_all('li'):\n",
    "    try:\n",
    "        link = elem.a.get('href')\n",
    "        G.add_node(link)\n",
    "        G.add_edge(link, START_NODE)\n",
    "    except Exception: \n",
    "        pass\n",
    "nx.draw(G)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. ~~Compute the HITS score for each node in this network.~~\n",
    "3.   Use the `networkx` package to display the network nicely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2 Pagerank and hits\n",
    "\n",
    "* Do questions Q5,Q6,Q9,Q10 from <http://nbviewer.ipython.org/url/maartenmarx.nl/teaching/zoekmachines/Assignments/ASSIGNMENTS/Week7InformationRetrieval.ipynb>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make inverted index\n",
    "\n",
    "[MakeInvertedIndex](MakeInvertedIndex1516.html): 11  \n",
    "\n",
    "#### 1 Choose three zones and indicate why these are good and interesting for this data set. Also give weights for each of them. \n",
    "\n",
    "- <b>trefwoordlod</b>: this is a good zone to look at, since it indicates the keywords from the file. Users often put such keywords in their queries. If they query terms are inside of these zone, this file is definitely a hit! <b>Weight: 0.45</b>\n",
    "\n",
    "- <b>citeertitel</b>: this is a good zone to look at, since it summarizes the content of the file in one sentence. If this zone contains a query term, this file is very likely to be a hit! <b>Weight: 0.3</b>\n",
    "\n",
    "- <b>tekstxml</b>: this is a good zone to look at because this is the body of the file that contains the actual text of the motion. If this zone does not contain a query term or a synonym of a query term, the file is not relevant. <b>Weight: 0.25</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 You want to use TF-IDF weighting for the scoring function of each zone. Think whether you must adjust IDF as well for each zone, or you can just use IDF as before? Argue why your answer is good, possibly using an instructive example.\n",
    "\n",
    "The original defitions:\n",
    "\n",
    "$tfidf_{t,d} = tf_{t,d} \\cdot idf_{t}$\n",
    "\n",
    "$idf_{t} = \\log(\\frac{N}{df_{t}})$ where $N$ is the number of documents in the collection.\n",
    "\n",
    "When using $tfidf$-weighting for the scoring function of each zone, the $idf$ formula must be adjusted. Why? Let's consider an example. \n",
    "Suppose the word ASIELZOEKERSCENTRA only appears in one 'trefwoordlod' in your entire corpus. When your query term contains ASIELZOEKERSCENTRA, that file is a hit! The fact that the word is in the 'trefwoordlod' zone is very important, since that zone is an important zone, since it contains the keywords from the file. But if ASIELZOEKERSCENTRA appears in many documents inside the 'tekstxml' zone, the idf weight for ASIELZOEKERSCENTRA in the 'trefwoordlod' zone will still be very very low. We do not want that, because the fact that ASIELZOEKERSCENTRA is in the 'trefwoordlod' zone still really distinguishes that file from all of the other files, when looking at the 'trefwoordlod' zone. So we want the idf weight for ASIELZOEKERSCENTRA in the 'trefwoordlod' to be high. Therefore it is a better idea to consider only the 'trefwoordlod' zone frequency when calculating that idf weight for that zone. So we would adapt the $idf$ formula:\n",
    "\n",
    "$idf_{t,z} = \\log(\\frac{N}{df_{t,z}})$ where $z$ indicates which zone to look at. So we do not use the regular document frequency, but the frequency of term t in all zones $z$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 3 Design an index for your 3 zones and an algorithm which computes score(q,d) based on the linear weighted scoring function from MRS 6.1. Implement it on top of your earlier code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from earlier_code import normalize\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from collections import Counter, defaultdict\n",
    "import os\n",
    "\n",
    "def zone_index(folder, zones, keep_capitals):\n",
    "    \"\"\"\n",
    "    Create an inverted index given a folder \n",
    "    containig a text corpus.\n",
    "    \"\"\"\n",
    "    index = defaultdict() # initialize MyIndex\n",
    "\n",
    "    # Prepare each file one by one\n",
    "    for filename in os.listdir(folder):\n",
    "        xmlfile = BeautifulSoup(open(os.path.join(folder, filename), 'rb'),\"xml\")\n",
    "        \n",
    "        for zone in zones:\n",
    "            zone_text = xmlfile.find(zone).get_text()\n",
    "\n",
    "            # Split the text on whitespaces\n",
    "            zone_text = WhitespaceTokenizer().tokenize(zone_text)\n",
    "\n",
    "            # Normalize text for capital letters, non-alphabetical tokens and whitespace\n",
    "            # normalize() is from earlier code, and can be found in attached .py file\n",
    "            text = [ normalize(word, keep_capitals)\n",
    "                     for word \n",
    "                     in zone_text\n",
    "                    ]\n",
    "            \n",
    "            # Update the index with each token\n",
    "            for w in text:    \n",
    "                if not w in index:\n",
    "                    index[w] = defaultdict(Counter)\n",
    "                index[w][filename[:-4]][zone]+=1 \n",
    "    \n",
    "    if '' in index.keys():\n",
    "        del index['']\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CORPUS = \"Moties/XML/MOT\"\n",
    "\n",
    "index = zone_index(CORPUS,['trefwoordlod','tekstxml','citeertitel'], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(collections.Counter,\n",
       "            {'1802274': Counter({'tekstxml': 1}),\n",
       "             '1807811': Counter({'tekstxml': 1}),\n",
       "             '1807918': Counter({'citeertitel': 1, 'tekstxml': 2}),\n",
       "             '1808035': Counter({'tekstxml': 2}),\n",
       "             '1949530': Counter({'tekstxml': 1}),\n",
       "             '1950824': Counter({'citeertitel': 1, 'tekstxml': 1}),\n",
       "             '1950890': Counter({'tekstxml': 2}),\n",
       "             '1950926': Counter({'citeertitel': 1, 'tekstxml': 1}),\n",
       "             '1950968': Counter({'tekstxml': 1}),\n",
       "             '1950979': Counter({'tekstxml': 2}),\n",
       "             '1955160': Counter({'tekstxml': 1}),\n",
       "             '1955666': Counter({'tekstxml': 1}),\n",
       "             '1956968': Counter({'tekstxml': 1}),\n",
       "             '1957578': Counter({'citeertitel': 1, 'tekstxml': 3}),\n",
       "             '1965629': Counter({'tekstxml': 1}),\n",
       "             '1970639': Counter({'tekstxml': 1}),\n",
       "             '1977588': Counter({'tekstxml': 3}),\n",
       "             '1978922': Counter({'tekstxml': 1}),\n",
       "             '1987335': Counter({'citeertitel': 1, 'tekstxml': 1}),\n",
       "             '2007905': Counter({'tekstxml': 1}),\n",
       "             '2016817': Counter({'tekstxml': 3, 'trefwoordlod': 1}),\n",
       "             '2016830': Counter({'trefwoordlod': 1}),\n",
       "             '2017464': Counter({'tekstxml': 1, 'trefwoordlod': 1}),\n",
       "             '2017478': Counter({'tekstxml': 3, 'trefwoordlod': 1}),\n",
       "             '2017490': Counter({'tekstxml': 1, 'trefwoordlod': 1}),\n",
       "             '2017507': Counter({'tekstxml': 2, 'trefwoordlod': 1}),\n",
       "             '2018556': Counter({'tekstxml': 3}),\n",
       "             '2020951': Counter({'trefwoordlod': 1}),\n",
       "             '2021008': Counter({'citeertitel': 1,\n",
       "                      'tekstxml': 4,\n",
       "                      'trefwoordlod': 1}),\n",
       "             '2027454': Counter({'tekstxml': 1}),\n",
       "             '2039379': Counter({'citeertitel': 1, 'tekstxml': 2}),\n",
       "             '2041634': Counter({'tekstxml': 2, 'trefwoordlod': 1}),\n",
       "             '2043794': Counter({'tekstxml': 1}),\n",
       "             '2043807': Counter({'tekstxml': 2}),\n",
       "             '2043847': Counter({'tekstxml': 1}),\n",
       "             '2044294': Counter({'tekstxml': 1}),\n",
       "             '2049338': Counter({'tekstxml': 2}),\n",
       "             '2049392': Counter({'tekstxml': 1, 'trefwoordlod': 1}),\n",
       "             '5579982': Counter({'citeertitel': 1,\n",
       "                      'tekstxml': 2,\n",
       "                      'trefwoordlod': 1}),\n",
       "             '5580117': Counter({'trefwoordlod': 1}),\n",
       "             '6001962': Counter({'citeertitel': 1,\n",
       "                      'tekstxml': 3,\n",
       "                      'trefwoordlod': 1}),\n",
       "             '6001967': Counter({'citeertitel': 1,\n",
       "                      'tekstxml': 4,\n",
       "                      'trefwoordlod': 1}),\n",
       "             '6001971': Counter({'citeertitel': 1,\n",
       "                      'tekstxml': 5,\n",
       "                      'trefwoordlod': 1}),\n",
       "             '6001979': Counter({'citeertitel': 1,\n",
       "                      'tekstxml': 3,\n",
       "                      'trefwoordlod': 1})})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index['palestijnse']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MySQL assignment\n",
    "\n",
    "\n",
    "### Mysql\n",
    "- Go to http://www.apachefriends.org/en/xampp.html, choose your OS, downloand XAMPP, and follow the instructions.\n",
    "- Or work from inside your IPython notebook.\n",
    "    \n",
    "### Get familiar with full text search in MYSQL.\n",
    "- <a href='http://devzone.zend.com/26/using-mysql-full-text-searching/'>step by step tutorial</a>\n",
    "- <a href='http://dev.mysql.com/doc/refman/5.0/en/fulltext-search.html'>The MySQL manual on Full Text searching.</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try it yourself\n",
    "\n",
    "- Open PHPMyadmin. Go to the mysql database. Choose the table 'help_topic' from the menu on the left. Now you see the contents of this table. Open the SQL tab and ask a query: e.g. give me all topics about 'minimum'\n",
    "\n",
    "- Ask this query using the familiar <em>where description LIKE '%minimum%'</em> construction. Notice the time.\n",
    "\n",
    "> SELECT * FROM `help_topic` WHERE description LIKE '%minimum%', Query took 0.0032 seconds \n",
    "\n",
    "- Now ask it using <em>MATCH description AGAINST ('mimimum')</em>, as described in the tutorial.\n",
    "\n",
    "- Now I got an error. It said that there was no FULLTEXT index on the description field.\n",
    "\n",
    "- Thus add it. In PHPMyadmin, choose the structure tab. Here you can do many things to your table. Make sure that the type of the description field is of TEXT. Now add  a FULLTEXT index to the description field using the last button on the right (with the T in it). \n",
    "\n",
    "- Now ask your MATCH query again. Note the time.\n",
    "\n",
    "> SELECT * FROM `help_topic` WHERE MATCH description AGAINST('minimum'), Query took 0.0009 seconds\n",
    "\n",
    "- Now ask for all helptopics with \"the\" in the description. First old-fashioned with LIKE. Then modern with MATCH. What's going on? The MySQL manual on full text search explains it (and the comments complain about it).\n",
    "\n",
    "> The difference in run time is pretty small. According to the comments at the MySQL manual the full-text search is slow when there are a lot of rows in the table.\n",
    "\n",
    "> SELECT * FROM `help_topic` WHERE description LIKE '%the%', Query took 0.0004 seconds\n",
    "\n",
    "> SELECT * FROM `help_topic` WHERE MATCH description AGAINST('the'), Query took 0.0003 seconds\n",
    "\n",
    "### Play time: Your first search engine\n",
    "This is really a MySQL exercise. So you should be able to do this. Refresh your MySQL skills and get help if needed.\n",
    "- Make a   TABLE named MyWebCollection with fields <em>URL, TITLE, BODY</em>. Put a full text index on body.</li>\n",
    "- Download a number of webpages (a lot maybe). If you want to stay on familiar ground, download all files from <a href=\"http://www.cafeconleche.org/examples/shakespeare/\">http://www.cafeconleche.org/examples/shakespeare/</a>. For each page,   extract the URL and title, and the complete text of the page (just strip of all HTML). \n",
    "- Make a script which creates a text file with MYSQL insert statements that creates a row for each webpage in your table MyWebCollection.\n",
    "- Now import these rows in your table (you can do this in PHPMyadmin using the import tab).\n",
    "- Create full text search queries.\n",
    "- Create a simple web user interface using the PHP example code from the tutorial. For each hit, display the title with a link to the real webpage and the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import mysql.connector\n",
    "import urllib\n",
    "\n",
    "# Start connection to database\n",
    "db = mysql.connector.connect(host='localhost',database='mysql',user='root',password='')\n",
    "cursor = db.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create new table\n",
    "cursor.execute(\"CREATE TABLE MyWebCollection (url TEXT, title TEXT, body LONGTEXT)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cursor.execute(\"ALTER TABLE MyWebCollection ADD FULLTEXT(body)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/verna/anaconda2/lib/python2.7/site-packages/bs4/__init__.py:166: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "To get rid of this warning, change this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "SCRAPING_SITE = 'http://www.cafeconleche.org/examples/shakespeare/'\n",
    "\n",
    "def add_links(website):\n",
    "    \"\"\"\n",
    "    Insert all files from the given website into\n",
    "    the database.\n",
    "    \"\"\"\n",
    "    text_website = urllib.urlopen(website).read()\n",
    "    soup_website = BeautifulSoup(text_website, \"lxml\")\n",
    "    \n",
    "    # Add all files to the database\n",
    "    for elem in soup_website.find_all('li'):\n",
    "        link = elem.a.get('href')\n",
    "        text_file = urllib.urlopen(website + link).read()\n",
    "        soup_file = BeautifulSoup(text_file, \"xml\")\n",
    "        \n",
    "        # Values for the three fields\n",
    "        url = str(website + link)\n",
    "        title = str(soup_file.TITLE.string)\n",
    "        body = str(soup_file.text)\n",
    "        \n",
    "        # Insert into database\n",
    "        cursor.execute(\"INSERT INTO MyWebCollection(url, title, body) \\\n",
    "                       VALUES(%s,%s,%s)\",(url,title,body))\n",
    "        db.commit()\n",
    "\n",
    "add_links(SCRAPING_SITE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'The Tragedy of Othello, the Moor of Venice',)]\n",
      "[(u'The Tragedy of Romeo and Juliet',), (u'The Tragedy of Othello, the Moor of Venice',), (u\"A Midsummer Night's Dream\",), (u'The Tragedy of Hamlet, Prince of Denmark',), (u'The Tragedy of Julius Caesar',), (u'The Tragedy of King Lear',), (u'The Tragedy of Macbeth',), (u'Much Ado about Nothing',), (u'The Merchant of Venice',), (u'The History of Troilus and Cressida',), (u'The Tragedy of Antony and Cleopatra',), (u'The Tragedy of Richard the Third',), (u'The First Part of Henry the Fourth',), (u'The Second Part of Henry the Fourth',), (u\"All's Well That Ends Well\",), (u'The Life of Henry the Fifth',), (u'The Life and Death of King John',), (u'The Second Part of Henry the Sixth',), (u'Cymbeline',), (u'The Merry Wives of Windsor',), (u'Pericles, Prince of Tyre',), (u'Measure for Measure',), (u'Twelfth Night, or What You Will',), (u'The Third Part of Henry the Sixth',), (u'The Two Gentlemen of Verona',), (u'The Taming of the Shrew',), (u'The Famous History of the Life of Henry the Eighth',), (u'The First Part of Henry the Sixth',), (u'The Tragedy of King Richard the Second',), (u'The Tempest',), (u\"Love's Labor's Lost\",), (u'The Comedy of Errors',), (u'The Tragedy of Titus Andronicus',), (u\"The Winter's Tale\",), (u'The Tragedy of Coriolanus',), (u'The Life of Timon of Athens',), (u'As You Like It',)]\n"
     ]
    }
   ],
   "source": [
    "cursor.execute(\"SELECT title FROM MyWebCollection WHERE MATCH body AGAINST('Othello')\")\n",
    "result = cursor.fetchall()\n",
    "print(result)\n",
    "\n",
    "cursor.execute(\"SELECT title FROM MyWebCollection WHERE MATCH body AGAINST('night')\")\n",
    "result = cursor.fetchall()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our tiny search engine for Shakespeare's work\n",
    "\n",
    "The tutorial contains an example of a php-form. Since we work in python, we used a python form to design the interface. This is wat the interface looks like:\n",
    "\n",
    "<img src='http://oi65.tinypic.com/2zgfb50.jpg' width = 50%/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL: http://www.cafeconleche.org/examples/shakespeare/othello.xml, Title: The Tragedy of Othello, the Moor of Venice, Score: 788.063720703\n"
     ]
    }
   ],
   "source": [
    "from formlayout import fedit\n",
    "import mysql.connector\n",
    "\n",
    "# Start connection to database\n",
    "db = mysql.connector.connect(host='localhost',database='mysql',user='root',password='')\n",
    "cursor = db.cursor()\n",
    "\n",
    "query = fedit([('Query','')], title=\"Shakespeare search engine\", \n",
    "      comment=\"Enter your query to search the collection of Shakespeare's work.\"\n",
    "     )\n",
    "query = str(query[0])\n",
    "\n",
    "cursor.execute(\"\"\"SELECT url,title , \\\n",
    "                    MATCH(body) AGAINST(%s) AS score \\\n",
    "                    FROM MyWebCollection \\\n",
    "                WHERE MATCH(body) AGAINST(%s) \\\n",
    "                ORDER BY score DESC;\"\"\",(query,query))\n",
    "result = cursor.fetchall()\n",
    "\n",
    "def display_result(result):\n",
    "    for line in result:\n",
    "        url, title, score = line\n",
    "        print \"URL: %s, Title: %s, Score: %s\" % (url, title, score)\n",
    "        \n",
    "display_result(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigger and better: More data and Faceted Search\n",
    "\n",
    "<p>The website <a href=\"https://zoek.officielebekendmakingen.nl/zoeken/parlementaire_documenten\">officielebekendmakingen.nl</a>lets you search in \"kamervragen\".\n",
    "You can play with the fields and see that fielded search is possible, but not really facetd search. </p>\n",
    "<p>In this exercise you will create a faceted search engine for kamervragen.</p>\n",
    "\n",
    "### Data\n",
    "<p>We preprocessed the data already for you  in two formats: a csv file and a collection of XML files.</p>\n",
    "<p>You can download <a href='http://data.politicalmashup.nl/kamervragen/PoliDocs_Kamervragen.zip'>this zipfile with Kamervragen in XML</a> to see the real data. It also contains style sheets to show the XML well in a browser.   </p>\n",
    "<p> <http://maartenmarx.nl/teaching/zoekmachines/LectureNotes/MySQL/> contains two gzipped csv files: a big one with 40K kamervragen, and a small one to get you going. See below to see the contents.</p> \n",
    " \n",
    "### Your task\n",
    "- Populate your database\n",
    "- Create a search engine, and create queries for calculating the values for a number of facets. Make facets for\n",
    "         - jaar\n",
    "         - partij\n",
    "         - Aantal deelvragen (dit zal je zelf nog moeten programmeren, of slimmer uit de XML halen).  (See the XQuery file KVR-collectie2CSV.xquery for hints how to do this. It is really easy in XPath. TRy it out on an example document.)</li>\n",
    "         - Haal het ministerie van de beantwoorder uit de XML, pas je data weer aan, en neem dat als facet op. \n",
    "         <br/>Kijk eens welke partij het ministerie van landbouw overspoelt met vragen...\n",
    "- If it is slow, try to precompute things (e.g. year)    , and possibly add good indexes. This thus involves changing your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect data using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>jaar</th>\n",
       "      <th>partij</th>\n",
       "      <th>titel</th>\n",
       "      <th>vraag</th>\n",
       "      <th>antwoord</th>\n",
       "      <th>ministerie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>KVR1000.xml</th>\n",
       "      <td>1994</td>\n",
       "      <td>PvdA</td>\n",
       "      <td>De vragen betreffen de betrouwbaarheid van de...</td>\n",
       "      <td>Hebt u kennisgenomen van het televisieprogram...</td>\n",
       "      <td>Ja. Het bedoelde geluidmeetpunt is eigendom v...</td>\n",
       "      <td>Verkeer en Waterstaat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KVR10000.xml</th>\n",
       "      <td>1999</td>\n",
       "      <td>PvdA</td>\n",
       "      <td>Vragen naar aanleiding van berichten (uitzend...</td>\n",
       "      <td>Kent u de berichten over de situatie in de Me...</td>\n",
       "      <td></td>\n",
       "      <td>Justitie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KVR10001.xml</th>\n",
       "      <td>1999</td>\n",
       "      <td>SP</td>\n",
       "      <td>Vragen naar aanleiding van de berichten \"Nede...</td>\n",
       "      <td>Kent u de berichten «Nederland steunt de Soeh...</td>\n",
       "      <td></td>\n",
       "      <td>Financien</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KVR10002.xml</th>\n",
       "      <td>1999</td>\n",
       "      <td>PvdA</td>\n",
       "      <td>Vragen over de gebrekkige opvang van verpleeg...</td>\n",
       "      <td>Kent u het bericht over onderzoek van Nu91 me...</td>\n",
       "      <td>Ja. Het onderzoek van NU’91 wijst uit dat het...</td>\n",
       "      <td>Volksgezondheid, Welzijn en Sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KVR10003.xml</th>\n",
       "      <td>1999</td>\n",
       "      <td>PvdA</td>\n",
       "      <td>Vragen over onbetrouwbaarheid van filemeldingen.</td>\n",
       "      <td>Hebt u kennisgenomen van de berichten over de...</td>\n",
       "      <td>Ja. Nee. Door de waarnemers van het Algemeen ...</td>\n",
       "      <td>Verkeer en Waterstaat</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                jaar partij  \\\n",
       "KVR1000.xml     1994   PvdA   \n",
       " KVR10000.xml   1999   PvdA   \n",
       " KVR10001.xml   1999     SP   \n",
       " KVR10002.xml   1999   PvdA   \n",
       " KVR10003.xml   1999   PvdA   \n",
       "\n",
       "                                                           titel  \\\n",
       "KVR1000.xml     De vragen betreffen de betrouwbaarheid van de...   \n",
       " KVR10000.xml   Vragen naar aanleiding van berichten (uitzend...   \n",
       " KVR10001.xml   Vragen naar aanleiding van de berichten \"Nede...   \n",
       " KVR10002.xml   Vragen over de gebrekkige opvang van verpleeg...   \n",
       " KVR10003.xml   Vragen over onbetrouwbaarheid van filemeldingen.   \n",
       "\n",
       "                                                           vraag  \\\n",
       "KVR1000.xml     Hebt u kennisgenomen van het televisieprogram...   \n",
       " KVR10000.xml   Kent u de berichten over de situatie in de Me...   \n",
       " KVR10001.xml   Kent u de berichten «Nederland steunt de Soeh...   \n",
       " KVR10002.xml   Kent u het bericht over onderzoek van Nu91 me...   \n",
       " KVR10003.xml   Hebt u kennisgenomen van de berichten over de...   \n",
       "\n",
       "                                                        antwoord  \\\n",
       "KVR1000.xml     Ja. Het bedoelde geluidmeetpunt is eigendom v...   \n",
       " KVR10000.xml                                                      \n",
       " KVR10001.xml                                                      \n",
       " KVR10002.xml   Ja. Het onderzoek van NU’91 wijst uit dat het...   \n",
       " KVR10003.xml   Ja. Nee. Door de waarnemers van het Algemeen ...   \n",
       "\n",
       "                                       ministerie  \n",
       "KVR1000.xml                 Verkeer en Waterstaat  \n",
       " KVR10000.xml                            Justitie  \n",
       " KVR10001.xml                           Financien  \n",
       " KVR10002.xml   Volksgezondheid, Welzijn en Sport  \n",
       " KVR10003.xml               Verkeer en Waterstaat  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "kvrdf= pd.read_csv('http://maartenmarx.nl/teaching/zoekmachines/LectureNotes/MySQL/KVR1000.csv.gz', \n",
    "                   compression='gzip', sep='\\t', \n",
    "                   index_col=0, names=['jaar', 'partij','titel','vraag','antwoord','ministerie']) \n",
    "print kvrdf.shape\n",
    "kvrdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mysql.connector.connection.MySQLConnection at 0x7f3d4857d1d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mysql.connector\n",
    "mysql.connector.connect(host='localhost',database='mysql',user='root',password='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
