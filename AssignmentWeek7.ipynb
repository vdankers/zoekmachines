{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Assignment  Practice Text classification with Naive Bayes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook made by   (If not filled in correctly: 0 pts for assignment)\n",
    "\n",
    "__Name(s)__: Adriaan de Vries, Max Briel, Verna Dankers\n",
    "\n",
    "__Student id(s)__ : 10795227, 10606513, 10761225\n",
    "\n",
    "### Pledge (taken from [Coursera's Honor Code](https://www.coursera.org/about/terms/honorcode) )\n",
    "\n",
    "\n",
    "\n",
    "Put here a selfie with your photo where you hold a signed paper with the following text: (if this is team work, put two selfies here). The link must be to some place on the web, not to a local file. **Assignments without the selfies will not be graded and receive 0 points.**\n",
    "\n",
    "> My answers to homework, quizzes and exams will be my own work (except for assignments that explicitly permit collaboration).\n",
    "\n",
    ">I will not make solutions to homework, quizzes or exams available to anyone else. This includes both solutions written by me, as well as any official solutions provided by the course staff.\n",
    "\n",
    ">I will not engage in any other activities that will dishonestly improve my results or dishonestly improve/hurt the results of others.\n",
    "\n",
    "<img width=30% src=\"http://i63.tinypic.com/2itj1ic.jpg\" border=\"0\">\n",
    "<img width=30% src='http://oi63.tinypic.com/2q3srh3.jpg'/>\n",
    "<img width=30% src='https://photos-2.dropbox.com/t/2/AAAtQkhxn93mSdCo78Akm_JrjGbR8ihz98WUkZ3kW8bZvA/12/3591884/jpeg/32x32/1/_/1/2/code.jpg/ENyV2wIYhmIgBygH/NmAVceS-6YjoIXKP3nEG6mMUSaAD7WRrf76MHAXLRIc?size=1600x1200&size_mode=3'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice Text classification with Naive Bayes  \n",
    "        \n",
    "        \n",
    "        \n",
    "<h3>Abstract</h3>\n",
    "<p>We will do text classification on a collection of Dutch parliamentary questions.\n",
    "    The website <a href=\"https://zoek.officielebekendmakingen.nl/zoeken/parlementaire_documenten\">officielebekendmakingen.nl</a>lets you search in \"kamervragen\".\n",
    "    You can donwload\n",
    "    <a href='http://data.politicalmashup.nl/kamervragen/PoliDocs_Kamervragen.zip'>this zipfile with Kamervragen in XML</a>\n",
    "    to see some of the  data in XML format. \n",
    "    It also contains style sheets to show the XML well in a browser.  \n",
    "    The <a href='http://maartenmarx.nl/teaching/zoekmachines/LectureNotes/MySQL/'>MYSQL directory</a> contains an <a href='http://maartenmarx.nl/teaching/zoekmachines/LectureNotes/MySQL/KVR14807.xml'>example   Kamervraag XML file</a>. Note that in your browser you see the result of applying stylesheets. So choose View Source or open it in an editor.</p>\n",
    "\n",
    "<h3>First exploration</h3>\n",
    "\n",
    "See below.\n",
    "\n",
    "<h2>Exercise</h2>\n",
    "\n",
    "<p>We will use the fields in elements of the form <tt> &lt;item attribuut=\"Afkomstig_van\"></tt> as our classes. \n",
    "    These are the ministeries to whom the question is addressed.\n",
    "    An example is \n",
    "    <pre>\n",
    "        &lt;item attribuut=\"Afkomstig_van\">Landbouw, Natuurbeheer en Visserij (LNV)&lt;/item>\n",
    "    </pre>\n",
    "    Note that these labels are <strong>not normalized</strong>, see e.g. the counts below:\n",
    "    <pre>\n",
    "Justitie (JUS)                                                   3219\n",
    "Volksgezondheid, Welzijn en Sport (VWS)                          2630\n",
    "Buitenlandse Zaken (BUZA)                                        1796\n",
    "Verkeer en Waterstaat (VW)                                       1441\n",
    "Justitie                                                         1333\n",
    "Sociale Zaken en Werkgelegenheid (SZW)                           1231\n",
    "Onderwijs, Cultuur en Wetenschappen (OCW)                        1187\n",
    "Volkshuisvesting, Ruimtelijke Ordening en Milieubeheer (VROM)     984\n",
    "FinanciÃ«n (FIN)                                                   960\n",
    "Volksgezondheid, Welzijn en Sport                                 951\n",
    "Economische Zaken (EZ)                                            946\n",
    "Buitenlandse Zaken                                                753\n",
    "Binnenlandse Zaken en Koninkrijksrelaties (BZK)                   725\n",
    "Verkeer en Waterstaat                                             724\n",
    "Defensie (DEF)                                                    646\n",
    "Sociale Zaken en Werkgelegenheid                                  607\n",
    "Landbouw, Natuurbeheer en Visserij (LNV)                          586\n",
    "Volkshuisvesting, Ruimtelijke Ordening en Milieubeheer            554\n",
    "Onderwijs, Cultuur en Wetenschappen                               532\n",
    "Vreemdelingenzaken en Integratie (VI)                             466\n",
    "    </pre>\n",
    "</p>\n",
    "\n",
    "  <ol>\n",
    "      <li>Normalize the values for \"ministerie\" and choose 10 ministeries to work with. </li>\n",
    "      <li>Implement the two algorithms in Fig MRS.13.2, using your earlier code for creating term and document frequencies.\n",
    "      It might be easier to use the representation and formula given in MRS section 13.4.1.</li>\n",
    "      <li>On this collection, train NB text classifiers for 10 different classes with enough and interesting data.</li>\n",
    "      <li>Compute for each term and each of your 10 classes its utility for that class using mutual information.</li>\n",
    "      <li>For each class, show the top 10 words as in Figure 13.7 in MRS.</li>\n",
    "      <li>Evaluate your classifiers using Precision, Recall and F1. (\n",
    "           <br/>\n",
    "          Give a table in which you show these values for using the top 10, top 100 terms and all terms, for all of your 10 classes.\n",
    "          Thus do feature selection per class, and use for each class the top n best features for that class. \n",
    "          <br/>\n",
    "      Also show the microaverage(s) for all 10 classes together.\n",
    "      <br/>\n",
    "      If you like you can also present this in a figure like MRS.13.8. \n",
    "      Then compute the F1 measure for the same number of terms as in that figure.</li>\n",
    "      <li>Reflect and report briefly about your choices in this process and about the obtained results. </li>\n",
    "  </ol>\n",
    "\n",
    "<h3>Training/Testing</h3>\n",
    "<p>It is important that you do not test your classifier using documents that have also been used in training.\n",
    "    So split up your collection in a training set and a test set. A 80%-20% split is reasonable.\n",
    "\n",
    "<br/>\n",
    "    If you have too little data you can use 5 or <a href=\"http://en.wikipedia.org/wiki/Cross-validation_(statistics)#k-fold_cross-validation\">10-fold cross validation</a>.</p>\n",
    "\n",
    "<h2>Form of presentation</h2>\n",
    "<ul>\n",
    "    <li>Make slides or wikipages and have your system running and be able to accept documents from the web. </li>\n",
    "    <li>Create one or two slides or wikipages for each of the sub exercises listed above.\n",
    "</li>\n",
    "<li>Make it clear in the heading of the slides which sub exercises you talk about.</li>\n",
    "    <li>Show running code with one or two  good examples (a TP of course, but also a FP and an error-analysis is nice to show). </li>\n",
    "\n",
    "</ul>\n",
    "\n",
    "<h2>Form of handing in your final product</h2>\n",
    "<ul>\n",
    "    <li>An IPython notebook would be perfect, with clear indications which part of the code answers which subquestion.</li>\n",
    "    <li>A clear git repo, with good comments and a clear separation and indication what code does what is also fine.</li>\n",
    "    <li> You are free to program in whatever language you prefer.</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some example code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 6)\n",
      "                jaar partij  \\\n",
      "KVR1000.xml     1994   PvdA   \n",
      " KVR10000.xml   1999   PvdA   \n",
      " KVR10001.xml   1999     SP   \n",
      " KVR10002.xml   1999   PvdA   \n",
      " KVR10003.xml   1999   PvdA   \n",
      "\n",
      "                                                           titel  \\\n",
      "KVR1000.xml     De vragen betreffen de betrouwbaarheid van de...   \n",
      " KVR10000.xml   Vragen naar aanleiding van berichten (uitzend...   \n",
      " KVR10001.xml   Vragen naar aanleiding van de berichten \"Nede...   \n",
      " KVR10002.xml   Vragen over de gebrekkige opvang van verpleeg...   \n",
      " KVR10003.xml   Vragen over onbetrouwbaarheid van filemeldingen.   \n",
      "\n",
      "                                                           vraag  \\\n",
      "KVR1000.xml     Hebt u kennisgenomen van het televisieprogram...   \n",
      " KVR10000.xml   Kent u de berichten over de situatie in de Me...   \n",
      " KVR10001.xml   Kent u de berichten «Nederland steunt de Soeh...   \n",
      " KVR10002.xml   Kent u het bericht over onderzoek van Nu91 me...   \n",
      " KVR10003.xml   Hebt u kennisgenomen van de berichten over de...   \n",
      "\n",
      "                                                        antwoord  \\\n",
      "KVR1000.xml     Ja. Het bedoelde geluidmeetpunt is eigendom v...   \n",
      " KVR10000.xml                                                      \n",
      " KVR10001.xml                                                      \n",
      " KVR10002.xml   Ja. Het onderzoek van NU’91 wijst uit dat het...   \n",
      " KVR10003.xml   Ja. Nee. Door de waarnemers van het Algemeen ...   \n",
      "\n",
      "                                       ministerie  \n",
      "KVR1000.xml                 Verkeer en Waterstaat  \n",
      " KVR10000.xml                            Justitie  \n",
      " KVR10001.xml                           Financien  \n",
      " KVR10002.xml   Volksgezondheid, Welzijn en Sport  \n",
      " KVR10003.xml               Verkeer en Waterstaat  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Change to KVR1000.csv.gz if this becomes to0 slow for you\n",
    "kvrdf= pd.read_csv('http://maartenmarx.nl/teaching/zoekmachines/LectureNotes/MySQL/KVR1000.csv.gz', \n",
    "                   compression='gzip', sep='\\t', encoding='utf-8',\n",
    "                   index_col=0, names=['jaar', 'partij','titel','vraag','antwoord','ministerie']) \n",
    "print kvrdf.shape\n",
    "print kvrdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from collections import Counter\n",
    "\n",
    "# Join all titels of the questions together\n",
    "allvragen= '\\n'.join(list(kvrdf.titel))\n",
    "print len(allvragen), allvragen[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'van', 2392),\n",
       " (u'de', 1846),\n",
       " (u'.', 1366),\n",
       " (u'het', 1137),\n",
       " (u',', 901),\n",
       " (u'in', 828),\n",
       " (u'Vragen', 792),\n",
       " (u'over', 764),\n",
       " (u'naar', 599),\n",
       " (u'en', 568),\n",
       " (u'een', 540),\n",
       " (u'aanleiding', 513),\n",
       " (u'1999', 406),\n",
       " (u')', 347),\n",
       " (u'(', 346),\n",
       " (u'op', 310),\n",
       " (u\"''\", 276),\n",
       " (u'voor', 272),\n",
       " (u'dat', 246),\n",
       " (u'vragen', 241)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize and turn into a bag of words\n",
    "BoW= Counter(nltk.word_tokenize(allvragen))\n",
    "# show the top 20 most used words \n",
    "BoW.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "from nltk.corpus import stopwords\n",
    "DutchStop= stopwords.words('dutch')\n",
    "print len(DutchStop)\n",
    "\n",
    "BoW= BoW= Counter([w for w in nltk.word_tokenize(allvragen) if not w in set(DutchStop)])\n",
    "# show the top 20 most used words \n",
    "BoW.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Normalize the values for \"ministerie\" and choose 10 ministeries to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'justitie', u'volksgezondheid, welzijn en sport', u'buitenlandse zaken', u'verkeer en waterstaat', u'sociale zaken en werkgelegenheid', u'onderwijs, cultuur en wetenschappen', u'volkshuisvesting, ruimtelijke ordening en milieubeheer', u'financi\\xebn', u'economische zaken', u'landbouw, natuurbeheer en visserij']\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "def get_corpus(frame):\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \"\"\"\n",
    "    ministeries = list(frame.ministerie)\n",
    "    documents = list(frame.titel)\n",
    "    new_documents = []\n",
    "    new_ministeries = []\n",
    "\n",
    "    for i, document in enumerate(documents):\n",
    "        try:\n",
    "            document = document.encode('utf-8')\n",
    "            new_documents.append(WhitespaceTokenizer().tokenize(document))\n",
    "            new_ministeries.append(ministeries[i])\n",
    "        except Exception:\n",
    "            pass\n",
    "    \n",
    "    return (new_documents, ministeries)\n",
    "\n",
    "def normalize_names(items):\n",
    "    \"\"\"\n",
    "    Normalizes names of given items and returns a counter\n",
    "    with the names and the number of occurences, \n",
    "    and a dict containing all equivalences.\n",
    "    \"\"\"\n",
    "    count_items = Counter()\n",
    "    synonyms = dict()\n",
    "    \n",
    "    for name in items:\n",
    "        if isinstance(name, (str, unicode)):\n",
    "            new_name = re.sub('\\(.*?\\)', '', name.lower()).strip()\n",
    "            count_items[new_name] += 1\n",
    "            synonyms[name] = new_name\n",
    "    \n",
    "    count_items = count_items.most_common(10)\n",
    "    classes = list(zip(*count_items)[0])\n",
    "    \n",
    "    return (synonyms, classes)\n",
    "\n",
    "# First we need to select all documents with their ministeries\n",
    "documents, ministeries = get_corpus(kvrdf)\n",
    "\n",
    "# Now we extract our top 10 classes\n",
    "synonyms, classes = normalize_names(ministeries)\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Implement the two algorithms in Fig MRS.13.2, using your earlier code for creating term and document frequencies. It might be easier to use the representation and formula given in MRS section 13.4.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "import codecs\n",
    "\n",
    "def train_multinomial_NB(classes, documents, ministeries, synonyms):\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \"\"\"\n",
    "    vocabulary = extract_vocabulary(documents)\n",
    "    n = len(documents)\n",
    "    class_members = calculate_membership(classes, zip(documents, ministeries), synonyms)\n",
    "    cond_probs = dict(Counter())\n",
    "    prior = dict()\n",
    "    \n",
    "    for c in classes:\n",
    "        nc = len(class_members[c])\n",
    "        prior[c] = nc / n\n",
    "        text = concatenate(class_members[c], documents)\n",
    "        \n",
    "        tokens_in_class = Counter()\n",
    "        for term in vocabulary:\n",
    "            tokens_in_class[term] = count_tokens(text, term)\n",
    "        \n",
    "        for term in vocabulary:\n",
    "            cond_probs[term] = calc_cond_prob(tokens_in_class, term)\n",
    "    \n",
    "    return (vocabulary, prior, cond_probs)\n",
    "\n",
    "def extract_vocabulary(documents):\n",
    "    \"\"\"\n",
    "    Given a list of documents, return the set\n",
    "    of unique tokens.\n",
    "    \"\"\"\n",
    "    vocabulary = set(str(token)\n",
    "                     for document in documents \n",
    "                     for token in document)\n",
    "    return vocabulary\n",
    "\n",
    "def calculate_membership(classes, data, synonyms):\n",
    "    \"\"\"\n",
    "    Create a dictionary with all indices of class members,\n",
    "    per class.\n",
    "    \"\"\"\n",
    "    membership = {c : [] for c in classes}\n",
    "    \n",
    "    for i, datarow in enumerate(data):\n",
    "        doc, ministerie = datarow\n",
    "        membership[synonyms[ministerie]].append(i)\n",
    "    \n",
    "    return membership\n",
    "\n",
    "# Babyvoorbeeld\n",
    "mem = calculate_membership(['dier','mens'],[('doc1','Dier'),('doc2','Mens (01)')],{'Dier':'dier','Mens (01)':'mens'})\n",
    "\n",
    "def concatenate(members, documents):\n",
    "    \"\"\"\n",
    "    Return the concatenation of all documents\n",
    "    that belong to members.\n",
    "    \"\"\"\n",
    "    \n",
    "def count_tokens(text, term):\n",
    "    \"\"\"\n",
    "    Return how often the term appears in the text.\n",
    "    \"\"\"\n",
    "    \n",
    "def calc_cond_prob(occurences, term):\n",
    "    \"\"\"\n",
    "    Return the conditional probability for a term\n",
    "    in a class.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def apply_multinomial_NB(classes, vocabulary, prior, cond_prob, doc):\n",
    "    \"\"\"\n",
    "    Classify doc with the the trained multinomial Naive Bayes classifier, which consists of the classes, documents, prior, and cond_prob. \n",
    "    \"\"\"\n",
    "\n",
    "    tokens = extract_tokens(vocabulary, doc)\n",
    "    for c in classes:\n",
    "        score[c] = log10(prior[c])\n",
    "        for t in tokens:\n",
    "            score[c] += log10(cond_prob[t][c])\n",
    "            \n",
    "    return max(score[c])\n",
    "\n",
    "\n",
    "\n",
    "def extract_tokens(vocabulary, doc)\n",
    "    \"\"\"\n",
    "    Extracts the tokens from to document that are present in the vocabulary.\n",
    "    \"\"\"\n",
    "    tokens = set()\n",
    "    for word in doc: \n",
    "        if word in vocabulary:\n",
    "            tokens.add(word)\n",
    "    return tokens\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'module' object has no attribute 'normalize'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-1eaa579cd577>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhelp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'module' object has no attribute 'normalize'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. On this collection, train NB text classifiers for 10 different classes with enough and interesting data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Compute for each term and each of your 10 classes its utility for that class using mutual information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. For each class, show the top 10 words as in Figure 13.7 in MRS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Evaluate your classifiers using Precision, Recall and F1. Give a table in which you show these values for using the top 10, top 100 terms and all terms, for all of your 10 classes. Thus do feature selection per class, and use for each class the top n best features for that class. Also show the microaverage(s) for all 10 classes together. If you like you can also present this in a figure like MRS.13.8. Then compute the F1 measure for the same number of terms as in that figure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 7. Reflect and report briefly about your choices in this process and about the obtained results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [NLTK]",
   "language": "python",
   "name": "Python [NLTK]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
