{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment  week 7 MRS Chap 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook made by   (If not filled in correctly: 0 pts for assignment)\n",
    "\n",
    "__Name(s)__: Adriaan de Vries, Max Briel\n",
    "\n",
    "__Student id(s)__ : 10795227, 10606513\n",
    "\n",
    "### Pledge (taken from [Coursera's Honor Code](https://www.coursera.org/about/terms/honorcode) )\n",
    "\n",
    "\n",
    "\n",
    "Put here a selfie with your photo where you hold a signed paper with the following text: (if this is team work, put two selfies here). The link must be to some place on the web, not to a local file. **Assignments without the selfies will not be graded and receive 0 points.**\n",
    "\n",
    "> My answers to homework, quizzes and exams will be my own work (except for assignments that explicitly permit collaboration).\n",
    "\n",
    ">I will not make solutions to homework, quizzes or exams available to anyone else. This includes both solutions written by me, as well as any official solutions provided by the course staff.\n",
    "\n",
    ">I will not engage in any other activities that will dishonestly improve my results or dishonestly improve/hurt the results of others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=30% src=\"http://i63.tinypic.com/2itj1ic.jpg\" border=\"0\">\n",
    "<img width=30% src='http://i67.tinypic.com/2hd1is5.jpg'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions  MRS chap 12: 12.3, 12.4, 12.6.a, 12.7-9\n",
    "\n",
    "\n",
    "### 12.3 Question\n",
    "\n",
    "What is the likelihood ratio of the document according to $M_1$ and $M_2$ in Example 12.2?\n",
    "\n",
    "### Answer\n",
    "\n",
    "Likelihood = to compare two models for a data set, we can calculate their likelihood ratio, which results from simply dividing the probability of the data according to one model by the probability of the data according to the other model.\n",
    "\n",
    "$P(s\\ | M_1 )$ = 0.00000000000048\n",
    "\n",
    "$P(s\\ | M_2 )$ = 0.000000000000000384\n",
    "\n",
    "likelihood = $\\frac{0.000000000000000384}{0.00000000000048}$ = 0.0008\n",
    "\n",
    "### 12.4 Question\n",
    "\n",
    "No explicit STOP probability appeared in Example 12.2. Assuming that the STOP probability of each model is 0.1, does this change the likelihood ratio of a document according to the two models?\n",
    "\n",
    "### Answer\n",
    "\n",
    "No, since the STOP probability will be the same for both models, the division will stay the same. When we look at this question mathematically, that's trivial.\n",
    "\n",
    "We can illustrate our point with the example from 12.3:\n",
    "\n",
    "likelihood = $\\frac{0.000000000000000384 * 0.1}{0.00000000000048 * 0.1}$ = 0.0008\n",
    "\n",
    "### 12.6.a Question\n",
    "\n",
    "Consider making a language model from the following training text: \"The martian has landed on the latin pop sensation ricky martin\". What are P(the) and P(martian) under a MLE-estimated unigram probability model?\n",
    "\n",
    "\n",
    "### Answer\n",
    "\n",
    "There are in total 11 words in the training text, one of which is 'martian', and 'the' occurs twice. This gives $P(the) = \\frac{2}{11}$ and $P(martian) = \\frac{1}{11}$\n",
    "\n",
    "### 12.7 Question\n",
    "\n",
    "Given 4 documents, give the query probability for 'click', 'shears' and 'click shears' for each of the documents using a mixture model between the documents and the collection, both weighted with 0.5. Use MLE and unigram models.\n",
    "\n",
    "Doc1: click go the shears boys click click click\n",
    "\n",
    "Doc2: click click\n",
    "\n",
    "Doc3: metal here\n",
    "\n",
    "Doc4: metal shears click here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Answer\n",
    "\n",
    "We have in total 16 words. 8 of them are click, 2 of them are shears. Using (12.12) from the book, this leads to the following table with results:\n",
    "\n",
    "| Query        | Doc1                                                   | Doc2                                                   | Doc3                                                  | Doc4                                                   |\n",
    "|--------------|--------------------------------------------------------|--------------------------------------------------------|-------------------------------------------------------|--------------------------------------------------------|\n",
    "| click        | $\\frac{\\frac{4}{8} + \\frac{7}{16}}{2} = \\frac{15}{32}$ | $\\frac{\\frac{2}{2} + \\frac{7}{16}}{2} = \\frac{23}{32}$ | $\\frac{\\frac{0}{2} + \\frac{7}{16}}{2} = \\frac{7}{32}$ | $\\frac{\\frac{1}{4} + \\frac{7}{16}}{2} = \\frac{11}{32}$ |\n",
    "| shears       | $\\frac{\\frac{1}{8} + \\frac{2}{16}}{2} = \\frac{1}{8}$   | $\\frac{\\frac{0}{2} + \\frac{2}{16}}{2} = \\frac{1}{16}$  | $\\frac{\\frac{0}{2} + \\frac{2}{16}}{2} = \\frac{1}{16}$ | $\\frac{\\frac{1}{4} + \\frac{2}{16}}{2} = \\frac{3}{16}$  |\n",
    "| click shears | $ \\frac{15}{32} * \\frac{1}{8} = \\frac{15}{256}$        | $\\frac{23}{32} * \\frac{1}{16} = \\frac{23}{512}$        | $\\frac{7}{32} * \\frac{1}{16} = \\frac{7}{512}$         | $\\frac{11}{32} * \\frac{3}{16} = \\frac{33}{512}$        |\n",
    "\n",
    "So the final ranking for 'click shears' is from better to worse: Doc4, Doc1, Doc2, Doc3\n",
    "\n",
    "### 12.8 Question\n",
    "\n",
    "For term frequency in a document, collection frequency of a term, document frequency of a term and length normalization of a term, indicate whether they are present in the model from equation (12.10), and write one sentence indicating its role in the model. Include whether the effect is raw or scaled.\n",
    "\n",
    "\n",
    "\n",
    "### Answer\n",
    "\n",
    "term frequency in a document: The term frequency in a document is used, scaled by the length of the document, as $P_{mle}(t|M_d)$\n",
    "\n",
    "collection frequency of a term: The collection frequency is used, scaled by the length in tokens of the collection, as $P_{mle}(t|M_c)$\n",
    "\n",
    "Document frequency of a term: The document frequency is not used at all in (12.10).\n",
    "\n",
    "Length normalization of a term: The lengths of the documents and of the collection are used, raw, to normalize the term frequency and collection frequency to get the true MLE estimates.\n",
    "\n",
    "### 12.9 Question\n",
    "\n",
    "In (12.12) the probability estimate is based on the term frequency of a word in the document, and the collection frequency of the word. Apart from avoiding probabilities of zero, it has a more subtle and important effect of implementing a form of term weighting. Explain how this works.\n",
    "\n",
    "### Answer\n",
    "\n",
    "In this model, a document containing rare words from the query gets rewarded. This is because the probability from the entire collection will be very low, and thus the probability in the document can make the total interpolated probability much higher relatively. Let us consider these examples: We have a collection with 100 tokens, half of which are 'click', and only one of which is 'shears'. Let's further assume all documents are 10 tokens long. The query is 'click shears'. We use the same model as in exercise (12.7).\n",
    "\n",
    "Now, if a document is just 'click' repeated 10 times, it will only be better than not containing any word from the query by a factor 3, as $(0.5 + 1) / 2 = 0.75$ and $0.5/2 = 0.25$.\n",
    "However, a document containing the word shear once will change its probability significantly more: $0.01/2 = 0.005$ and $(0.01 + 0.1)/2 = 0.055$. That's eleven times as much!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
