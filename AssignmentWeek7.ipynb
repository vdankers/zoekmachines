{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Assignment  Practice Text classification with Naive Bayes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook made by   (If not filled in correctly: 0 pts for assignment)\n",
    "\n",
    "__Name(s)__: Adriaan de Vries, Max Briel, Verna Dankers\n",
    "\n",
    "__Student id(s)__ : 10795227, 10606513, 10761225\n",
    "\n",
    "### Pledge (taken from [Coursera's Honor Code](https://www.coursera.org/about/terms/honorcode) )\n",
    "\n",
    "\n",
    "\n",
    "Put here a selfie with your photo where you hold a signed paper with the following text: (if this is team work, put two selfies here). The link must be to some place on the web, not to a local file. **Assignments without the selfies will not be graded and receive 0 points.**\n",
    "\n",
    "> My answers to homework, quizzes and exams will be my own work (except for assignments that explicitly permit collaboration).\n",
    "\n",
    ">I will not make solutions to homework, quizzes or exams available to anyone else. This includes both solutions written by me, as well as any official solutions provided by the course staff.\n",
    "\n",
    ">I will not engage in any other activities that will dishonestly improve my results or dishonestly improve/hurt the results of others.\n",
    "\n",
    "<img width=30% src=\"http://i63.tinypic.com/2itj1ic.jpg\" border=\"0\">\n",
    "<img width=30% src='http://oi63.tinypic.com/2q3srh3.jpg'/>\n",
    "<img width=30% src='https://photos-2.dropbox.com/t/2/AAAtQkhxn93mSdCo78Akm_JrjGbR8ihz98WUkZ3kW8bZvA/12/3591884/jpeg/32x32/1/_/1/2/code.jpg/ENyV2wIYhmIgBygH/NmAVceS-6YjoIXKP3nEG6mMUSaAD7WRrf76MHAXLRIc?size=1600x1200&size_mode=3'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice Text classification with Naive Bayes  \n",
    "        \n",
    "        \n",
    "        \n",
    "<h3>Abstract</h3>\n",
    "<p>We will do text classification on a collection of Dutch parliamentary questions.\n",
    "    The website <a href=\"https://zoek.officielebekendmakingen.nl/zoeken/parlementaire_documenten\">officielebekendmakingen.nl</a>lets you search in \"kamervragen\".\n",
    "    You can donwload\n",
    "    <a href='http://data.politicalmashup.nl/kamervragen/PoliDocs_Kamervragen.zip'>this zipfile with Kamervragen in XML</a>\n",
    "    to see some of the  data in XML format. \n",
    "    It also contains style sheets to show the XML well in a browser.  \n",
    "    The <a href='http://maartenmarx.nl/teaching/zoekmachines/LectureNotes/MySQL/'>MYSQL directory</a> contains an <a href='http://maartenmarx.nl/teaching/zoekmachines/LectureNotes/MySQL/KVR14807.xml'>example   Kamervraag XML file</a>. Note that in your browser you see the result of applying stylesheets. So choose View Source or open it in an editor.</p>\n",
    "\n",
    "<h3>First exploration</h3>\n",
    "\n",
    "See below.\n",
    "\n",
    "<h2>Exercise</h2>\n",
    "\n",
    "<p>We will use the fields in elements of the form <tt> &lt;item attribuut=\"Afkomstig_van\"></tt> as our classes. \n",
    "    These are the ministeries to whom the question is addressed.\n",
    "    An example is \n",
    "    <pre>\n",
    "        &lt;item attribuut=\"Afkomstig_van\">Landbouw, Natuurbeheer en Visserij (LNV)&lt;/item>\n",
    "    </pre>\n",
    "    Note that these labels are <strong>not normalized</strong>, see e.g. the counts below:\n",
    "    <pre>\n",
    "Justitie (JUS)                                                   3219\n",
    "Volksgezondheid, Welzijn en Sport (VWS)                          2630\n",
    "Buitenlandse Zaken (BUZA)                                        1796\n",
    "Verkeer en Waterstaat (VW)                                       1441\n",
    "Justitie                                                         1333\n",
    "Sociale Zaken en Werkgelegenheid (SZW)                           1231\n",
    "Onderwijs, Cultuur en Wetenschappen (OCW)                        1187\n",
    "Volkshuisvesting, Ruimtelijke Ordening en Milieubeheer (VROM)     984\n",
    "FinanciÃ«n (FIN)                                                   960\n",
    "Volksgezondheid, Welzijn en Sport                                 951\n",
    "Economische Zaken (EZ)                                            946\n",
    "Buitenlandse Zaken                                                753\n",
    "Binnenlandse Zaken en Koninkrijksrelaties (BZK)                   725\n",
    "Verkeer en Waterstaat                                             724\n",
    "Defensie (DEF)                                                    646\n",
    "Sociale Zaken en Werkgelegenheid                                  607\n",
    "Landbouw, Natuurbeheer en Visserij (LNV)                          586\n",
    "Volkshuisvesting, Ruimtelijke Ordening en Milieubeheer            554\n",
    "Onderwijs, Cultuur en Wetenschappen                               532\n",
    "Vreemdelingenzaken en Integratie (VI)                             466\n",
    "    </pre>\n",
    "</p>\n",
    "\n",
    "  <ol>\n",
    "      <li>Normalize the values for \"ministerie\" and choose 10 ministeries to work with. </li>\n",
    "      <li>Implement the two algorithms in Fig MRS.13.2, using your earlier code for creating term and document frequencies.\n",
    "      It might be easier to use the representation and formula given in MRS section 13.4.1.</li>\n",
    "      <li>On this collection, train NB text classifiers for 10 different classes with enough and interesting data.</li>\n",
    "      <li>Compute for each term and each of your 10 classes its utility for that class using mutual information.</li>\n",
    "      <li>For each class, show the top 10 words as in Figure 13.7 in MRS.</li>\n",
    "      <li>Evaluate your classifiers using Precision, Recall and F1. (\n",
    "           <br/>\n",
    "          Give a table in which you show these values for using the top 10, top 100 terms and all terms, for all of your 10 classes.\n",
    "          Thus do feature selection per class, and use for each class the top n best features for that class. \n",
    "          <br/>\n",
    "      Also show the microaverage(s) for all 10 classes together.\n",
    "      <br/>\n",
    "      If you like you can also present this in a figure like MRS.13.8. \n",
    "      Then compute the F1 measure for the same number of terms as in that figure.</li>\n",
    "      <li>Reflect and report briefly about your choices in this process and about the obtained results. </li>\n",
    "  </ol>\n",
    "\n",
    "<h3>Training/Testing</h3>\n",
    "<p>It is important that you do not test your classifier using documents that have also been used in training.\n",
    "    So split up your collection in a training set and a test set. A 80%-20% split is reasonable.\n",
    "\n",
    "<br/>\n",
    "    If you have too little data you can use 5 or <a href=\"http://en.wikipedia.org/wiki/Cross-validation_(statistics)#k-fold_cross-validation\">10-fold cross validation</a>.</p>\n",
    "\n",
    "<h2>Form of presentation</h2>\n",
    "<ul>\n",
    "    <li>Make slides or wikipages and have your system running and be able to accept documents from the web. </li>\n",
    "    <li>Create one or two slides or wikipages for each of the sub exercises listed above.\n",
    "</li>\n",
    "<li>Make it clear in the heading of the slides which sub exercises you talk about.</li>\n",
    "    <li>Show running code with one or two  good examples (a TP of course, but also a FP and an error-analysis is nice to show). </li>\n",
    "\n",
    "</ul>\n",
    "\n",
    "<h2>Form of handing in your final product</h2>\n",
    "<ul>\n",
    "    <li>An IPython notebook would be perfect, with clear indications which part of the code answers which subquestion.</li>\n",
    "    <li>A clear git repo, with good comments and a clear separation and indication what code does what is also fine.</li>\n",
    "    <li> You are free to program in whatever language you prefer.</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import our zipped data from a weblink into a pandas data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Import the kvr data from the online storage\n",
    "kvrdf= pd.read_csv('http://maartenmarx.nl/teaching/zoekmachines/LectureNotes/MySQL/KVR.csv.gz', \n",
    "                   compression='gzip', sep='\\t', encoding='utf-8',\n",
    "                   index_col=0, names=['jaar', 'partij','titel','vraag','antwoord','ministerie'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Normalize the values for \"ministerie\" and choose 10 ministeries to work with.\n",
    "\n",
    "After normalization, these were the 10 most common \"ministeries\", we chose to work with these 10: \n",
    "\"justitie\", \"volksgezondheid, welzijn en sport\", \"buitenlandse zaken\", \"verkeer en waterstaat\", \"sociale zaken en werkgelegenheid\", \"onderwijs, cultuur en wetenschappen\", \"volkshuisvesting, ruimtelijke ordening en milieubeheer\", \"financi\\xebn\", \"economische zaken\", \"landbouw, natuurbeheer en visserij\""
   ]
  },
  {
   "cell_type": "code",

   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'justitie', u'volksgezondheid, welzijn en sport', u'buitenlandse zaken', u'verkeer en waterstaat', u'sociale zaken en werkgelegenheid', u'onderwijs, cultuur en wetenschappen', u'volkshuisvesting, ruimtelijke ordening en milieubeheer', u'financi\\xebn', u'economische zaken', u'landbouw, natuurbeheer en visserij']\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "def get_corpus(frame, n):\n",
    "    \"\"\"\n",
    "    Extract the normalised documents from the top n most common normalised ministries \n",
    "    and return a list of the documents, ministries, and the top n classes. \n",
    "    \"\"\"\n",
    "    # Extract all documents and their ministries from the pandas Dataframe\n",
    "    ministeries = list(frame.ministerie)\n",
    "    documents = list(frame.vraag)\n",
    "    \n",
    "    # Now we extract our top classes and the synonyms per class\n",
    "    synonyms, classes = normalize_names(ministeries, n)\n",
    "    \n",
    "    # Find all documents that belong to the top classes and normalize all the words in them \n",
    "    # and at the same time store the corresponding normalised ministry in a seperate list. \n",
    "    new_d, new_m = zip(*((normalize(x.encode('utf-8')).split(),synonyms[y])\n",
    "                             for (x,y) in zip(documents, ministeries) \n",
    "                                 if isinstance(y,(str,unicode)) and synonyms[y] in classes))\n",
    "\n",
    "    return (new_d, new_m, classes)\n",
    "\n",
    "def normalize(document):\n",
    "    \"\"\"\n",
    "    Normalize capital letters and punctuation in  a document. \n",
    "    \"\"\"\n",
    "    document = re.sub(r'[^\\w\\s]', '', document.lower())\n",
    "    return document\n",
    "\n",
    "def normalize_names(items, n):\n",
    "    \"\"\"\n",
    "    Normalizes names of given items and returns a counter\n",
    "    with the names and the number of occurences, \n",
    "    and a dict containing all equivalences.\n",
    "    \"\"\"\n",
    "    count_items = Counter()\n",
    "    synonyms = dict()\n",
    "    \n",
    "    for name in items:\n",
    "        if isinstance(name, (str, unicode)):\n",
    "            new_name = re.sub('\\(.*?\\)', '', name.lower()).strip()\n",
    "            count_items[new_name] += 1\n",
    "            synonyms[name] = new_name\n",
    "    \n",
    "    count_items = count_items.most_common(n)\n",
    "    classes = list(zip(*count_items)[0])\n",
    "    \n",
    "    return (synonyms, classes)\n",
    "\n",
    "# Normalize the kamervragen from the top 10 normalized ministeries.  \n",
    "documents, ministeries, classes = get_corpus(kvrdf, 10)"
    "print(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Implement the two algorithms in Fig MRS.13.2, using your earlier code for creating term and document frequencies. It might be easier to use the representation and formula given in MRS section 13.4.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "import codecs\n",
    "from __future__ import division\n",
    "\n",
    "def train_multinomial_NB(classes, documents, ministeries):\n",
    "    \"\"\"\n",
    "    Train a Naive Bayes classifier on the given data for \n",
    "    a given set of classes. Return the vocabulary, and\n",
    "    prior and conditional probabilities.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract the vocabulary from the documents. \n",
    "    vocabulary = set(token for document in documents for token in document)\n",
    "\n",
    "    n = len(documents)\n",
    "    class_members = calculate_membership(classes, ministeries)\n",
    "\n",
    "    cond_probs = {term : dict() for term in vocabulary}\n",
    "    prior = dict()\n",
    "    \n",
    "    for c in classes:\n",
    "        nc = len(class_members[c])\n",
    "        # Calculate the prior for each class\n",
    "        prior[c] = nc / n\n",
    "        # Get all the words from a specific class \n",
    "        text = [word for i in class_members[c] for word in documents[i]]\n",
    "        \n",
    "        # Get the term frequency per term for documents in the class. \n",
    "        occurences = Counter()\n",
    "        for token in text:\n",
    "            occurences[token] += 1\n",
    "        \n",
    "        # Calculate the cond_probability per term per class. \n",
    "        denominator = sum(occurences[term] for term in occurences) + 1\n",
    "        for term in vocabulary:\n",
    "            cond_probs[term][c] = (occurences[term] + 1) / denominator\n",
    "    \n",
    "    return (vocabulary, prior, cond_probs)\n",
    "\n",
    "\n",
    "def calculate_membership(classes, ministeries):\n",
    "    \"\"\"\n",
    "    Create a dictionary with all indices of class members,\n",
    "    per class.\n",
    "    \"\"\"\n",
    "    \n",
    "    membership = {c : [] for c in classes}\n",
    "    for i, ministerie in enumerate(ministeries):\n",
    "        membership[ministerie].append(i)\n",
    "    return membership"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "def apply_multinomial_NB(classes, vocabulary, prior, cond_prob, doc):\n",
    "    \"\"\"\n",
    "    Classify doc with the the trained multinomial Naive Bayes classifier, \n",
    "    which consists of the classes, documents, prior, and cond_prob. \n",
    "    The funciton returns a predicted class for the document. \n",
    "    \"\"\"\n",
    "    score = {}\n",
    "    tokens = set([word for word in doc if word in vocabulary])\n",
    "            \n",
    "    for c in classes:\n",
    "        score[c] = log(prior[c],10)\n",
    "        for t in tokens:\n",
    "            score[c] += log(cond_prob[t][c],10)   \n",
    "    # return the class for which the score is maximum. \n",
    "    return max(score, key=score.get) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. On this collection, train NB text classifiers for 10 different classes with enough and interesting data.\n",
    "\n",
    "With these 10 classes, there are 21621 samples that we can use, which is more than half of the data. We split these samples in train and test samples. We use 75% of the samples as data for training and 25% of the samples as data for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 21621 samples, 16215 are used for training, 5406 are used for testing.\n"
     ]
    }
   ],
   "source": [
    "l = len(documents)\n",
    "n = int(l * 0.75)\n",
    "\n",
    "train_docs = documents[:n]\n",
    "train_mins = ministeries[:n]\n",
    "test_docs = documents[n:]\n",
    "test_mins = ministeries[n:]\n",
    "\n",
    "print \"There are %d samples, %d are used for training, %d are used for testing.\" % (l, n, l - n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'financi\\xebn': 0.6956521739130435, u'economische zaken': 0.5683453237410072, u'sociale zaken en werkgelegenheid': 0.8138075313807531, u'justitie': 0.6644351464435146, u'landbouw, natuurbeheer en visserij': 0.8823529411764706, u'onderwijs, cultuur en wetenschappen': 0.7780678851174935, u'volkshuisvesting, ruimtelijke ordening en milieubeheer': 0.6823529411764706, u'buitenlandse zaken': 0.7686567164179104, u'verkeer en waterstaat': 0.6313043478260869, u'volksgezondheid, welzijn en sport': 0.6252900232018561}\n"
     ]
    }
   ],
   "source": [
    "def test_NB(classes, vocab, prior, cond_prob, documents, ministeries):\n",
    "    \"\"\"\n",
    "    Test the Naive Bayes classifier on given test data\n",
    "    and return the accuracy per class.\n",
    "    \"\"\"\n",
    "    accuracy = { c : Counter() for c in classes }\n",
    "\n",
    "    for i, doc in enumerate(documents):\n",
    "        score = apply_multinomial_NB(classes, vocab, prior, cond_prob, doc)\n",
    "        ministerie = ministeries[i]\n",
    "        accuracy[ministerie]['all'] += 1\n",
    "    \n",
    "        if score == ministerie:\n",
    "            accuracy[ministerie]['right'] += 1\n",
    "\n",
    "    return { c : accuracy[c]['right'] / accuracy[c]['all'] for c in accuracy }\n",
    "\n",
    "# Train NB text classifiers for 10 classes \n",
    "vocab, prior, cond_prob = train_multinomial_NB(classes, train_docs, train_mins)\n",
    "\n",
    "# Test NB classifier on test data\n",
    "acc = test_NB(classes, vocab, prior, cond_prob, test_docs, test_mins)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Compute for each term and each of your 10 classes its utility for that class using mutual information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from __future__ import division\n",
    "\n",
    "# The division import had already been made, but if that somehow failed\n",
    "# these functions would return wrong answers instead of giving an error\n",
    "# so they're added for certainty\n",
    "\n",
    "def get_occurence_counts(docs, ministeries):\n",
    "    \"\"\"\n",
    "    Returns a dict that for each term has the amount \n",
    "    of documents it appears in per class/ministerie.\n",
    "    \"\"\"\n",
    "    count_dict = defaultdict(Counter)\n",
    "    for doc,mini in zip(docs, ministeries):\n",
    "        \n",
    "        # remove duplicates\n",
    "        doc = set(doc)\n",
    "        for token in doc:\n",
    "            count_dict[token][mini] += 1\n",
    "            count_dict[token]['total'] += 1\n",
    "            \n",
    "    # returning a defaultdict is asking for trouble. Thus, return normal dict\n",
    "    return dict(count_dict)\n",
    "\n",
    "def mutual_inf(term, ministerie, count_dict, N, docs_in_class):\n",
    "    \"\"\"\n",
    "    Returns the mutual information of term and ministerie.\n",
    "    \"\"\"\n",
    "    if term not in count_dict:\n",
    "        return 0\n",
    "    N11 = count_dict[term].get(ministerie, 0)\n",
    "    N1_ = count_dict[term].get('total', 0)\n",
    "    N10 = N1_ - N11\n",
    "    N01 = docs_in_class - N11\n",
    "    N0_ = N - N1_\n",
    "    N00 = N0_ - N01\n",
    "    N_1 = docs_in_class\n",
    "    N_0 = N - N_1\n",
    "    \n",
    "    result = 0\n",
    "    result+=(N11/N) * my_log_and_div(N*N11,N1_*N_1)\n",
    "    result+=(N01/N) * my_log_and_div(N*N01,N0_*N_1)\n",
    "    result+=(N10/N) * my_log_and_div(N*N10,N1_*N_0)\n",
    "    result+=(N00/N) * my_log_and_div(N*N00,N0_*N_0)\n",
    "    return result\n",
    "    \n",
    "def my_log_and_div(n, m):\n",
    "    \"\"\"\n",
    "    Returns log of n/m with base 2,\n",
    "    except it returns 0 for n=0.\n",
    "    \"\"\"\n",
    "    if n == 0:\n",
    "        return 0\n",
    "    else: \n",
    "        return log(n/m,2)\n",
    "\n",
    "def get_mutual_infs(docs, ministeries):\n",
    "    \"\"\"\n",
    "    Returns a nested dict matching terms and classes\n",
    "    with their mutual information.\n",
    "    \"\"\"\n",
    "    count_dict = get_occurence_counts(docs, ministeries)\n",
    "    N = len(docs)\n",
    "    \n",
    "    classes = set(ministeries)\n",
    "    result = {}\n",
    "    for mini in classes:\n",
    "        docs_in_class = ministeries.count(mini)\n",
    "        result[mini] = {}\n",
    "        for term in count_dict:\n",
    "            result[mini][term] = mutual_inf(term, mini, count_dict, N, docs_in_class)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. For each class, show the top 10 words as in Figure 13.7 in MRS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "financiën\n",
      "\t belastingdienst: 0.03819\n",
      "\t fiscale: 0.01861\n",
      "\t financin: 0.01731\n",
      "\t bank: 0.01354\n",
      "\t belastingplichtigen: 0.01128\n",
      "\t inkomstenbelasting: 0.01111\n",
      "\t afm: 0.01054\n",
      "\t markten: 0.01003\n",
      "\t nederlandsche: 0.00989\n",
      "\t belasting: 0.00960\n",
      "economische zaken\n",
      "\t economische: 0.01388\n",
      "\t energiebedrijven: 0.01165\n",
      "\t dte: 0.01122\n",
      "\t nma: 0.01112\n",
      "\t bedrijven: 0.01090\n",
      "\t energie: 0.01088\n",
      "\t liberalisering: 0.00785\n",
      "\t elektriciteit: 0.00785\n",
      "\t markt: 0.00746\n",
      "\t gas: 0.00726\n",
      "sociale zaken en werkgelegenheid\n",
      "\t sociale: 0.04516\n",
      "\t werknemers: 0.03115\n",
      "\t werk: 0.02464\n",
      "\t uwv: 0.02388\n",
      "\t werkgevers: 0.02190\n",
      "\t uitkering: 0.01831\n",
      "\t uitvoeringsinstituut: 0.01725\n",
      "\t arbeid: 0.01651\n",
      "\t werkgelegenheid: 0.01622\n",
      "\t wao: 0.01512\n",
      "justitie\n",
      "\t justitie: 0.05817\n",
      "\t politie: 0.05193\n",
      "\t openbaar: 0.02382\n",
      "\t asielzoekers: 0.01763\n",
      "\t vervolging: 0.01688\n",
      "\t uitzetting: 0.01545\n",
      "\t ind: 0.01505\n",
      "\t zaak: 0.01448\n",
      "\t strafbare: 0.01419\n",
      "\t verdachte: 0.01315\n",
      "landbouw, natuurbeheer en visserij\n",
      "\t dieren: 0.02446\n",
      "\t agrarisch: 0.01720\n",
      "\t landbouw: 0.01691\n",
      "\t boeren: 0.01175\n",
      "\t aid: 0.01015\n",
      "\t natuurbeheer: 0.00970\n",
      "\t visserij: 0.00896\n",
      "\t natuur: 0.00889\n",
      "\t faunawet: 0.00864\n",
      "\t vlees: 0.00856\n",
      "onderwijs, cultuur en wetenschappen\n",
      "\t onderwijs: 0.09209\n",
      "\t scholen: 0.06436\n",
      "\t leerlingen: 0.05363\n",
      "\t school: 0.04602\n",
      "\t studenten: 0.02464\n",
      "\t cultuur: 0.01789\n",
      "\t voortgezet: 0.01589\n",
      "\t onderwijsinspectie: 0.01456\n",
      "\t docenten: 0.01322\n",
      "\t basisscholen: 0.01291\n",
      "volkshuisvesting, ruimtelijke ordening en milieubeheer\n",
      "\t woningen: 0.02058\n",
      "\t ordening: 0.01975\n",
      "\t volkshuisvesting: 0.01943\n",
      "\t ruimtelijke: 0.01936\n",
      "\t afval: 0.01875\n",
      "\t milieubeheer: 0.01709\n",
      "\t vrom: 0.01678\n",
      "\t milieu: 0.01327\n",
      "\t huurders: 0.01297\n",
      "\t gemeente: 0.01211\n",
      "buitenlandse zaken\n",
      "\t regering: 0.09850\n",
      "\t eu: 0.04061\n",
      "\t autoriteiten: 0.03746\n",
      "\t nederlandse: 0.03118\n",
      "\t bilateraal: 0.03082\n",
      "\t mensenrechten: 0.02902\n",
      "\t president: 0.02708\n",
      "\t unie: 0.02706\n",
      "\t vn: 0.02630\n",
      "\t politieke: 0.02450\n",
      "verkeer en waterstaat\n",
      "\t verkeer: 0.03298\n",
      "\t ns: 0.03283\n",
      "\t waterstaat: 0.03059\n",
      "\t rijkswaterstaat: 0.02726\n",
      "\t vervoer: 0.02341\n",
      "\t spoor: 0.01894\n",
      "\t aanleg: 0.01656\n",
      "\t prorail: 0.01650\n",
      "\t treinen: 0.01600\n",
      "\t trein: 0.01520\n",
      "volksgezondheid, welzijn en sport\n",
      "\t zorg: 0.06255\n",
      "\t patinten: 0.06116\n",
      "\t ziekenhuizen: 0.04835\n",
      "\t gezondheidszorg: 0.04658\n",
      "\t medisch: 0.03117\n",
      "\t zorgverzekeraars: 0.03116\n",
      "\t artsen: 0.02683\n",
      "\t ziekenhuis: 0.02663\n",
      "\t huisartsen: 0.02556\n",
      "\t medische: 0.02302\n"
     ]
    }
   ],
   "source": [
    "MI = get_mutual_infs(train_docs, train_mins)\n",
    "\n",
    "def top_mutual_infs(mutual_infs,n):\n",
    "    \"\"\"\n",
    "    Given the mutual information and interger n, \n",
    "    return a dict of top n words based on the mutual information.\n",
    "    \"\"\"\n",
    "    ministeries = {}\n",
    "    for mini in mutual_infs:\n",
    "        lst = [(mutual_infs[mini][term],term) for term in mutual_infs[mini]]\n",
    "        ministeries[mini] = sorted(lst, reverse=True)[:n]\n",
    "        \n",
    "    return ministeries\n",
    "        \n",
    "    \n",
    "top_10 = top_mutual_infs(MI,10)\n",
    "# print the top mutual informations using the following code\n",
    "for item in top_10:\n",
    "    print item\n",
    "    for i in top_10[item]: \n",
    "        print \"\\t %s: %.5f\" % (i[1], i[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Evaluate your classifiers using Precision, Recall and F1 and do feature selection per class. \n",
    "\n",
    "Give a table in which you show these values for using the top 10, top 100 terms and all terms, for all of your 10 classes. Thus do feature selection per class, and use for each class the top n best features for that class. Also show the microaverage(s) for all 10 classes together. If you like you can also present this in a figure like MRS.13.8. Then compute the F1 measure for the same number of terms as in that figure.\n",
    "\n",
    "#### Implementing evaluation measures\n",
    "\n",
    "To apply feature selection we first have to implement the evaluation measures that we are going use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def test2_NB(classes, vocab, prior, cond_prob, documents, ministeries):\n",
    "    \"\"\"\n",
    "    Test the Naive Bayes classifier on given test data\n",
    "    and return a list of classifications for the test documents\n",
    "    \"\"\"\n",
    "    pred_classes = []\n",
    "    \n",
    "    for i, doc in enumerate(documents):\n",
    "        score = apply_multinomial_NB(classes, vocab, prior, cond_prob, doc)\n",
    "        pred_classes.append(score)\n",
    "\n",
    "    return pred_classes\n",
    "\n",
    "pred_classes = test2_NB(classes, vocab, prior, cond_prob, test_docs, test_mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def distribution(classes, test_mins, pred_classes):\n",
    "    \"\"\"\n",
    "    Calculate the True Positives, False Positives, and False Negatives per class \n",
    "    and return a dict with a counter as value for each class. \n",
    "    \"\"\"\n",
    "    data = { c : Counter() for c in classes }\n",
    "    \n",
    "    for c in classes:\n",
    "        for i, ministerie in enumerate(test_mins):\n",
    "            if (c == ministerie and c == pred_classes[i]):\n",
    "                data[c]['TP'] += 1\n",
    "            elif (c == pred_classes[i] and c != ministerie):\n",
    "                data[c]['FP'] += 1\n",
    "            elif (c != pred_classes[i] and c == ministerie):\n",
    "                data[c]['FN'] += 1\n",
    "            \n",
    "    return data\n",
    "\n",
    "def recall(data):\n",
    "    \"\"\"\n",
    "    Calculates the recall per class from the data \n",
    "    structure returned by the distribution function\n",
    "    \"\"\"\n",
    "    recall = { c : float(data[c]['TP'])/float(data[c]['TP'] + data[c]['FN'])\n",
    "               for c in data\n",
    "               if data[c]['TP'] + data[c]['FN'] != 0 }\n",
    "    \n",
    "    for c in data:\n",
    "        if not c in recall:\n",
    "            recall[c] = 0\n",
    "    \n",
    "    return recall\n",
    "\n",
    "def precision(data):\n",
    "    \"\"\"\n",
    "    Calculates the precision per class from the data \n",
    "    structure returned by the distribution function.\n",
    "    \"\"\"\n",
    "    precision = { c : float(data[c]['TP'])/float(data[c]['TP'] + data[c]['FP'])\n",
    "                  for c in data\n",
    "                  if data[c]['TP'] + data[c]['FP'] != 0 }\n",
    "    \n",
    "    for c in data:\n",
    "        if not c in precision:\n",
    "            precision[c] = 0\n",
    "    return precision\n",
    "\n",
    "def f1(data):\n",
    "    \"\"\"\n",
    "    Calculate the micro-averaged F1 value from the data \n",
    "    structure returned by the distribution function.\n",
    "    \"\"\"\n",
    "    TP = sum([data[c]['TP'] for c in classes])\n",
    "    FP = sum([data[c]['FP'] for c in classes])\n",
    "    FN = sum([data[c]['FN'] for c in classes])\n",
    "\n",
    "    p = float(TP)/float(TP+FP)\n",
    "    r = float(TP)/float(TP+FN)\n",
    "    return 2.0*((p*r)/(p+r)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature selection\n",
    "\n",
    "Now that we have implemented our evaluation measures, we can start the process of feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjQAAAF5CAYAAACIpbAsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nklR6/x/ZoP7hKFJJ3wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x139e05e50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "MI = get_mutual_infs(train_docs, train_mins)\n",
    "\n",
    "def feature_selection(vocabulary,MI, nr_words):\n",
    "    \"\"\"\n",
    "    Using Mutual Information select the top nr_words \n",
    "    for classification and return a new vocabulary using those words. \n",
    "    \"\"\"\n",
    "    top = top_mutual_infs(MI, nr_words)\n",
    "    return set(word[1] for c in top for word in top[c])\n",
    "\n",
    "def diff_vocab_run(classes, vocab, prior, cond_prob, test_docs, test_mins, MI, word_nr_list):\n",
    "    \"\"\"\n",
    "    Using a train NB classifier (prior, cond_prob) and the mutual information of the training set,\n",
    "    run the classifier on the test set (test_docs, test_mins) with the top number of words per class \n",
    "    from the word_nr_list. The classifier is also run with all the words in the vocab.\n",
    "    The F1 values are calculated and plotted. \n",
    "    Values in word_nr_list cannot be higher than the words in vocab. \n",
    "    \"\"\"\n",
    "    F_values = []\n",
    "    precisions = dict()\n",
    "    recalls = dict()\n",
    "    nr_words = []\n",
    "    \n",
    "    # Using the number of words per class calculate the predicted classes and calculate the accuracy, precision and F1.\n",
    "    for i in word_nr_list:\n",
    "        vocab_top = feature_selection(vocab, MI,i)\n",
    "        pred_classes_top = test2_NB(classes, vocab_top, prior, cond_prob, test_docs, test_mins)\n",
    "        data_top = distribution(classes, test_mins, pred_classes_top)\n",
    "        nr_words.append(len(vocab_top))\n",
    "        F_values.append(f1(data_top))\n",
    "        recalls[i] = recall(data_top)\n",
    "        precisions[i] = precision(data_top)\n",
    "      \n",
    "\n",
    "    # evaluate the classifier using all words\n",
    "    pred_classes = test2_NB(classes, vocab, prior, cond_prob, test_docs, test_mins)\n",
    "    data_all = distribution(classes,test_mins, pred_classes)\n",
    "    F_values.append(f1(data_all))\n",
    "    recalls['all'] = recall(data_all)\n",
    "    precisions['all'] = precision(data_all)\n",
    "    nr_words.append(len(vocab))\n",
    "    plt.plot(nr_words,F_values,'-o')\n",
    "    plt.xlabel('Vocabulary size')\n",
    "    plt.ylabel('F1')\n",
    "    plt.show()\n",
    "    \n",
    "    return (precisions, recalls)\n",
    "        \n",
    "\n",
    "precisions, recalls = diff_vocab_run(classes, vocab, prior, cond_prob, \n",
    "                                     test_docs, test_mins, MI, [10,100,1000,10000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph is the graph created by the function above. It shows the F1 for several sizes of the vocabulary. The F1 was calculated by microaveraging as stated in the assignment. Below the graph the table is shown with the precision and recall information.\n",
    "\n",
    "<img width=75% src='http://oi64.tinypic.com/3465255.jpg' />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named tabulate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-596c72284583>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtabulate\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtabulate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m precision_table = [ [str(i), c, str(precisions[i][c])]\n\u001b[1;32m      4\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprecisions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                     for c in precisions[i] ]\n",
      "\u001b[0;31mImportError\u001b[0m: No module named tabulate"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "precision_table = [ [str(i), c, str(precisions[i][c])]\n",
    "                    for i in sorted(precisions)\n",
    "                    for c in precisions[i] ]\n",
    "\n",
    "recall_table = [ [str(i), c, str(recalls[i][c])]\n",
    "                    for i in sorted(recalls)\n",
    "                    for c in recalls[i] ]\n",
    "\n",
    "print tabulate(precision_table, headers=[\"Words per class\", \"Class\", \"Precision\"])\n",
    "print tabulate(recall_table, headers=[\"Words per class\", \"Class\", \"Recall\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 7. Reflect and report briefly about your choices in this process and about the obtained results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We would like to explain our design choices and discuss the results for each stage of the project. \n",
    "\n",
    "#### Stage 1: Preparing the data\n",
    "\n",
    "This stage includes extracting the data from a given website into the desired format and doing natural language processing in order to make the data useful for the process of prediction. \n",
    "We used the sample code to read the csv file with pandas. We extracted the 'vraag' column as documents and the 'ministerie' column as the corresponding classes. After a short inspection of the data we decided that for the ministries we would change all capital letters and that we would remove all abbreviations between brackets. We also normalized the documents by removing capital letters and punctuation. Furthermore, we seperated the words in the documents to allow for easier analysis later on. We could do further normalization in the future, by, for example, taking care of different abbreviations and documents that belong to more than one ministry.\n",
    "\n",
    "#### Stage 2: Training and testing\n",
    "\n",
    "Due to the normalization, the top 10 classes cover more than half of the data. We used 75% of the data for training and tested the NB classifier on the other 25% of the samples. Before looking into feature selection we wanted to have an idea about whether our classifier could classify at all. Therefore we initially trained and tested using all features. The accuracies differed for different classes, but it looked like our code was working properly.\n",
    "\n",
    "#### Stage 3: Feature selection\n",
    "\n",
    "To obtain the best results one should apply feature selection. In this classification problem the words from the documents are the features we work with. To determine which features are the most useful we use the Mutual Information (MI) measure. When implemented correctly, the top 10 of words with the highest MI for each class makes sense in the intuitive sense. And indeed, we checked the top 10 for each class, and the results look very good. For example, for the class \"landbouw, natuurbeheer en visserij\" the words with the highest MI are \"dieren\", \"agrarisch\", \"landbouw\", which sounds like a good result intuitively. We used the MI to select features to work with for each class. To determine the number of features to use, we created the graph shown above, which uses the F1 measure as a goodness measure for the classifier. Clearly we obtain the best result when using a maximum of 1000 words per class. \n",
    "\n",
    "#### Stage 4: Evaluation\n",
    "\n",
    "In the process of testing your classifier and selecting features to work with, you need a measure that expresses how well your algorithm works. We implemented code for calculating the precision, recall and the F1 measure of the NB classifier. The precision and recall are calculated per class to see for which the class (ministery) the classifier is working best. The F1 measure is micro-averaged over all the classes to give a single measure for the \"goodness\" of the classifier. This measure is especially important for the feature selection to show how many feautures are beneficial for the classification. From the different words per class we find that 1000 obtains the best results. For future research more number of words should be tested to find an optimum, but one has to watch out for overfitting with this and possibly use another data set/split for such feature selection. Another thing to keep in mind during feature selection is that using Mutual Information rare words present in the train data only in a single class can affect the classification. If this word is actually part of the class, it might be a great feature, but if it is by accident, the word should be removed. However, it is very hard to make this distinction without human/expert judgement.\n",
    "\n",
    "The precision and recall seem to differ significantly per class. Looking at the 1000 words per class the highest recall is 0.81 (landbouw, natuurbeheer en visserij) and the lowest 0.51 (economische zaken). The difference is even bigger at the precision, there the lowest is 0.44 (economische zaken) and the highest 0.85 (volksgezondheid, welzijn en sport). These differences are quite big and could possibly be improved in future iterations of this simple Naive Bayes classifier.   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
