{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment  week 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook made by   (If not filled in correctly: 0 pts for assignment)\n",
    "\n",
    "__Name(s)__: Adriaan de Vries, Verna Dankers\n",
    "\n",
    "__Student id(s)__ : 10795227, 10761225\n",
    "\n",
    "__Email(s)__: adriaan.devries@student.uva.nl, verna.dankers@student.uva.nl\n",
    "\n",
    "### Pledge (taken from [Coursera's Honor Code](https://www.coursera.org/about/terms/honorcode) )\n",
    "\n",
    "\n",
    "\n",
    "Put here a selfie with your photo where you hold a signed paper with the following text: (if this is team work, put two selfies here). The link must be to some place on the web, not to a local file. **Assignments without the selfies will not be graded and receive 0 points.**\n",
    "\n",
    "> My answers to homework, quizzes and exams will be my own work (except for assignments that explicitly permit collaboration).\n",
    "\n",
    ">I will not make solutions to homework, quizzes or exams available to anyone else. This includes both solutions written by me, as well as any official solutions provided by the course staff.\n",
    "\n",
    ">I will not engage in any other activities that will dishonestly improve my results or dishonestly improve/hurt the results of others.\n",
    "\n",
    "<img width=30% src='http://oi63.tinypic.com/2q3srh3.jpg'/>\n",
    "<img src=\"http://i63.tinypic.com/2itj1ic.jpg\" border=\"0\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "#### Book\n",
    "\n",
    "* Chap 6: 6.8 to 6.13, 6.14 to 6.17. \n",
    " \n",
    "#### Programming\n",
    "\n",
    "* [MakeInvertedIndex](MakeInvertedIndex1516.html): Exercises 8.1 to 8.6 about Boolean Search Engines and Exercises 9.1 to 9.5 about Ranked Retrieval \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Book\n",
    "\n",
    "#### 6.8 \n",
    "\n",
    "$idf_t = log( \\frac{N}{df_t} )$\n",
    "\n",
    "For $df_t > 0$, the $idf_t$ will always stay between the lower and the upper bound. The lower bound for $idf_t$ will be reached when the $df_t = N$ and the upper bound will be reached when $df_t = 1$. So the $idf_t$ can only be one of the numbers from the finite set of the number between these two bounds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.9 \n",
    "\n",
    "The $idf_t$ of a term that appears in every document: $log(\\frac{N}{N}) = log(1) = 0$\n",
    "\n",
    "Such a word will act like a stop word when the $tf-idf$ weighting will be used, since its weight becomes zero. It will not be able to affect the search results, due to its zero weight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.10\n",
    "\n",
    "$tf idf_{t,d} = tf_{t,d} \\times idf_t$\n",
    "\n",
    "##### car\n",
    "$tf idf_{car,d1} = 27 \\times log(\\frac{3}{3}) = 0 $  \n",
    "\n",
    "$tf idf_{car,d2} = 4 \\times log(\\frac{3}{3}) = 0 $  \n",
    "\n",
    "$tf idf_{car,d3} = 24 \\times log(\\frac{3}{3}) = 0 $  \n",
    "\n",
    "##### auto\n",
    "\n",
    "$tf idf_{auto,d1} = 3 \\times log(\\frac{3}{2}) = 0.53 $  \n",
    "\n",
    "$tf idf_{auto,d2} = 33 \\times log(\\frac{3}{2}) = 5.81 $\n",
    "\n",
    "$tf idf_{auto,d3} = 0 \\times log(\\frac{3}{2}) = 0 $  \n",
    "\n",
    "##### insurance\n",
    "\n",
    "$tf idf_{insurance,d1} = 0 \\times log(\\frac{3}{2}) = 0 $  \n",
    "\n",
    "$tf idf_{insurance,d2} = 33 \\times log(\\frac{3}{2}) = 5.81 $  \n",
    "\n",
    "$tf idf_{insurance,d3} = 29 \\times log(\\frac{3}{2}) = 5.11 $  \n",
    "\n",
    "##### best\n",
    "\n",
    "$tf idf_{best,d1} = 14 \\times log(\\frac{3}{2}) = 2.47 $  \n",
    "\n",
    "$tf idf_{best,d2} = 0 \\times log(\\frac{3}{2}) = 0 $  \n",
    "\n",
    "$tf idf_{best,d3} = 17 \\times log(\\frac{3}{2}) = 2.99 $  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.11\n",
    "\n",
    "Yes, obivously. Dependend on the number of documents in the collection (N) the upper bound for $tf-idf$ is $log(\\frac{N}{1})$, which can definitely exceed one. This is illustrated by exercise 6.10. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 6.12\n",
    "\n",
    "The base of the logarithm will affect the score calculation, by lowering the $idf$ value when using a larger number for the base with the same $N$ and $df$. $log_{n+1}(\\frac{N}{df})$ will always result in a lower number than $log_{n}(\\frac{N}{df})$. Therefore the $tf idf$ score will be lower as well.\n",
    "\n",
    "When comparing relative scores of different documents, they will seem more alike when a higher number has been used for the base of the log in the $idf$ calculation. Similarly they will seem less alike when a lower number has been used for the base of the log.\n",
    "\n",
    "Increasing the number for the base \"compresses\" the $idf$ and therefore the $tf-idf$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.14\n",
    "\n",
    "When applying stemming the terms being indexed are stemmed before indexing. Jealous and jealousy would be considered as a single dimension. Then:\n",
    "\n",
    "$tf_{jealous, d} :=  tf_{jealous,d} + tf_{jealousy,d}$\n",
    "\n",
    "$idf_{jealous} := idf_{jealous} + idf_{jealousy}$\n",
    "\n",
    "ADRIAAN HOE KUNNEN WE DIT GENERALISEREN?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.15\n",
    "\n",
    "Vectors from exercise 6.10:\n",
    "\n",
    "1. car: $[0,0,0]^T$\n",
    "2. auto: $[0.53,5.81,0]^T$ \n",
    "3. insurance: $[0,5.81,5.11]^T$ \n",
    "4. best:$[2.47,0,2.99]^T$\n",
    "\n",
    "Normalized vectors:\n",
    "\n",
    "1. car: We can not normalize the zero vector.\n",
    "2. auto: $\\frac{[0.53,5.81,0]^T}{\\sqrt{0.53^2 + 5.81^2}} = [0.09, 0.997, 0]^T$\n",
    "3. insurance: $\\frac{[0,5.81,5.11]^T}{\\sqrt{5.81^2 + 5.11^2}} = [0, 0.75, 0.66]^T$ \n",
    "4. best: $\\frac{[2.47,0,2.99]^T}{\\sqrt{2.47^2 + 2.99^2}} = [0.64, 0, 0.77]^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Programming\n",
    "\n",
    "For the programming part, we'll first copy and paste what we need from Assignment 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from collections import Counter, defaultdict\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "CORPUS = \"Shakespeare\"\n",
    "\n",
    "def normalize(word, keep_capitals):\n",
    "    \"\"\"\n",
    "    Normalize capital letters and punctuation\n",
    "    \"\"\"\n",
    "    if (keep_capitals and not word == word.upper()) or not keep_capitals:\n",
    "        word = word.lower()\n",
    "    word = re.sub(r'[^\\w\\s]', '', word)\n",
    "    return word\n",
    "\n",
    "def prepare_data(folder, new_folder, keep_capitals, byte):\n",
    "    \"\"\"\n",
    "    Prepare a text corpus for the process of\n",
    "    making an inverted index.\n",
    "    \"\"\"\n",
    "    # Create a directory for the processed files\n",
    "    if not os.path.exists(new_folder):\n",
    "        os.makedirs(new_folder)\n",
    "\n",
    "    # Prepare each file one by one\n",
    "    for filename in os.listdir(folder):\n",
    "        if not os.path.exists(os.path.join(new_folder, filename[:-4] + \".pkl\")):\n",
    "            # Strip xml tags\n",
    "            if byte:\n",
    "                xmlfile = BeautifulSoup(open(os.path.join(folder, filename), 'rb'),\"xml\")\n",
    "            else:\n",
    "                xmlfile = BeautifulSoup(open(os.path.join(folder, filename), 'r'),\"xml\")\n",
    "            textfile = xmlfile.get_text()\n",
    "\n",
    "            # Split the text on whitespaces\n",
    "            beautifile = WhitespaceTokenizer().tokenize(textfile)\n",
    "\n",
    "            # Normalize text for capital letters, non-alphabetical tokens and whitespace\n",
    "            finalfile = [ normalize(word, keep_capitals)\n",
    "                          for word \n",
    "                          in beautifile \n",
    "                         ]\n",
    "            new_filename = os.path.join(new_folder, filename[:-4] + \".pkl\")\n",
    "            savefile = open(new_filename, 'wb')\n",
    "            pickle.dump(finalfile, savefile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def index_collection(folder):\n",
    "    \"\"\"\n",
    "    Create an inverted index given a folder \n",
    "    containig a text corpus.\n",
    "    \"\"\"\n",
    "    index= defaultdict(Counter) # initialize MyIndex\n",
    "    for filename in os.listdir(folder):\n",
    "        text = pickle.load(open(os.path.join(folder,filename),'rb'))\n",
    "        \n",
    "        # Update the index with each token\n",
    "        for w in text:    \n",
    "            index[w][filename[:-4]]+=1   \n",
    "    if '' in index.keys():\n",
    "        del index['']\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prepare_data(CORPUS, CORPUS + \"New\", 0, 0)\n",
    "index_org = index_collection(CORPUS + \"New\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_doc_freq(index, word):\n",
    "    \"\"\"\n",
    "    Calculate in how many documents a given \n",
    "    term occurs, based on the inverted index.\n",
    "    \"\"\"\n",
    "    return len(index[word])\n",
    "    \n",
    "def calculate_corp_freq(index, word):\n",
    "    \"\"\"\n",
    "    Calculate how often a term occurs in a \n",
    "    corpus, based on the inverted index.\n",
    "    \"\"\"\n",
    "    corpus_frequency = 0\n",
    "    \n",
    "    for document in index[word]:\n",
    "        corpus_frequency += index[word][document]\n",
    "        \n",
    "    return corpus_frequency\n",
    "\n",
    "def add_frequencies(index):\n",
    "    for word in index:\n",
    "        original_counter = index[word]\n",
    "        document_frequency = calculate_doc_freq(index, word)\n",
    "        corpus_frequency = calculate_corp_freq(index, word)\n",
    "        index[word] = {\"counter\" : original_counter, \n",
    "                       \"doc_freq\" : document_frequency, \n",
    "                       \"corp_freq\" : corpus_frequency\n",
    "                      }\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "index_with_freq = add_frequencies(index_org)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making a list of all documents\n",
    "\n",
    "Will be useful for negation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def make_doc_set(index_with_frequencies):\n",
    "    \n",
    "    doc_list = []\n",
    "    for token in index_with_freq:\n",
    "        doc_list.extend(list(index_with_freq[token]['counter'].elements()))\n",
    "    return set(doc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tempest', 'lear', 'two_gent', 'hamlet', 'rich_iii', 'r_and_j', 'hen_v', 'troilus', 'cymbelin', 'all_well', 'lll', 't_night', 'taming', 'com_err', 'hen_vi_2', 'm_wives', 'm_for_m', 'john', 'timon', 'as_you', 'hen_viii', 'titus', 'hen_vi_1', 'win_tale', 'dream', 'pericles', 'a_and_c', 'hen_vi_3', 'rich_ii', 'merchant', 'j_caesar', 'much_ado', 'hen_iv_2', 'coriolan', 'othello', 'macbeth', 'hen_iv_1'}\n"
     ]
    }
   ],
   "source": [
    "doc_set = make_doc_set(index_with_freq)\n",
    "print(doc_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On to the actual exercises:\n",
    "\n",
    "##### 8.1, a search engine for negation (so useful)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def docs_with_token(token):\n",
    "    if token in index_with_freq:\n",
    "        return set(index_with_freq[token]['counter'].elements())\n",
    "    else:\n",
    "        return set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def negation_search(word):\n",
    "    return doc_set.difference(docs_with_token(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lear', 'two_gent', 'rich_iii', 'r_and_j', 'hen_v', 'troilus', 'cymbelin', 'all_well', 'lll', 't_night', 'taming', 'com_err', 'hen_vi_2', 'm_wives', 'john', 'timon', 'as_you', 'hen_viii', 'titus', 'hen_vi_1', 'win_tale', 'dream', 'pericles', 'a_and_c', 'hen_vi_3', 'rich_ii', 'merchant', 'j_caesar', 'much_ado', 'hen_iv_2', 'coriolan', 'macbeth', 'hen_iv_1'}\n",
      "{'tempest', 'hamlet', 'othello', 'm_for_m'}\n",
      "{'hen_viii', 'pericles', 'two_gent', 'lll', 't_night', 'hamlet', 'm_wives', 'hen_v', 'troilus', 'j_caesar', 'much_ado', 'timon', 'othello', 'as_you', 'dream', 'hen_iv_1'}\n"
     ]
    }
   ],
   "source": [
    "print (negation_search('test'))\n",
    "print (docs_with_token('test'))\n",
    "print (negation_search('despair'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.2 Disjunction search\n",
    "\n",
    "Input as a list of words. Output is unordered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def disjunction_search(query):\n",
    "    if len(query) ==0:\n",
    "        return set()\n",
    "    result = docs_with_token(query[0])\n",
    "    for word in query[1:]:\n",
    "        result = result.union(docs_with_token(query[1]))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a_and_c',\n",
       " 'all_well',\n",
       " 'com_err',\n",
       " 'coriolan',\n",
       " 'cymbelin',\n",
       " 'hamlet',\n",
       " 'hen_iv_2',\n",
       " 'hen_vi_1',\n",
       " 'hen_vi_2',\n",
       " 'hen_vi_3',\n",
       " 'john',\n",
       " 'lear',\n",
       " 'm_for_m',\n",
       " 'macbeth',\n",
       " 'merchant',\n",
       " 'othello',\n",
       " 'r_and_j',\n",
       " 'rich_ii',\n",
       " 'rich_iii',\n",
       " 'taming',\n",
       " 'tempest',\n",
       " 'titus',\n",
       " 'win_tale'}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disjunction_search(['test', 'despair'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8.4 Conjunctive search\n",
    "\n",
    "Input is, again, a list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conjunction_search(query):\n",
    "    if len(query) ==0:\n",
    "        return set()\n",
    "    result = docs_with_token(query[0])\n",
    "    for word in query[1:]:\n",
    "        result = result.intersection(docs_with_token(query[1]))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tempest', 'm_for_m'}\n"
     ]
    }
   ],
   "source": [
    "print(conjunction_search(['test', 'despair']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8.5 Ordering of conjunctive search\n",
    "\n",
    "Conjunctive search could be ordered by the amount of times terms appear in a document. Simply taking the sum is an option, or one could choose to do it relative to the rarity of the tokens. So the ranking could be the total amount of times a word from the query appears in the document, divided by the total amount of times the word appears in the corpus. Taking the sum over all the words in the query in this way would give a fairly good ranking.\n",
    "\n",
    "Each document score could then possibibly be divided by the document length, to punish longer documents as they will obviously on average have more terms in them. One could, however, argue that these longer documents are better documents, so they should be ranked higher and not punished."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8.6 Relaxing the search\n",
    "\n",
    "Instead of removing documents that fail to contain a search word, we can add some form of counter to them. Only when the counter exceeds a specified value will it be deleted. Then what's left is to decide how many terms the document should at least match. We think three quarters would be a good threshold, so it would not be allowed to miss more than floor(N/4) where N is the amount of words in the query. This is easily adjustable to any other value.\n",
    "\n",
    "As for ranking, the documents could be ranked the same way as specified in 8.5, except the documents with the lowest counter would be placed highest. So the documents that match the most terms will end up at the top of the ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.3.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
