{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment  week 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook made by   (If not filled in correctly: 0 pts for assignment)\n",
    "\n",
    "__Name(s)__: \n",
    "\n",
    "__Student id(s)__ : \n",
    "\n",
    "### Pledge (taken from [Coursera's Honor Code](https://www.coursera.org/about/terms/honorcode) )\n",
    "\n",
    "\n",
    "\n",
    "Put here a selfie with your photo where you hold a signed paper with the following text: (if this is team work, put two selfies here). The link must be to some place on the web, not to a local file. **Assignments without the selfies will not be graded and receive 0 points.**\n",
    "\n",
    "> My answers to homework, quizzes and exams will be my own work (except for assignments that explicitly permit collaboration).\n",
    "\n",
    ">I will not make solutions to homework, quizzes or exams available to anyone else. This includes both solutions written by me, as well as any official solutions provided by the course staff.\n",
    "\n",
    ">I will not engage in any other activities that will dishonestly improve my results or dishonestly improve/hurt the results of others.\n",
    "\n",
    "<img width=30% src='http://oi63.tinypic.com/2q3srh3.jpg'/>\n",
    "<img src=\"http://i63.tinypic.com/2itj1ic.jpg\" border=\"0\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "#### Book\n",
    "\n",
    "* Chap 6: 6.8 to 6.13, 6.14 to 6.17. \n",
    " \n",
    "#### Programming\n",
    "\n",
    "* [MakeInvertedIndex](MakeInvertedIndex1516.html): Exercises 8.1 to 8.6 about Boolean Search Engines and Exercises 9.1 to 9.5 about Ranked Retrieval \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Programming\n",
    "\n",
    "For the programming part, we'll first copy and paste what we need from Assignment 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from collections import Counter, defaultdict\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "CORPUS = \"Shakespeare\"\n",
    "\n",
    "def normalize(word, keep_capitals):\n",
    "    \"\"\"\n",
    "    Normalize capital letters and punctuation\n",
    "    \"\"\"\n",
    "    if (keep_capitals and not word == word.upper()) or not keep_capitals:\n",
    "        word = word.lower()\n",
    "    word = re.sub(r'[^\\w\\s]', '', word)\n",
    "    return word\n",
    "\n",
    "def prepare_data(folder, new_folder, keep_capitals, byte):\n",
    "    \"\"\"\n",
    "    Prepare a text corpus for the process of\n",
    "    making an inverted index.\n",
    "    \"\"\"\n",
    "    # Create a directory for the processed files\n",
    "    if not os.path.exists(new_folder):\n",
    "        os.makedirs(new_folder)\n",
    "\n",
    "    # Prepare each file one by one\n",
    "    for filename in os.listdir(folder):\n",
    "        if not os.path.exists(os.path.join(new_folder, filename[:-4] + \".pkl\")):\n",
    "            # Strip xml tags\n",
    "            if byte:\n",
    "                xmlfile = BeautifulSoup(open(os.path.join(folder, filename), 'rb'),\"xml\")\n",
    "            else:\n",
    "                xmlfile = BeautifulSoup(open(os.path.join(folder, filename), 'r'),\"xml\")\n",
    "            textfile = xmlfile.get_text()\n",
    "\n",
    "            # Split the text on whitespaces\n",
    "            beautifile = WhitespaceTokenizer().tokenize(textfile)\n",
    "\n",
    "            # Normalize text for capital letters, non-alphabetical tokens and whitespace\n",
    "            finalfile = [ normalize(word, keep_capitals)\n",
    "                          for word \n",
    "                          in beautifile \n",
    "                         ]\n",
    "            new_filename = os.path.join(new_folder, filename[:-4] + \".pkl\")\n",
    "            savefile = open(new_filename, 'wb')\n",
    "            pickle.dump(finalfile, savefile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def index_collection(folder):\n",
    "    \"\"\"\n",
    "    Create an inverted index given a folder \n",
    "    containig a text corpus.\n",
    "    \"\"\"\n",
    "    index= defaultdict(Counter) # initialize MyIndex\n",
    "    for filename in os.listdir(folder):\n",
    "        text = pickle.load(open(os.path.join(folder,filename),'rb'))\n",
    "        \n",
    "        # Update the index with each token\n",
    "        for w in text:    \n",
    "            index[w][filename[:-4]]+=1   \n",
    "    if '' in index.keys():\n",
    "        del index['']\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prepare_data(CORPUS, CORPUS + \"New\", 0, 0)\n",
    "index_org = index_collection(CORPUS + \"New\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_doc_freq(index, word):\n",
    "    \"\"\"\n",
    "    Calculate in how many documents a given \n",
    "    term occurs, based on the inverted index.\n",
    "    \"\"\"\n",
    "    return len(index[word])\n",
    "    \n",
    "def calculate_corp_freq(index, word):\n",
    "    \"\"\"\n",
    "    Calculate how often a term occurs in a \n",
    "    corpus, based on the inverted index.\n",
    "    \"\"\"\n",
    "    corpus_frequency = 0\n",
    "    \n",
    "    for document in index[word]:\n",
    "        corpus_frequency += index[word][document]\n",
    "        \n",
    "    return corpus_frequency\n",
    "\n",
    "def add_frequencies(index):\n",
    "    for word in index:\n",
    "        original_counter = index[word]\n",
    "        document_frequency = calculate_doc_freq(index, word)\n",
    "        corpus_frequency = calculate_corp_freq(index, word)\n",
    "        index[word] = {\"counter\" : original_counter, \n",
    "                       \"doc_freq\" : document_frequency, \n",
    "                       \"corp_freq\" : corpus_frequency\n",
    "                      }\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "index_with_freq = add_frequencies(index_org)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### making a list of all documents\n",
    "\n",
    "Will be useful for negation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def make_doc_set(index_with_frequencies):\n",
    "    \n",
    "    doc_list = []\n",
    "    for token in index_with_freq:\n",
    "        doc_list.extend(list(index_with_freq[token]['counter'].elements()))\n",
    "    return set(doc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tempest', 'lear', 'two_gent', 'hamlet', 'rich_iii', 'r_and_j', 'hen_v', 'troilus', 'cymbelin', 'all_well', 'lll', 't_night', 'taming', 'com_err', 'hen_vi_2', 'm_wives', 'm_for_m', 'john', 'timon', 'as_you', 'hen_viii', 'titus', 'hen_vi_1', 'win_tale', 'dream', 'pericles', 'a_and_c', 'hen_vi_3', 'rich_ii', 'merchant', 'j_caesar', 'much_ado', 'hen_iv_2', 'coriolan', 'othello', 'macbeth', 'hen_iv_1'}\n"
     ]
    }
   ],
   "source": [
    "doc_set = make_doc_set(index_with_freq)\n",
    "print(doc_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On to the actual exercises:\n",
    "\n",
    "##### 8.1, a search engine for negation (so useful)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def docs_with_token(token):\n",
    "    if token in index_with_freq:\n",
    "        return set(index_with_freq[token]['counter'].elements())\n",
    "    else:\n",
    "        return set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def negation_search(word):\n",
    "    return doc_set.difference(docs_with_token(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lear', 'two_gent', 'rich_iii', 'r_and_j', 'hen_v', 'troilus', 'cymbelin', 'all_well', 'lll', 't_night', 'taming', 'com_err', 'hen_vi_2', 'm_wives', 'john', 'timon', 'as_you', 'hen_viii', 'titus', 'hen_vi_1', 'win_tale', 'dream', 'pericles', 'a_and_c', 'hen_vi_3', 'rich_ii', 'merchant', 'j_caesar', 'much_ado', 'hen_iv_2', 'coriolan', 'macbeth', 'hen_iv_1'}\n",
      "{'tempest', 'hamlet', 'othello', 'm_for_m'}\n",
      "{'hen_viii', 'pericles', 'two_gent', 'lll', 't_night', 'hamlet', 'm_wives', 'hen_v', 'troilus', 'j_caesar', 'much_ado', 'timon', 'othello', 'as_you', 'dream', 'hen_iv_1'}\n"
     ]
    }
   ],
   "source": [
    "print (negation_search('test'))\n",
    "print (docs_with_token('test'))\n",
    "print (negation_search('despair'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8.2 Disjunction search\n",
    "\n",
    "Input as a list of words. Output is unordered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def disjunction_search(query):\n",
    "    if len(query) ==0:\n",
    "        return set()\n",
    "    result = docs_with_token(query[0])\n",
    "    for word in query[1:]:\n",
    "        result = result.union(docs_with_token(query[1]))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a_and_c',\n",
       " 'all_well',\n",
       " 'com_err',\n",
       " 'coriolan',\n",
       " 'cymbelin',\n",
       " 'hamlet',\n",
       " 'hen_iv_2',\n",
       " 'hen_vi_1',\n",
       " 'hen_vi_2',\n",
       " 'hen_vi_3',\n",
       " 'john',\n",
       " 'lear',\n",
       " 'm_for_m',\n",
       " 'macbeth',\n",
       " 'merchant',\n",
       " 'othello',\n",
       " 'r_and_j',\n",
       " 'rich_ii',\n",
       " 'rich_iii',\n",
       " 'taming',\n",
       " 'tempest',\n",
       " 'titus',\n",
       " 'win_tale'}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disjunction_search(['test', 'despair'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
