{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment  week 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook made by   (If not filled in correctly: 0 pts for assignment)\n",
    "\n",
    "__Name(s)__: Adriaan de Vries, Verna Dankers\n",
    "\n",
    "__Student id(s)__ : 10795227, 10761225\n",
    "\n",
    "__Email(s)__: adriaan.devries@student.uva.nl, verna.dankers@student.uva.nl\n",
    "\n",
    "### Pledge (taken from [Coursera's Honor Code](https://www.coursera.org/about/terms/honorcode) )\n",
    "\n",
    "\n",
    "\n",
    "Put here a selfie with your photo where you hold a signed paper with the following text: (if this is team work, put two selfies here). The link must be to some place on the web, not to a local file. **Assignments without the selfies will not be graded and receive 0 points.**\n",
    "\n",
    "> My answers to homework, quizzes and exams will be my own work (except for assignments that explicitly permit collaboration).\n",
    "\n",
    ">I will not make solutions to homework, quizzes or exams available to anyone else. This includes both solutions written by me, as well as any official solutions provided by the course staff.\n",
    "\n",
    ">I will not engage in any other activities that will dishonestly improve my results or dishonestly improve/hurt the results of others.\n",
    "\n",
    "<img width=30% src='http://oi63.tinypic.com/2q3srh3.jpg'/>\n",
    "<img src=\"http://i63.tinypic.com/2itj1ic.jpg\" border=\"0\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "#### Book\n",
    "\n",
    "* Chap 6: 6.8 to 6.13, 6.14 to 6.17. \n",
    " \n",
    "#### Programming\n",
    "\n",
    "* [MakeInvertedIndex](MakeInvertedIndex1516.html): Exercises 8.1 to 8.6 about Boolean Search Engines and Exercises 9.1 to 9.5 about Ranked Retrieval \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Book\n",
    "\n",
    "#### 6.8 \n",
    "\n",
    "$idf_t = log( \\frac{N}{df_t} )$\n",
    "\n",
    "For $df_t > 0$, the $idf_t$ will always stay between the lower and the upper bound. The lower bound for $idf_t$ will be reached when the $df_t = N$ and the upper bound will be reached when $df_t = 1$. So the $idf_t$ can only be one of the numbers from the finite set of the number between these two bounds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.9 \n",
    "\n",
    "The $idf_t$ of a term that appears in every document: $log(\\frac{N}{N}) = log(1) = 0$\n",
    "\n",
    "Such a word will act like a stop word when the $tf-idf$ weighting will be used, since its weight becomes zero. It will not be able to affect the search results, due to its zero weight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.10\n",
    "\n",
    "$tf idf_{t,d} = tf_{t,d} \\times idf_t$\n",
    "\n",
    "##### car\n",
    "$tf idf_{car,d1} = 27 \\times log(\\frac{3}{3}) = 0 $  \n",
    "\n",
    "$tf idf_{car,d2} = 4 \\times log(\\frac{3}{3}) = 0 $  \n",
    "\n",
    "$tf idf_{car,d3} = 24 \\times log(\\frac{3}{3}) = 0 $  \n",
    "\n",
    "##### auto\n",
    "\n",
    "$tf idf_{auto,d1} = 3 \\times log(\\frac{3}{2}) = 0.53 $  \n",
    "\n",
    "$tf idf_{auto,d2} = 33 \\times log(\\frac{3}{2}) = 5.81 $\n",
    "\n",
    "$tf idf_{auto,d3} = 0 \\times log(\\frac{3}{2}) = 0 $  \n",
    "\n",
    "##### insurance\n",
    "\n",
    "$tf idf_{insurance,d1} = 0 \\times log(\\frac{3}{2}) = 0 $  \n",
    "\n",
    "$tf idf_{insurance,d2} = 33 \\times log(\\frac{3}{2}) = 5.81 $  \n",
    "\n",
    "$tf idf_{insurance,d3} = 29 \\times log(\\frac{3}{2}) = 5.11 $  \n",
    "\n",
    "##### best\n",
    "\n",
    "$tf idf_{best,d1} = 14 \\times log(\\frac{3}{2}) = 2.47 $  \n",
    "\n",
    "$tf idf_{best,d2} = 0 \\times log(\\frac{3}{2}) = 0 $  \n",
    "\n",
    "$tf idf_{best,d3} = 17 \\times log(\\frac{3}{2}) = 2.99 $  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.11\n",
    "\n",
    "Yes, obivously. Dependend on the number of documents in the collection (N) the upper bound for $tf-idf$ is $log(\\frac{N}{1})$, which can definitely exceed one. This is illustrated by exercise 6.10. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 6.12\n",
    "\n",
    "Altering the base of a logarithm is equivalent to multiplying the result by a constant. This will obviously alter the specific values for the idf, and thus also for $Score(q,d)$ (6.9), but the ratios between scores of different documents will remain the same. For sintance, if in base A the score of Doc1 is 2, and it's 4 in base B, than Doc2 will also have a score twice as high in base B as opposed to A. It is, however, important to calculate the score for each document with a logarithm of the same base. If we change bases during the process, some documents could get unfairly high or low rankings. There is, however, absolutely no reason to change the base of the logarithm mid-computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.13\n",
    "\n",
    "As stated before, the result of a logarithm in base two for some value, is just a constant times the logarithm with another base of that value. There really cannot be an easy approximation that works in base 2 but not in other bases. However, if what is meant in the question is an easy approximation of the idf in base 2 given the idf in base 10, then one could get this value by multiplying the idf by $\\frac{log(10)}{log(2)}$. For these logarithms, any base gives literally the same exact solution, as they're divided by eachother. Also, this is not so much an approximation as it is an exact way to calculate the idf in base 2 from base 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.15\n",
    "\n",
    "Both the term frequency and the inverse document frequency would have to see jealous and jealousy as the same term. For the term frequency, it's sufficient to simply add the term frequencies of the two previously seperate terms. In the calculation of the inverse document frequency, we can't simply add the document frequencies of the two terms as there may be overlap. Instead, we'd have to take the sets of documents that the terms appear in, take the union of those sets, and the cardinality of this set is the new document frequency of jealous(y) that can be used normally for the calculation of the idf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.15\n",
    "\n",
    "Vectors from exercise 6.10:\n",
    "\n",
    "1. d1 = $[0,0.53,0,2.47]^T$\n",
    "2. d2 = $[0,5.81,5.81,0]^T$\n",
    "3. d3 = $[0,0,5.11,2.99]^T$\n",
    "\n",
    "\n",
    "Normalized vectors:\n",
    "\n",
    "1. d1 = $\\frac{ [0,0.53,0,2.47]^T }{\\sqrt{0.53^2 + 2.47^2}} = \\frac{[0,0.53,0,2.47]^T}{2.53} = [0,0.21, 0  , 0.98]^T$\n",
    "2. d2 = $\\frac{ [0,5.81,5.81,0]^T }{\\sqrt{5.81^2 + 5.81^2}} = \\frac{[0,5.81,5.81,0]^T}{8.22} = [0,0.71,0.71, 0   ]^T$\n",
    "3. d3 = $\\frac{ [0,0,5.11,2.99]^T }{\\sqrt{5.11^2 + 2.99^2}} = \\frac{[0,0,5.11,2.99]^T}{5.92} = [0,   0,0.86, 0.51]^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.16\n",
    "\n",
    "1. length d1: $\\sqrt{0.21^2 + 0.98^2} = 1.002$\n",
    "2. length d2: $\\sqrt{0.71^2 + 0.71^2} = 1.004$\n",
    "3. length d3: $\\sqrt{0.86^2 + 0.51^2} = 1.000$ \n",
    "\n",
    "Now the vectors have length 1 since we divided the original vectors by their length in exercise 6.15."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.17\n",
    "\n",
    "1. The weight of a term is 1 if present in the query, 0 otherwise.\n",
    "\n",
    "query: car insurance\n",
    "\n",
    "1. d1 = $[0,0.21, 0  , 0.98]^T \\cdot [1,0,1,0]^T = 0$\n",
    "2. d2 = $[0,0.71,0.71, 0   ]^T \\cdot [1,0,1,0]^T = 0.71$\n",
    "3. d3 = $[0,   0,0.86, 0.51]^T \\cdot [1,0,1,0]^T = 0.86$\n",
    "\n",
    "2. Euclidean normalized idf.\n",
    "\n",
    "1. d1 = $[0,0.21, 0  , 0.98]^T \\cdot [0,0,0.18,0]^T = 0$\n",
    "2. d2 = $[0,0.71,0.71, 0   ]^T \\cdot [0,0,0.18,0]^T = 0.13$\n",
    "3. d3 = $[0,   0,0.86, 0.51]^T \\cdot [0,0,0.18,0]^T = 0.15$\n",
    "\n",
    "If we normalize the query, the dot product will be $\\frac{1}{0.18}$ times as high for all documents.\n",
    "\n",
    "For both ways of calculating the third document is the best hit for the query, followed by the second document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Programming\n",
    "\n",
    "For the programming part, we'll first copy and paste what we need from Assignment 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from collections import Counter, defaultdict\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "CORPUS = \"Shakespeare\"\n",
    "\n",
    "def normalize(word, keep_capitals):\n",
    "    \"\"\"\n",
    "    Normalize capital letters and punctuation\n",
    "    \"\"\"\n",
    "    if (keep_capitals and not word == word.upper()) or not keep_capitals:\n",
    "        word = word.lower()\n",
    "    word = re.sub(r'[^\\w\\s]', '', word)\n",
    "    return word\n",
    "\n",
    "def prepare_data(folder, new_folder, keep_capitals, byte):\n",
    "    \"\"\"\n",
    "    Prepare a text corpus for the process of\n",
    "    making an inverted index.\n",
    "    \"\"\"\n",
    "    # Create a directory for the processed files\n",
    "    if not os.path.exists(new_folder):\n",
    "        os.makedirs(new_folder)\n",
    "\n",
    "    # Prepare each file one by one\n",
    "    for filename in os.listdir(folder):\n",
    "        if not os.path.exists(os.path.join(new_folder, filename[:-4] + \".pkl\")):\n",
    "            # Strip xml tags\n",
    "            if byte:\n",
    "                xmlfile = BeautifulSoup(open(os.path.join(folder, filename), 'rb'),\"xml\")\n",
    "            else:\n",
    "                xmlfile = BeautifulSoup(open(os.path.join(folder, filename), 'r'),\"xml\")\n",
    "            textfile = xmlfile.get_text()\n",
    "\n",
    "            # Split the text on whitespaces\n",
    "            beautifile = WhitespaceTokenizer().tokenize(textfile)\n",
    "\n",
    "            # Normalize text for capital letters, non-alphabetical tokens and whitespace\n",
    "            finalfile = [ normalize(word, keep_capitals)\n",
    "                          for word \n",
    "                          in beautifile \n",
    "                         ]\n",
    "            new_filename = os.path.join(new_folder, filename[:-4] + \".pkl\")\n",
    "            savefile = open(new_filename, 'wb')\n",
    "            pickle.dump(finalfile, savefile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def index_collection(folder):\n",
    "    \"\"\"\n",
    "    Create an inverted index given a folder \n",
    "    containig a text corpus.\n",
    "    \"\"\"\n",
    "    index= defaultdict(Counter) # initialize MyIndex\n",
    "    for filename in os.listdir(folder):\n",
    "        text = pickle.load(open(os.path.join(folder,filename),'rb'))\n",
    "        \n",
    "        # Update the index with each token\n",
    "        for w in text:    \n",
    "            index[w][filename[:-4]]+=1   \n",
    "    if '' in index.keys():\n",
    "        del index['']\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_doc_freq(index, word):\n",
    "    \"\"\"\n",
    "    Calculate in how many documents a given \n",
    "    term occurs, based on the inverted index.\n",
    "    \"\"\"\n",
    "    return len(index[word])\n",
    "    \n",
    "def calculate_corp_freq(index, word):\n",
    "    \"\"\"\n",
    "    Calculate how often a term occurs in a \n",
    "    corpus, based on the inverted index.\n",
    "    \"\"\"\n",
    "    corpus_frequency = 0\n",
    "    \n",
    "    for document in index[word]:\n",
    "        corpus_frequency += index[word][document]\n",
    "        \n",
    "    return corpus_frequency\n",
    "\n",
    "def add_frequencies(index):\n",
    "    for word in index:\n",
    "        original_counter = index[word]\n",
    "        document_frequency = calculate_doc_freq(index, word)\n",
    "        corpus_frequency = calculate_corp_freq(index, word)\n",
    "        index[word] = {\"counter\" : original_counter, \n",
    "                       \"doc_freq\" : document_frequency, \n",
    "                       \"corp_freq\" : corpus_frequency\n",
    "                      }\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prepare_data(CORPUS, CORPUS + \"New\", 0, 0)\n",
    "index_org = index_collection(CORPUS + \"New\")\n",
    "index_with_freq = add_frequencies(index_org)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making a list of all documents\n",
    "\n",
    "Will be useful for negation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_doc_set(index):\n",
    "    \"\"\"\n",
    "    Create a set containing all documents that\n",
    "    are present in a given index.\n",
    "    \"\"\"\n",
    "    doc_list = []\n",
    "    for token in index:\n",
    "        doc_list.extend(list(index[token]['counter'].elements()))\n",
    "    return set(doc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'titus', 'win_tale', 'a_and_c', 'troilus', 'othello', 'merchant', 'hen_vi_1', 'much_ado', 'hen_v', 'hen_viii', 'hen_vi_2', 'taming', 'tempest', 'j_caesar', 'coriolan', 'com_err', 'hen_iv_2', 'hen_iv_1', 'rich_ii', 'john', 'as_you', 'timon', 'lll', 'hen_vi_3', 'r_and_j', 'rich_iii', 'two_gent', 't_night', 'dream', 'cymbelin', 'm_wives', 'all_well', 'm_for_m', 'hamlet', 'macbeth', 'lear', 'pericles'}\n"
     ]
    }
   ],
   "source": [
    "doc_set = make_doc_set(index_with_freq)\n",
    "print(doc_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On to the actual exercises:\n",
    "\n",
    "We start with the code and the results for the Shakespeare corpus.\n",
    "\n",
    "#### 8.1 Create a search engine for negation (very useful)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def docs_with_token(index, token):\n",
    "    \"\"\"\n",
    "    Return all documents that contain a\n",
    "    certain token.\n",
    "    \"\"\"\n",
    "    if token in index:\n",
    "        return set(index[token]['counter'].elements())\n",
    "    else:\n",
    "        return set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def negation_search(doc_set, index, word):\n",
    "    \"\"\"\n",
    "    Find all documents form an index that\n",
    "    do not contain a certain word.\n",
    "    \"\"\"\n",
    "    return doc_set.difference(docs_with_token(index, word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'titus', 'win_tale', 'a_and_c', 'troilus', 'lll', 'r_and_j', 'j_caesar', 'hen_v', 'hen_viii', 'timon', 'taming', 'much_ado', 'coriolan', 'com_err', 'hen_iv_2', 'hen_iv_1', 'rich_ii', 'john', 'as_you', 'merchant', 'hen_vi_1', 'hen_vi_3', 'hen_vi_2', 'rich_iii', 'two_gent', 't_night', 'dream', 'cymbelin', 'm_wives', 'all_well', 'macbeth', 'lear', 'pericles'}\n",
      "{'hamlet', 'othello', 'm_for_m', 'tempest'}\n",
      "{'as_you', 'timon', 'lll', 'pericles', 'troilus', 'othello', 'dream', 't_night', 'j_caesar', 'two_gent', 'hamlet', 'hen_iv_1', 'much_ado', 'hen_v', 'm_wives', 'hen_viii'}\n"
     ]
    }
   ],
   "source": [
    "print (negation_search(doc_set, index_with_freq, 'test'))\n",
    "print (docs_with_token(index_with_freq, 'test'))\n",
    "print (negation_search(doc_set, index_with_freq, 'despair'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.2 Disjunction search\n",
    "\n",
    "Input as a list of words. Output is unordered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def disjunction_search(query, index):\n",
    "    \"\"\"\n",
    "    Return documents that contain at least\n",
    "    one of the words from the query.\n",
    "    \"\"\"\n",
    "    if len(query) ==0:\n",
    "        return set()\n",
    "    result = docs_with_token(index, query[0])\n",
    "    for word in query[1:]:\n",
    "        result = result.union(docs_with_token(index, word))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'titus', 'merchant', 'hen_vi_1', 'win_tale', 'hen_vi_2', 'a_and_c', 'othello', 'rich_iii', 'r_and_j', 'lear', 'cymbelin', 'hen_vi_3', 'taming', 'tempest', 'm_for_m', 'coriolan', 'com_err', 'hamlet', 'hen_iv_2', 'macbeth', 'rich_ii', 'john', 'all_well'}\n"
     ]
    }
   ],
   "source": [
    "print(disjunction_search(['test', 'despair'], index_with_freq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8.3.1 Ordering by number of terms matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def num_of_terms(query, index, doc):\n",
    "    \"\"\"\n",
    "    Return how many words from the query\n",
    "    a given document contains.\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    for token in query:\n",
    "        if doc in docs_with_token(index, token):\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "def disj_search_simple_order(query, index):\n",
    "    \"\"\"\n",
    "    Return documents that contain words from the query\n",
    "    ordered by the number of query words they contained.\n",
    "    \"\"\"\n",
    "    set_of_docs = disjunction_search(query, index)\n",
    "    return sorted(set_of_docs, reverse = True, key = lambda x: num_of_terms(query,index,x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['titus', 'a_and_c', 'taming', 'tempest', 'com_err', 'hen_iv_2', 'john', 'merchant', 'rich_iii', 'm_for_m', 'hamlet', 'win_tale', 'troilus', 'othello', 'hen_vi_1', 'hen_v', 'hen_vi_2', 'j_caesar', 'coriolan', 'hen_iv_1', 'as_you', 'lll', 'hen_vi_3', 'r_and_j', 'two_gent', 'dream', 'm_wives', 'macbeth', 'lear', 'much_ado', 'timon', 'rich_ii', 't_night', 'cymbelin', 'all_well']\n",
      "merchant: 3\n",
      "r_and_j: 2\n",
      "all_well: 1\n"
     ]
    }
   ],
   "source": [
    "print(disj_search_simple_order(['test', 'despair', 'fat', 'beast'], index_with_freq))\n",
    "print('merchant:', num_of_terms(['test', 'despair', 'fat', 'beast'], index_with_freq, 'merchant'))\n",
    "print('r_and_j:', num_of_terms(['test', 'despair', 'fat', 'beast'], index_with_freq, 'r_and_j'))\n",
    "print('all_well:', num_of_terms(['test', 'despair', 'fat', 'beast'], index_with_freq, 'all_well'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8.3.2 Ordering by term frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def term_frequency(query, index, doc):\n",
    "    \"\"\"\n",
    "    Count how often the terms from a query\n",
    "    appear in a given document.\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    for token in query:\n",
    "        if doc in index[token]['counter']:\n",
    "            count += index[token]['counter'][doc]\n",
    "    return count\n",
    "\n",
    "def disj_search_term_freq_order(query, index):\n",
    "    \"\"\"\n",
    "    Return documents that contain words from the query\n",
    "    ordered by how frequent those terms appeared.\n",
    "    \"\"\"\n",
    "    set_of_docs = disjunction_search(query, index)\n",
    "    return sorted(set_of_docs, reverse = True, key = lambda x: term_frequency(query,index,x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hen_iv_1 19\n",
      "rich_iii 17\n",
      "m_wives 16\n",
      "hamlet 13\n",
      "timon 11\n",
      "hen_iv_2 8\n",
      "lll 8\n",
      "taming 7\n",
      "com_err 7\n",
      "rich_ii 6\n",
      "othello 5\n",
      "as_you 5\n",
      "hen_vi_3 5\n",
      "r_and_j 5\n",
      "lear 5\n",
      "troilus 4\n",
      "tempest 4\n",
      "john 4\n",
      "dream 4\n",
      "m_for_m 4\n",
      "titus 3\n",
      "win_tale 3\n",
      "a_and_c 3\n",
      "hen_v 3\n",
      "hen_vi_2 3\n",
      "j_caesar 3\n",
      "merchant 3\n",
      "macbeth 3\n",
      "hen_vi_1 2\n",
      "much_ado 2\n",
      "coriolan 2\n",
      "two_gent 2\n",
      "cymbelin 2\n",
      "all_well 2\n",
      "t_night 1\n"
     ]
    }
   ],
   "source": [
    "for doc in disj_search_term_freq_order(['test', 'despair', 'fat', 'beast'], index_with_freq):\n",
    "    print (doc, term_frequency(['test', 'despair', 'fat', 'beast'], index_with_freq, doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8.3.3 Ordering by number of matched terms and term frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tf_and_matchedNo_combo(query, index, doc):\n",
    "    \"\"\"\n",
    "    Returns both the sum of term frequencies for the query and document, \n",
    "    as the number of terms from the query that match the document\n",
    "    \"\"\"\n",
    "    return([num_of_terms(query, index, doc), term_frequency(query, index, doc)])\n",
    "\n",
    "def disj_search_combo_order(query, index):\n",
    "    \"\"\"\n",
    "    Return documents that contain words from the query\n",
    "    ordered by how many words from the query they\n",
    "    contained and on how frequent those terms appeared.\n",
    "    \"\"\"\n",
    "    set_of_docs = disjunction_search(query, index)\n",
    "    return sorted(set_of_docs, \n",
    "                  reverse = True, \n",
    "                  key = lambda x: tf_and_matchedNo_combo(query, index, x)\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rich_iii [3, 17]\n",
      "hamlet [3, 13]\n",
      "hen_iv_2 [3, 8]\n",
      "taming [3, 7]\n",
      "com_err [3, 7]\n",
      "tempest [3, 4]\n",
      "john [3, 4]\n",
      "m_for_m [3, 4]\n",
      "titus [3, 3]\n",
      "a_and_c [3, 3]\n",
      "merchant [3, 3]\n",
      "hen_iv_1 [2, 19]\n",
      "m_wives [2, 16]\n",
      "lll [2, 8]\n",
      "othello [2, 5]\n",
      "as_you [2, 5]\n",
      "hen_vi_3 [2, 5]\n",
      "r_and_j [2, 5]\n",
      "lear [2, 5]\n",
      "troilus [2, 4]\n",
      "dream [2, 4]\n",
      "win_tale [2, 3]\n",
      "hen_v [2, 3]\n",
      "hen_vi_2 [2, 3]\n",
      "j_caesar [2, 3]\n",
      "macbeth [2, 3]\n",
      "hen_vi_1 [2, 2]\n",
      "coriolan [2, 2]\n",
      "two_gent [2, 2]\n",
      "timon [1, 11]\n",
      "rich_ii [1, 6]\n",
      "much_ado [1, 2]\n",
      "cymbelin [1, 2]\n",
      "all_well [1, 2]\n",
      "t_night [1, 1]\n"
     ]
    }
   ],
   "source": [
    "for doc in disj_search_combo_order(['test', 'despair', 'fat', 'beast'], \n",
    "                                   index_with_freq):\n",
    "    print (doc, tf_and_matchedNo_combo(['test', 'despair', 'fat', 'beast'], \n",
    "                                       index_with_freq, doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8.4 Conjunctive search\n",
    "\n",
    "Input is, again, a list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conjunction_search(query, index):\n",
    "    \"\"\"\n",
    "    Return all documents that contain all words\n",
    "    from the given query.\n",
    "    \"\"\"\n",
    "    result = \"\"\n",
    "    \n",
    "    if len(query)==0:\n",
    "        return set()\n",
    "    \n",
    "    for word in query:\n",
    "        if result:\n",
    "            result = result.intersection(docs_with_token(index, word))\n",
    "        else: \n",
    "            result = docs_with_token(index, word)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a_and_c', 'lear', 'hen_vi_1', 'hen_vi_2', 'all_well', 'tempest', 'rich_iii', 'coriolan', 'hen_iv_2', 'cymbelin', 'macbeth', 'rich_ii'}\n"
     ]
    }
   ],
   "source": [
    "print(conjunction_search(['despair','terrible'], index_with_freq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8.5 Ordering of conjunctive search\n",
    "\n",
    "Conjunctive search could be ordered by the amount of times terms appear in a document. Simply taking the sum is an option, or one could choose to do it relative to the rarity of the tokens. So the ranking could be the total amount of times a word from the query appears in the document, divided by the total amount of times the word appears in the corpus. Taking the sum over all the words in the query in this way would give a fairly good ranking.\n",
    "\n",
    "Each document score could then possibibly be divided by the document length, to punish longer documents as they will obviously on average have more terms in them. One could, however, argue that these longer documents are better documents, so they should be ranked higher and not punished."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8.6 Relaxing the search\n",
    "\n",
    "Instead of removing documents that fail to contain a search word, we can add some form of counter to them. Only when the counter exceeds a specified value will it be deleted. Then what's left is to decide how many terms the document should at least match. We think three quarters would be a good threshold, so it would not be allowed to miss more than floor(N/4) where N is the amount of words in the query. This is easily adjustable to any other value.\n",
    "\n",
    "As for ranking, the documents could be ranked the same way as specified in 8.5, except the documents with the lowest counter would be placed highest. So the documents that match the most terms will end up at the top of the ranking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 9.1 TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "def TF_IDF(term, doc_set, index):\n",
    "    \"\"\"\n",
    "    Return the tf-idf weight of a given term.\n",
    "    \"\"\"\n",
    "    rated_docs = []\n",
    "    IDF = log(len(doc_set) / len(docs_with_token(index, term)))\n",
    "    for doc in doc_set:\n",
    "        if doc in index[term]['counter']:\n",
    "            rated_docs.append([index[term]['counter'][doc]*IDF,doc])\n",
    "        else:\n",
    "            rated_docs.append([0,doc])\n",
    "    return sorted(rated_docs, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[7.9295366488912205, 'rich_iii'],\n",
       " [3.398372849524809, 'rich_ii'],\n",
       " [2.265581899683206, 'hen_vi_3'],\n",
       " [1.6991864247624044, 'lear'],\n",
       " [1.132790949841603, 'win_tale'],\n",
       " [1.132790949841603, 'r_and_j'],\n",
       " [1.132790949841603, 'macbeth'],\n",
       " [1.132790949841603, 'john'],\n",
       " [1.132790949841603, 'hen_vi_2'],\n",
       " [1.132790949841603, 'cymbelin'],\n",
       " [1.132790949841603, 'all_well'],\n",
       " [0.5663954749208014, 'titus'],\n",
       " [0.5663954749208014, 'tempest'],\n",
       " [0.5663954749208014, 'taming'],\n",
       " [0.5663954749208014, 'merchant'],\n",
       " [0.5663954749208014, 'm_for_m'],\n",
       " [0.5663954749208014, 'hen_vi_1'],\n",
       " [0.5663954749208014, 'hen_iv_2'],\n",
       " [0.5663954749208014, 'coriolan'],\n",
       " [0.5663954749208014, 'com_err'],\n",
       " [0.5663954749208014, 'a_and_c'],\n",
       " [0, 'two_gent'],\n",
       " [0, 'troilus'],\n",
       " [0, 'timon'],\n",
       " [0, 't_night'],\n",
       " [0, 'pericles'],\n",
       " [0, 'othello'],\n",
       " [0, 'much_ado'],\n",
       " [0, 'm_wives'],\n",
       " [0, 'lll'],\n",
       " [0, 'j_caesar'],\n",
       " [0, 'hen_viii'],\n",
       " [0, 'hen_v'],\n",
       " [0, 'hen_iv_1'],\n",
       " [0, 'hamlet'],\n",
       " [0, 'dream'],\n",
       " [0, 'as_you']]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TF_IDF('despair', doc_set, index_with_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 9.2 TF-IDF scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getKey(list_):\n",
    "    return list_[1]\n",
    "\n",
    "def TF_IDF_search(query, doc_set, index):\n",
    "    \"\"\"\n",
    "    For all documents calculate how they\n",
    "    score compared to the query, by tf-idf weighting.\n",
    "    \"\"\"\n",
    "    dic = {}\n",
    "    for doc in doc_set:\n",
    "        dic[doc]=0\n",
    "    for term in query:\n",
    "        TFIDF = TF_IDF(term, doc_set, index)\n",
    "        for elem in TFIDF:\n",
    "            if elem[0]==0:\n",
    "                break\n",
    "            else:\n",
    "                dic[elem[1]]+=elem[0]\n",
    "    return sorted(dic.items(), reverse=True, key=getKey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('hen_iv_1', 9.793193846706075)\n",
      "('rich_iii', 8.983176289127522)\n",
      "('m_wives', 7.448460637417568)\n",
      "('hamlet', 7.084728896995646)\n",
      "('lll', 4.20839040710336)\n",
      "('hen_iv_2', 4.20839040710336)\n",
      "('rich_ii', 3.398372849524809)\n",
      "('tempest', 3.278263191760636)\n",
      "('m_for_m', 3.278263191760636)\n",
      "('othello', 3.199111882155336)\n",
      "('taming', 2.9964481476564573)\n",
      "('com_err', 2.9964481476564573)\n",
      "('timon', 2.679842909235256)\n",
      "('hen_vi_3', 2.5092039823409564)\n",
      "('lear', 2.1864305900779053)\n",
      "('troilus', 1.9428085074201549)\n",
      "('john', 1.9428085074201549)\n",
      "('as_you', 1.8636571978148544)\n",
      "('r_and_j', 1.8636571978148544)\n",
      "('win_tale', 1.6991864247624044)\n",
      "('titus', 1.3764130324993533)\n",
      "('merchant', 1.3764130324993533)\n",
      "('hen_vi_2', 1.3764130324993533)\n",
      "('a_and_c', 1.3764130324993533)\n",
      "('macbeth', 1.3764130324993533)\n",
      "('dream', 1.2972617228940528)\n",
      "('hen_vi_1', 1.132790949841603)\n",
      "('cymbelin', 1.132790949841603)\n",
      "('all_well', 1.132790949841603)\n",
      "('j_caesar', 1.0536396402363024)\n",
      "('hen_v', 1.0536396402363024)\n",
      "('two_gent', 0.810017557578552)\n",
      "('coriolan', 0.810017557578552)\n",
      "('t_night', 0.5663954749208014)\n",
      "('much_ado', 0.487244165315501)\n",
      "('pericles', 0)\n",
      "('hen_viii', 0)\n"
     ]
    }
   ],
   "source": [
    "returnval = TF_IDF_search(['test', 'despair', 'fat', 'beast'], doc_set, index_with_freq)\n",
    "for elem in returnval:\n",
    "    print (elem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.3 Euclidean length of docs\n",
    "\n",
    "Function will return a dict, as that seems useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "def euclidean_lengths(doc_set, index):\n",
    "    \"\"\"\n",
    "    Compute for each document its Euclidean length.\n",
    "    \"\"\"\n",
    "    dic={}\n",
    "    for doc in doc_set:\n",
    "        dic[str(doc)] = 0\n",
    "    for key in index:\n",
    "        for doc in index[key]['counter']:\n",
    "            dic[str(doc)] += index[key]['counter'][str(doc)]**2\n",
    "    for key in dic:\n",
    "        dic[key] = sqrt(dic[key])\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'titus': 1752.0644965297367, 'merchant': 1917.1390142605726, 'hen_vi_1': 1822.1355602698718, 'win_tale': 2088.30337834329, 'hen_vi_2': 2173.9512414035416, 'a_and_c': 2062.9069295535364, 'troilus': 2117.9992917845843, 'othello': 2297.0711786969077, 'rich_iii': 2529.0595089874814, 'two_gent': 1501.6590824817729, 'pericles': 1527.190230455918, 'lll': 1852.4702426759789, 'as_you': 1955.6847905529153, 'j_caesar': 1714.6008281813001, 'hen_v': 2310.7985632676855, 'macbeth': 1460.7621298486622, 'hen_viii': 2117.5880146997433, 'cymbelin': 2259.077909236421, 't_night': 1792.8792485831275, 'timon': 1492.1628597442036, 'hen_vi_3': 2102.5584415183325, 'r_and_j': 1986.4090716667602, 'm_wives': 2020.1821700034875, 'taming': 1843.2970460563322, 'dream': 1387.939840194812, 'tempest': 1355.7916506602332, 'much_ado': 1900.3881182537425, 'm_for_m': 1917.6334373388465, 'coriolan': 2372.4721283926606, 'com_err': 1462.9962405966735, 'hamlet': 2665.07917330799, 'hen_iv_2': 2310.8329667026997, 'hen_iv_1': 2173.7451552562457, 'rich_ii': 1956.5198695643242, 'john': 1771.741798344217, 'lear': 2162.9942209816463, 'all_well': 2011.9169962997976}\n"
     ]
    }
   ],
   "source": [
    "print(euclidean_lengths(doc_set, index_with_freq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.4 Cosine Similarity\n",
    "\n",
    "*If a word appears multiple times in the query, the length of the query vector is not just the square root of the length of the query. However, as this would change all results by multiplying them with a constant, it would not interfere with the ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "def cosine_similarity_search(query, doc_set, index):\n",
    "    \"\"\"\n",
    "    Calculate the cosine similarity for each document\n",
    "    in the set to the given query.\n",
    "    \"\"\"\n",
    "    lengths = euclidean_lengths(doc_set, index)\n",
    "    \n",
    "    # for querylength we assume the query to consist of unique words, \n",
    "    # for justification check markdown box above\n",
    "    querylength = sqrt(len(query))\n",
    "    \n",
    "    TF_IDF_results = TF_IDF_search(query, doc_set, index)\n",
    "    final_result = []\n",
    "    for result in TF_IDF_results:\n",
    "        final_result.append([result[0],result[1] / (lengths[result[0]] * querylength)])\n",
    "    \n",
    "    # sort again\n",
    "    return sorted(final_result, reverse=True, key=getKey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hen_iv_1 0.00225260855050684\n",
      "m_wives 0.001843512121831248\n",
      "rich_iii 0.001775991481656351\n",
      "hamlet 0.0013291779411194436\n",
      "tempest 0.0012089848724780876\n",
      "lll 0.001135886102284738\n",
      "com_err 0.0010240792370164859\n",
      "hen_iv_2 0.0009105786674638504\n",
      "timon 0.000897972661541332\n",
      "rich_ii 0.0008684738914206772\n",
      "m_for_m 0.0008547679467641046\n",
      "taming 0.0008127957873277263\n",
      "othello 0.0006963458319933607\n",
      "hen_vi_3 0.0005967025536110594\n",
      "john 0.0005482764218905397\n",
      "lear 0.0005054175755230688\n",
      "as_you 0.0004764717726541089\n",
      "macbeth 0.00047112839399866986\n",
      "r_and_j 0.000469102065731882\n",
      "dream 0.00046733355629879767\n",
      "troilus 0.00045864238835112707\n",
      "win_tale 0.00040683418951091696\n",
      "titus 0.00039279747841063344\n",
      "merchant 0.0003589758025529063\n",
      "a_and_c 0.0003336100656749558\n",
      "hen_vi_2 0.00031656943501886376\n",
      "hen_vi_1 0.00031084156814156793\n",
      "j_caesar 0.0003072550832003015\n",
      "all_well 0.00028152029927799383\n",
      "two_gent 0.0002697075411550291\n",
      "cymbelin 0.00025071976163595254\n",
      "hen_v 0.00022798171527905862\n",
      "coriolan 0.00017071171203333278\n",
      "t_night 0.00015795694979692894\n",
      "much_ado 0.0001281959618236372\n",
      "pericles 0.0\n",
      "hen_viii 0.0\n"
     ]
    }
   ],
   "source": [
    "result = cosine_similarity_search(['test', 'despair', 'fat', 'beast'], \n",
    "                                  doc_set, index_with_freq)\n",
    "for [doc,similarity] in result:\n",
    "    print (doc, similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.5 Okapi BM25\n",
    "\n",
    "We will need to do some calculations with the values of k1, b, the term frequency, the document vector length and the average document vector length. All of these are calculable except for k1 and b, which are free variables. The wikipedia page suggests putting b=0.75 and k1 being between 1.2 and 2, so we'll take b=0.75 and k1=1.6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "def Okapi_BM25_search(query, doc_set, index):\n",
    "    \"\"\"\n",
    "    Performs a search using the given query, using the \n",
    "    Okapi BM25 algorithm to rank the results\n",
    "    \"\"\"\n",
    "    k1 = 1.6\n",
    "    b = 0.75\n",
    "    \n",
    "    lengths = euclidean_lengths(doc_set, index)\n",
    "    querylength = sqrt(len(query))\n",
    "    \n",
    "    # calculating the average doc length\n",
    "    s = 0\n",
    "    counter = 0\n",
    "    for key in lengths:\n",
    "        s += lengths[key]\n",
    "        counter += 1\n",
    "    avg_len = s/counter\n",
    "    \n",
    "    result = []\n",
    "    for doc in doc_set:\n",
    "        subresult = [str(doc), 0]\n",
    "        for term in query:\n",
    "            if doc in index[term]['counter']:\n",
    "                tf = index[term]['counter'][str(doc)]\n",
    "                val = (tf * (k1+1)) / (tf + k1 * (1-b+b*(lengths[str(doc)]/avg_len)))\n",
    "                subresult[1]+=val\n",
    "        result.append(subresult)\n",
    "    return sorted(result, reverse=True, key=getKey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "com_err 4.757900788908586\n",
      "hamlet 4.735788265712869\n",
      "rich_iii 4.475000185681662\n",
      "taming 4.466287992275215\n",
      "m_wives 4.218962695507708\n",
      "hen_iv_1 3.991643004338016\n",
      "tempest 3.935030785476478\n",
      "hen_iv_2 3.8369575226705264\n",
      "john 3.5779035786569837\n",
      "m_for_m 3.4678816094848157\n",
      "lll 3.1547609456526753\n",
      "titus 3.1474160300055054\n",
      "as_you 3.1373801507638284\n",
      "r_and_j 3.12291745368682\n",
      "lear 3.042348710037171\n",
      "merchant 3.0234778718563797\n",
      "dream 2.9869408622156985\n",
      "a_and_c 2.9218776135644076\n",
      "hen_vi_3 2.791647389921127\n",
      "othello 2.7129781812814624\n",
      "macbeth 2.707204937926539\n",
      "troilus 2.6201123101368444\n",
      "j_caesar 2.563982455029559\n",
      "win_tale 2.3793607597775392\n",
      "hen_vi_2 2.3408220560348383\n",
      "timon 2.321752809048462\n",
      "hen_v 2.281829347310559\n",
      "two_gent 2.23740263821172\n",
      "hen_vi_1 2.0623911143031286\n",
      "rich_ii 2.051539523818834\n",
      "coriolan 1.818167029385026\n",
      "much_ado 1.4567858550578259\n",
      "all_well 1.429302949090377\n",
      "cymbelin 1.3719446855480477\n",
      "t_night 1.0386120618788026\n",
      "hen_viii 0\n",
      "pericles 0\n"
     ]
    }
   ],
   "source": [
    "result = Okapi_BM25_search(['test', 'despair', 'fat', 'beast'], \n",
    "                           doc_set, index_with_freq)\n",
    "for [doc, Okapi_score] in result:\n",
    "    print (doc, Okapi_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now on to the Motie files. \n",
    "\n",
    "We start with preparing the corpus, the index and the list of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MOTIONS = \"Moties/XML/MOT\"\n",
    "\n",
    "# prepare the files\n",
    "prepare_data(MOTIONS, MOTIONS + \"New\", 0, 1)\n",
    "\n",
    "# create the dinex\n",
    "index_motions = index_collection(MOTIONS + \"New\")\n",
    "\n",
    "index_with_freq_motions = add_frequencies(index_motions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1822272', '6324358', '1927493', '1863470', '1883867', '1978215', '1922047', '1952563', '1955718', '5649055', '5878188', '1888724', '2044403', '1916534', '1923937', '1817433', '1996621', '2007121', '1989904', '1898315', '1897160', '2043750', '5910716', '6232136', '5574386', '1802912', '1888493', '1861832', '1896789', '2005637', '1896782', '6182922', '1893324', '1972350', '1921768', '2019808', '1912329', '1953387', '6210236', '1805147', '1857425', '1828258', '1805140', '2047187', '1965008', '1952384', '1858643', '5734032', '1912092', '1921895', '2037198', '2056451', '1876972', '1958608', '1918140', '1914876', '1843841', '5873662', '6050773', '6165396', '2057498', '1913053', '2042934', '2040148', '1930408', '1810279', '2034752', '2039100', '2034759', '1934982', '5939404', '1964665', '2046633', '2049054', '2040141', '1891258', '1857283', '1960785', '1875409', '1875401', '1959950', '1854881', '1961780', '6211185', '6190317', '6190312', '1961789', '1856777', '2037221', '2033762', '5645259', '1851344', '1933973', '1815047', '1891785', '1882093', '5642285', '1858397', '1874460', '1944179']\n",
      "27946\n"
     ]
    }
   ],
   "source": [
    "# Because there are so many motion files, we only print the first 100\n",
    "doc_set_motions = make_doc_set(index_with_freq_motions)\n",
    "print(list(doc_set_motions)[0:100])\n",
    "print(len(doc_set_motions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 8.1 Create a search engine for negation (very useful)\n",
    "\n",
    "Since the collection is very large, we won't show the names of the documents, but we illustrate the correctness of our code with the numbers involved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents not containing 'zonder': 26900\n",
      "Documents containing 'newspaper': 1046\n",
      "Total number of documents in the corpus: 27946\n"
     ]
    }
   ],
   "source": [
    "print(\"Documents not containing 'zonder': \" + \n",
    "      str(len(negation_search(doc_set_motions, \n",
    "                              index_with_freq_motions, \n",
    "                              'zonder'))))\n",
    "\n",
    "print(\"Documents containing 'newspaper': \" + \n",
    "      str(len(docs_with_token(index_with_freq_motions, \n",
    "                              'zonder'))))\n",
    "\n",
    "print(\"Total number of documents in the corpus: \" + \n",
    "      str(len(doc_set_motions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.2 Disjunction search\n",
    "\n",
    "Input as a list of words. Output is unordered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1823284', '5945299', '6304174', '2001242', '1904282', '1834971', '1974612', '1841958', '1863222', '6232530', '1826482', '1950877', '1823273', '1908772', '1834720', '1989310', '1807231', '1959309', '1837222', '1887235', '2018685', '2058611', '1873652', '1910310', '6214573', '2015849', '1991585', '1881458', '1855695', '6323646', '6289279', '1821883', '1882915', '5727825', '1870041', '1838354', '2020711', '1927026', '1834263', '1823260', '1963548', '5578138', '1812066', '1833513', '1910982', '2063748', '1837083', '1871239', '1885926', '1880230', '1902433', '1926643', '1896341', '1873660', '1818493', '2021060', '1890611', '1989385', '1816738', '2021917', '5941586', '1854634', '1950410', '2003781', '1826894', '1957555', '2003813', '1823310', '1963559', '1932339', '2009553', '1950359', '1835389', '1812815', '2025463', '1969376', '2059080', '2046450', '1902350', '1824882', '1969035', '1886141', '1823293', '1930868', '1958339', '6197783', '1910052', '1930776', '5945280', '1911140', '1845505', '5478243', '2015838', '1833916', '2015188', '2046042', '6198690', '6316205', '2014928', '1810171', '1959375', '6217398', '1812803', '1899347', '2063818', '2055457', '1957634', '1959234', '6289357', '1844813', '1877907'}\n"
     ]
    }
   ],
   "source": [
    "print(disjunction_search(['kortstondig', 'macht'], index_with_freq_motions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### 8.3.1 Ordering by number of terms matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5259\n",
      "1851372: 2\n",
      "5668949: 1\n"
     ]
    }
   ],
   "source": [
    "# We displayed this by length, because of the size\n",
    "print(len(disj_search_simple_order(['naar', 'zoeken'], \n",
    "                               index_with_freq_motions)))\n",
    "print('1851372:', num_of_terms(['naar', 'zoeken'], \n",
    "                               index_with_freq_motions, '1851372'))\n",
    "print('5668949:', num_of_terms(['naar', 'zoeken'], \n",
    "                               index_with_freq_motions, '5668949'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "##### 8.3.2 Ordering by term frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5752534 5\n",
      "5858769 5\n",
      "6165509 5\n",
      "2036160 4\n",
      "2062632 4\n",
      "2045458 4\n",
      "1990070 4\n",
      "1917070 4\n",
      "2027968 4\n",
      "2063185 4\n",
      "5701234 4\n",
      "6224761 4\n",
      "1898824 4\n",
      "1871320 3\n",
      "1921768 3\n",
      "2037994 3\n",
      "1898875 3\n",
      "1921779 3\n",
      "1818455 3\n",
      "5692890 3\n",
      "2019969 3\n",
      "2056837 3\n",
      "2029562 3\n",
      "2015495 3\n",
      "1898881 3\n",
      "2056814 3\n",
      "1992631 3\n",
      "5863350 3\n",
      "2015631 3\n",
      "5869275 3\n"
     ]
    }
   ],
   "source": [
    "# Only print the first 30 because of the size of the corpus\n",
    "count = 0\n",
    "\n",
    "for doc in disj_search_term_freq_order(['uitzondering', 'zonder'], \n",
    "                                       index_with_freq_motions):\n",
    "    if count < 30:\n",
    "        print(doc, term_frequency(['uitzondering', 'zonder'], \n",
    "                               index_with_freq_motions, doc))\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8.3.3 Ordering by number of matched terms and term frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5752534 [2, 5]\n",
      "1871320 [2, 3]\n",
      "2056837 [2, 3]\n",
      "2056814 [2, 3]\n",
      "5863350 [2, 3]\n",
      "6165521 [2, 2]\n",
      "1811999 [2, 2]\n",
      "1905968 [2, 2]\n",
      "2007313 [2, 2]\n",
      "2018000 [2, 2]\n",
      "6177478 [2, 2]\n",
      "2001958 [2, 2]\n",
      "1920324 [2, 2]\n",
      "1812045 [2, 2]\n",
      "5858769 [1, 5]\n",
      "6165509 [1, 5]\n",
      "2036160 [1, 4]\n",
      "2062632 [1, 4]\n",
      "2045458 [1, 4]\n",
      "1990070 [1, 4]\n",
      "1917070 [1, 4]\n",
      "2027968 [1, 4]\n",
      "2063185 [1, 4]\n",
      "5701234 [1, 4]\n",
      "6224761 [1, 4]\n",
      "1898824 [1, 4]\n",
      "1921768 [1, 3]\n",
      "2037994 [1, 3]\n",
      "1898875 [1, 3]\n",
      "1921779 [1, 3]\n"
     ]
    }
   ],
   "source": [
    "# Only print the first 30 due to the size of the corpus\n",
    "count = 0\n",
    "\n",
    "for doc in disj_search_combo_order(['uitzondering', 'zonder'], \n",
    "                                   index_with_freq_motions):\n",
    "    if count < 30: \n",
    "        print (doc, tf_and_matchedNo_combo(['uitzondering', 'zonder'], \n",
    "                                       index_with_freq_motions, doc))\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8.4 Conjunctive search\n",
    "\n",
    "Input is, again, a list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a_and_c', 'lear', 'hen_vi_1', 'hen_vi_2', 'all_well', 'tempest', 'rich_iii', 'coriolan', 'hen_iv_2', 'cymbelin', 'macbeth', 'rich_ii'}\n"
     ]
    }
   ],
   "source": [
    "print(conjunction_search(['despair','terrible'], index_with_freq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.1 TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13.1412028440621, '6224761']\n",
      "[13.1412028440621, '5752534']\n",
      "[13.1412028440621, '5701234']\n",
      "[13.1412028440621, '2063185']\n",
      "[13.1412028440621, '2062632']\n",
      "[13.1412028440621, '2027968']\n",
      "[13.1412028440621, '1990070']\n",
      "[13.1412028440621, '1917070']\n",
      "[13.1412028440621, '1898824']\n",
      "[9.855902133046575, '5869275']\n",
      "[9.855902133046575, '5752624']\n",
      "[9.855902133046575, '5692890']\n",
      "[9.855902133046575, '2037994']\n",
      "[9.855902133046575, '2029562']\n",
      "[9.855902133046575, '2019969']\n",
      "[9.855902133046575, '1992631']\n",
      "[9.855902133046575, '1921779']\n",
      "[9.855902133046575, '1921768']\n",
      "[9.855902133046575, '1898881']\n",
      "[9.855902133046575, '1898875']\n",
      "[9.855902133046575, '1818455']\n",
      "[6.57060142203105, '6313964']\n",
      "[6.57060142203105, '6284030']\n",
      "[6.57060142203105, '6199417']\n",
      "[6.57060142203105, '6174948']\n",
      "[6.57060142203105, '6172745']\n",
      "[6.57060142203105, '6165504']\n",
      "[6.57060142203105, '6165043']\n",
      "[6.57060142203105, '6085273']\n",
      "[6.57060142203105, '6085268']\n"
     ]
    }
   ],
   "source": [
    "returnval = TF_IDF('zonder', doc_set_motions, index_with_freq_motions)\n",
    "for elem in returnval[:30]:\n",
    "    print (elem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 9.2 TF-IDF scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('1876602', 28.58794760912831)\n",
      "('1896402', 28.195211121541675)\n",
      "('1826452', 21.440960706846234)\n",
      "('2041356', 14.293973804564155)\n",
      "('2062352', 13.508500829390881)\n",
      "('2062405', 13.508500829390881)\n",
      "('6081866', 12.212674713303658)\n",
      "('2063081', 11.81993822571702)\n",
      "('2062443', 11.81993822571702)\n",
      "('1960847', 11.81993822571702)\n",
      "('5778376', 11.81993822571702)\n",
      "('1998806', 11.81993822571702)\n",
      "('6081858', 10.524112109629797)\n",
      "('1966248', 10.524112109629797)\n",
      "('1997533', 10.13137562204316)\n",
      "('2062475', 10.13137562204316)\n",
      "('5731495', 10.13137562204316)\n",
      "('1929440', 10.13137562204316)\n",
      "('2062384', 10.13137562204316)\n",
      "('1899883', 10.13137562204316)\n",
      "('2042866', 10.13137562204316)\n",
      "('1929425', 10.13137562204316)\n",
      "('2005810', 10.13137562204316)\n",
      "('1802743', 8.835549505955937)\n",
      "('1917427', 8.4428130183693)\n",
      "('1990639', 8.4428130183693)\n",
      "('2061789', 8.4428130183693)\n",
      "('1833138', 8.4428130183693)\n",
      "('2043770', 8.4428130183693)\n",
      "('1852543', 8.4428130183693)\n"
     ]
    }
   ],
   "source": [
    "returnval = TF_IDF_search(['naar','gevaarlijk'], doc_set_motions, index_with_freq_motions)\n",
    "\n",
    "for elem in returnval[:30]:\n",
    "    print (elem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.3 Euclidean length of docs\n",
    "\n",
    "Function will return a dict, as that seems useful.\n",
    "\n",
    "Careful! This thing will be very very long, which is why we have not included the result in the current status of this notebook. This is the beginning of the dict:\n",
    "\n",
    "{'1822272': 19.131126469708992, '6324358': 16.822603841260722, '1969967': 26.13426869074396, '2062541': 26.795522013948524, '1927493': 22.627416997969522, '1863470': 20.97617696340303, '1883867': 27.147743920996454, '1884761': 22.847319317591726, '1924595': 25.592967784139454, '1807366': 32.07802986469088, '1978215': 19.72308292331602, '1969541': 35.482389998420345, '5941723': 29.274562336608895, '1922047': 21.047565179849187, '1872076': 21.02379604162864, '2052517': 32.78719262151, '1952563': 35.98610843089316, '1955718': 23.065125189341593, '5649055': 25.729360660537214, '1963911': 22.22611077089287, '5878188': 36.29049462324811, '1888724': 37.067505985701274, '2044403': 27.622454633866266, '1886567': 22.64950330581225, '2016526': 44.181444068749045, '1916534': 22.293496809607955, '2023929': 26.795522013948524, '1944418': 17.26267650163207, '1923937': 29.46183972531247, '1817433': 26.870057685088806, ......"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(euclidean_lengths(doc_set_motions, index_with_freq_motions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 9.4 Cosine Similarity\n",
    "\n",
    "*If a word appears multiple times in the query, the length of the query vector is not just the square root of the length of the query. However, as this would change all results by multiplying them with a constant, it would not interfere with the ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6224761 0.4723507819680428\n",
      "2062632 0.3448671363406695\n",
      "2063185 0.3249946715053997\n",
      "1929425 0.3194446745966354\n",
      "1982261 0.31778464351192137\n",
      "6196232 0.3142089859794301\n",
      "1898824 0.3089696604389248\n",
      "1929440 0.2966143295949211\n",
      "2001180 0.2812579150222812\n",
      "1917427 0.27782691651165714\n",
      "1917070 0.2669106896335223\n",
      "1898881 0.25838356481755875\n",
      "1899429 0.25822499302351104\n",
      "2027968 0.2576706440012177\n",
      "2019969 0.2566739070868738\n",
      "6235992 0.25394720885772337\n",
      "6020986 0.251785993573431\n",
      "1898875 0.24881978506648197\n",
      "5847988 0.24636498010616054\n",
      "1843575 0.24410815410035483\n",
      "5752624 0.24263553770061774\n",
      "2041946 0.24091613383605268\n",
      "2020617 0.24062983644061398\n",
      "1897105 0.24014038373472946\n",
      "5701234 0.23922088344989634\n",
      "1986176 0.2382984197241302\n",
      "1921768 0.23813196787713978\n",
      "1811954 0.23782436644357258\n",
      "1909972 0.23562667933499212\n",
      "5752534 0.2350393796307733\n"
     ]
    }
   ],
   "source": [
    "result = cosine_similarity_search(['zonder', 'gevaar', 'naar'], \n",
    "                                  doc_set_motions, index_with_freq_motions)\n",
    "\n",
    "# Show only 30 because of the size of the corpus\n",
    "\n",
    "count = 0\n",
    "for [doc,similarity] in result:\n",
    "    if count < 30:\n",
    "        print (doc, similarity)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.5 Okapi BM25\n",
    "\n",
    "We will need to do some calculations with the values of k1, b, the term frequency, the document vector length and the average document vector length. All of these are calculable except for k1 and b, which are free variables. The wikipedia page suggests putting b=0.75 and k1 being between 1.2 and 2, so we'll take b=0.75 and k1=1.6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1929425 3.123064218158644\n",
      "1873014 3.07069714454952\n",
      "1929440 3.065856887653787\n",
      "1917427 3.039159563179651\n",
      "2019969 3.032459236276599\n",
      "1897105 2.988009944150866\n",
      "1872513 2.9788001366348835\n",
      "2033252 2.955692240669091\n",
      "1872418 2.949558256855314\n",
      "2055849 2.9398180834149983\n",
      "1811954 2.9298859179264216\n",
      "1876795 2.9214741236595105\n",
      "1913813 2.898651581474115\n",
      "5858761 2.8665191430646404\n",
      "6064612 2.8477379989066534\n",
      "5876031 2.8093005342931683\n",
      "1879926 2.77516633826521\n",
      "1899429 2.7714170903137116\n",
      "1842779 2.7664597962116817\n",
      "5649060 2.7344853842871792\n",
      "1909972 2.7249852386700044\n",
      "1852543 2.7095474300523543\n",
      "1896948 2.703777897986984\n",
      "1956993 2.695099298071142\n",
      "1917070 2.690480196273434\n",
      "6199417 2.6780444430061148\n",
      "2022699 2.661200956359316\n",
      "1988081 2.6540512390051014\n",
      "6190285 2.6475819040399395\n",
      "2063136 2.6448638690738506\n"
     ]
    }
   ],
   "source": [
    "result = Okapi_BM25_search(['zonder', 'gevaar', 'naar'], \n",
    "                           doc_set_motions, index_with_freq_motions)\n",
    "count = 0\n",
    "for [doc, Okapi_score] in result:\n",
    "    if count < 30:\n",
    "        print (doc, Okapi_score)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.3.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
