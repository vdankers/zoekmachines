{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment  week 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook made by   (If not filled in correctly: 0 pts for assignment)\n",
    "\n",
    "__Name(s)__: Adriaan de Vries, Verna Dankers\n",
    "\n",
    "__Student id(s)__ : 10795227, 10761225\n",
    "\n",
    "__Email(s)__: adriaan.devries@student.uva.nl, verna.dankers@student.uva.nl\n",
    "\n",
    "### Pledge (taken from [Coursera's Honor Code](https://www.coursera.org/about/terms/honorcode) )\n",
    "\n",
    "\n",
    "\n",
    "Put here a selfie with your photo where you hold a signed paper with the following text: (if this is team work, put two selfies here). The link must be to some place on the web, not to a local file. **Assignments without the selfies will not be graded and receive 0 points.**\n",
    "\n",
    "> My answers to homework, quizzes and exams will be my own work (except for assignments that explicitly permit collaboration).\n",
    "\n",
    ">I will not make solutions to homework, quizzes or exams available to anyone else. This includes both solutions written by me, as well as any official solutions provided by the course staff.\n",
    "\n",
    ">I will not engage in any other activities that will dishonestly improve my results or dishonestly improve/hurt the results of others.\n",
    "\n",
    "<img width=30% src='http://oi63.tinypic.com/2q3srh3.jpg'/>\n",
    "<img src=\"http://i63.tinypic.com/2itj1ic.jpg\" border=\"0\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "#### Book\n",
    "\n",
    "* Chap 6: 6.8 to 6.13, 6.14 to 6.17. \n",
    " \n",
    "#### Programming\n",
    "\n",
    "* [MakeInvertedIndex](MakeInvertedIndex1516.html): Exercises 8.1 to 8.6 about Boolean Search Engines and Exercises 9.1 to 9.5 about Ranked Retrieval \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Book\n",
    "\n",
    "#### 6.8 \n",
    "\n",
    "$idf_t = log( \\frac{N}{df_t} )$\n",
    "\n",
    "For $df_t > 0$, the $idf_t$ will always stay between the lower and the upper bound. The lower bound for $idf_t$ will be reached when the $df_t = N$ and the upper bound will be reached when $df_t = 1$. So the $idf_t$ can only be one of the numbers from the finite set of the number between these two bounds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.9 \n",
    "\n",
    "The $idf_t$ of a term that appears in every document: $log(\\frac{N}{N}) = log(1) = 0$\n",
    "\n",
    "Such a word will act like a stop word when the $tf-idf$ weighting will be used, since its weight becomes zero. It will not be able to affect the search results, due to its zero weight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.10\n",
    "\n",
    "$tf idf_{t,d} = tf_{t,d} \\times idf_t$\n",
    "\n",
    "##### car\n",
    "$tf idf_{car,d1} = 27 \\times log(\\frac{3}{3}) = 0 $  \n",
    "\n",
    "$tf idf_{car,d2} = 4 \\times log(\\frac{3}{3}) = 0 $  \n",
    "\n",
    "$tf idf_{car,d3} = 24 \\times log(\\frac{3}{3}) = 0 $  \n",
    "\n",
    "##### auto\n",
    "\n",
    "$tf idf_{auto,d1} = 3 \\times log(\\frac{3}{2}) = 0.53 $  \n",
    "\n",
    "$tf idf_{auto,d2} = 33 \\times log(\\frac{3}{2}) = 5.81 $\n",
    "\n",
    "$tf idf_{auto,d3} = 0 \\times log(\\frac{3}{2}) = 0 $  \n",
    "\n",
    "##### insurance\n",
    "\n",
    "$tf idf_{insurance,d1} = 0 \\times log(\\frac{3}{2}) = 0 $  \n",
    "\n",
    "$tf idf_{insurance,d2} = 33 \\times log(\\frac{3}{2}) = 5.81 $  \n",
    "\n",
    "$tf idf_{insurance,d3} = 29 \\times log(\\frac{3}{2}) = 5.11 $  \n",
    "\n",
    "##### best\n",
    "\n",
    "$tf idf_{best,d1} = 14 \\times log(\\frac{3}{2}) = 2.47 $  \n",
    "\n",
    "$tf idf_{best,d2} = 0 \\times log(\\frac{3}{2}) = 0 $  \n",
    "\n",
    "$tf idf_{best,d3} = 17 \\times log(\\frac{3}{2}) = 2.99 $  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.11\n",
    "\n",
    "Yes, obivously. Dependend on the number of documents in the collection (N) the upper bound for $tf-idf$ is $log(\\frac{N}{1})$, which can definitely exceed one. This is illustrated by exercise 6.10. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 6.12\n",
    "\n",
    "Altering the base of a logarithm is equivalent to multiplying the result by a constant. This will obviously alter the specific values for the idf, and thus also for $Score(q,d)$ (6.9), but the ratios between scores of different documents will remain the same. For sintance, if in base A the score of Doc1 is 2, and it's 4 in base B, than Doc2 will also have a score twice as high in base B as opposed to A. It is, however, important to calculate the score for each document with a logarithm of the same base. If we change bases during the process, some documents could get unfairly high or low rankings. There is, however, absolutely no reason to change the base of the logarithm mid-computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.13\n",
    "\n",
    "As stated before, the result of a logarithm in base two for some value, is just a constant times the logarithm with another base of that value. There really cannot be an easy approximation that works in base 2 but not in other bases. However, if what is meant in the question is an easy approximation of the idf in base 2 given the idf in base 10, then one could get this value by multiplying the idf by $\\frac{log(10)}{log(2)}$. For these logarithms, any base gives literally the same exact solution, as they're divided by eachother. Also, this is not so much an approximation as it is an exact way to calculate the idf in base 2 from base 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.15\n",
    "\n",
    "Both the term frequency and the inverse document frequency would have to see jealous and jealousy as the same term. For the term frequency, it's sufficient to simply add the term frequencies of the two previously seperate terms. In the calculation of the inverse document frequency, we can't simply add the document frequencies of the two terms as there may be overlap. Instead, we'd have to take the sets of documents that the terms appear in, take the union of those sets, and the cardinality of this set is the new document frequency of jealous(y) that can be used normally for the calculation of the idf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.15\n",
    "\n",
    "Vectors from exercise 6.10:\n",
    "\n",
    "1. d1 = $[0,0.53,0,2.47]^T$\n",
    "2. d2 = $[0,5.81,5.81,0]^T$\n",
    "3. d3 = $[0,0,5.11,2.99]^T$\n",
    "\n",
    "\n",
    "Normalized vectors:\n",
    "\n",
    "1. d1 = $\\frac{ [0,0.53,0,2.47]^T }{\\sqrt{0.53^2 + 2.47^2}} = \\frac{[0,0.53,0,2.47]^T}{2.53} = [0,0.21, 0  , 0.98]^T$\n",
    "2. d2 = $\\frac{ [0,5.81,5.81,0]^T }{\\sqrt{5.81^2 + 5.81^2}} = \\frac{[0,5.81,5.81,0]^T}{8.22} = [0,0.71,0.71, 0   ]^T$\n",
    "3. d3 = $\\frac{ [0,0,5.11,2.99]^T }{\\sqrt{5.11^2 + 2.99^2}} = \\frac{[0,0,5.11,2.99]^T}{5.92} = [0,   0,0.86, 0.51]^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.16\n",
    "\n",
    "1. length d1: $\\sqrt{0.21^2 + 0.98^2} = 1.002$\n",
    "2. length d2: $\\sqrt{0.71^2 + 0.71^2} = 1.004$\n",
    "3. length d3: $\\sqrt{0.86^2 + 0.51^2} = 1.000$ \n",
    "\n",
    "Now the vectors have length 1 since we divided the original vectors by their length in exercise 6.15."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.17\n",
    "\n",
    "1. The weight of a term is 1 if present in the query, 0 otherwise.\n",
    "\n",
    "query: car insurance\n",
    "\n",
    "1. d1 = $[0,0.21, 0  , 0.98]^T \\cdot [1,0,1,0]^T = 0$\n",
    "2. d2 = $[0,0.71,0.71, 0   ]^T \\cdot [1,0,1,0]^T = 0.71$\n",
    "3. d3 = $[0,   0,0.86, 0.51]^T \\cdot [1,0,1,0]^T = 0.86$\n",
    "\n",
    "2. Euclidean normalized idf.\n",
    "\n",
    "1. d1 = $[0,0.21, 0  , 0.98]^T \\cdot [0,0,0.18,0]^T = 0$\n",
    "2. d2 = $[0,0.71,0.71, 0   ]^T \\cdot [0,0,0.18,0]^T = 0.13$\n",
    "3. d3 = $[0,   0,0.86, 0.51]^T \\cdot [0,0,0.18,0]^T = 0.15$\n",
    "\n",
    "If we normalize the query, the dot product will be $\\frac{1}{0.18}$ times as high for all documents.\n",
    "\n",
    "For both ways of calculating the third document is the best hit for the query, followed by the second document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Programming\n",
    "\n",
    "For the programming part, we'll first copy and paste what we need from Assignment 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from collections import Counter, defaultdict\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "CORPUS = \"Shakespeare\"\n",
    "\n",
    "def normalize(word, keep_capitals):\n",
    "    \"\"\"\n",
    "    Normalize capital letters and punctuation\n",
    "    \"\"\"\n",
    "    if (keep_capitals and not word == word.upper()) or not keep_capitals:\n",
    "        word = word.lower()\n",
    "    word = re.sub(r'[^\\w\\s]', '', word)\n",
    "    return word\n",
    "\n",
    "def prepare_data(folder, new_folder, keep_capitals, byte):\n",
    "    \"\"\"\n",
    "    Prepare a text corpus for the process of\n",
    "    making an inverted index.\n",
    "    \"\"\"\n",
    "    # Create a directory for the processed files\n",
    "    if not os.path.exists(new_folder):\n",
    "        os.makedirs(new_folder)\n",
    "\n",
    "    # Prepare each file one by one\n",
    "    for filename in os.listdir(folder):\n",
    "        if not os.path.exists(os.path.join(new_folder, filename[:-4] + \".pkl\")):\n",
    "            # Strip xml tags\n",
    "            if byte:\n",
    "                xmlfile = BeautifulSoup(open(os.path.join(folder, filename), 'rb'),\"xml\")\n",
    "            else:\n",
    "                xmlfile = BeautifulSoup(open(os.path.join(folder, filename), 'r'),\"xml\")\n",
    "            textfile = xmlfile.get_text()\n",
    "\n",
    "            # Split the text on whitespaces\n",
    "            beautifile = WhitespaceTokenizer().tokenize(textfile)\n",
    "\n",
    "            # Normalize text for capital letters, non-alphabetical tokens and whitespace\n",
    "            finalfile = [ normalize(word, keep_capitals)\n",
    "                          for word \n",
    "                          in beautifile \n",
    "                         ]\n",
    "            new_filename = os.path.join(new_folder, filename[:-4] + \".pkl\")\n",
    "            savefile = open(new_filename, 'wb')\n",
    "            pickle.dump(finalfile, savefile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def index_collection(folder):\n",
    "    \"\"\"\n",
    "    Create an inverted index given a folder \n",
    "    containig a text corpus.\n",
    "    \"\"\"\n",
    "    index= defaultdict(Counter) # initialize MyIndex\n",
    "    for filename in os.listdir(folder):\n",
    "        text = pickle.load(open(os.path.join(folder,filename),'rb'))\n",
    "        \n",
    "        # Update the index with each token\n",
    "        for w in text:    \n",
    "            index[w][filename[:-4]]+=1   \n",
    "    if '' in index.keys():\n",
    "        del index['']\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prepare_data(CORPUS, CORPUS + \"New\", 0, 0)\n",
    "index_org = index_collection(CORPUS + \"New\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_doc_freq(index, word):\n",
    "    \"\"\"\n",
    "    Calculate in how many documents a given \n",
    "    term occurs, based on the inverted index.\n",
    "    \"\"\"\n",
    "    return len(index[word])\n",
    "    \n",
    "def calculate_corp_freq(index, word):\n",
    "    \"\"\"\n",
    "    Calculate how often a term occurs in a \n",
    "    corpus, based on the inverted index.\n",
    "    \"\"\"\n",
    "    corpus_frequency = 0\n",
    "    \n",
    "    for document in index[word]:\n",
    "        corpus_frequency += index[word][document]\n",
    "        \n",
    "    return corpus_frequency\n",
    "\n",
    "def add_frequencies(index):\n",
    "    for word in index:\n",
    "        original_counter = index[word]\n",
    "        document_frequency = calculate_doc_freq(index, word)\n",
    "        corpus_frequency = calculate_corp_freq(index, word)\n",
    "        index[word] = {\"counter\" : original_counter, \n",
    "                       \"doc_freq\" : document_frequency, \n",
    "                       \"corp_freq\" : corpus_frequency\n",
    "                      }\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "index_with_freq = add_frequencies(index_org)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making a list of all documents\n",
    "\n",
    "Will be useful for negation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def make_doc_set(index_with_frequencies):\n",
    "    \n",
    "    doc_list = []\n",
    "    for token in index_with_freq:\n",
    "        doc_list.extend(list(index_with_freq[token]['counter'].elements()))\n",
    "    return set(doc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'win_tale', 'com_err', 'rich_ii', 'm_wives', 'hamlet', 'taming', 'hen_viii', 'cymbelin', 'coriolan', 'hen_v', 'much_ado', 'merchant', 'pericles', 'r_and_j', 'a_and_c', 'hen_iv_1', 'all_well', 'lear', 'm_for_m', 'hen_vi_2', 'j_caesar', 'hen_vi_1', 'hen_vi_3', 'titus', 'timon', 'rich_iii', 't_night', 'two_gent', 'hen_iv_2', 'troilus', 'as_you', 'john', 'othello', 'tempest', 'dream', 'lll', 'macbeth'}\n"
     ]
    }
   ],
   "source": [
    "doc_set = make_doc_set(index_with_freq)\n",
    "print(doc_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On to the actual exercises:\n",
    "\n",
    "##### 8.1 a search engine for negation (so useful)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def docs_with_token(token):\n",
    "    if token in index_with_freq:\n",
    "        return set(index_with_freq[token]['counter'].elements())\n",
    "    else:\n",
    "        return set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def negation_search(word):\n",
    "    return doc_set.difference(docs_with_token(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'win_tale', 'com_err', 'rich_ii', 'm_wives', 'taming', 'hen_viii', 'cymbelin', 'coriolan', 'hen_v', 'much_ado', 'merchant', 'pericles', 'r_and_j', 'a_and_c', 'hen_iv_1', 'all_well', 'lear', 'hen_vi_2', 'j_caesar', 'hen_vi_1', 'hen_vi_3', 'titus', 'timon', 'rich_iii', 't_night', 'two_gent', 'hen_iv_2', 'troilus', 'as_you', 'john', 'dream', 'lll', 'macbeth'}\n",
      "{'m_for_m', 'othello', 'hamlet', 'tempest'}\n",
      "{'hen_v', 'much_ado', 'troilus', 'as_you', 'm_wives', 'hen_viii', 'hamlet', 'pericles', 'hen_iv_1', 'othello', 'timon', 't_night', 'dream', 'two_gent', 'lll', 'j_caesar'}\n"
     ]
    }
   ],
   "source": [
    "print (negation_search('test'))\n",
    "print (docs_with_token('test'))\n",
    "print (negation_search('despair'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.2 Disjunction search\n",
    "\n",
    "Input as a list of words. Output is unordered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def disjunction_search(query):\n",
    "    if len(query) ==0:\n",
    "        return set()\n",
    "    result = docs_with_token(query[0])\n",
    "    for word in query[1:]:\n",
    "        result = result.union(docs_with_token(query[1]))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a_and_c',\n",
       " 'all_well',\n",
       " 'com_err',\n",
       " 'coriolan',\n",
       " 'cymbelin',\n",
       " 'hamlet',\n",
       " 'hen_iv_2',\n",
       " 'hen_vi_1',\n",
       " 'hen_vi_2',\n",
       " 'hen_vi_3',\n",
       " 'john',\n",
       " 'lear',\n",
       " 'm_for_m',\n",
       " 'macbeth',\n",
       " 'merchant',\n",
       " 'othello',\n",
       " 'r_and_j',\n",
       " 'rich_ii',\n",
       " 'rich_iii',\n",
       " 'taming',\n",
       " 'tempest',\n",
       " 'titus',\n",
       " 'win_tale'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disjunction_search(['test', 'despair'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####8.3.1 Ordering by number of terms matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def num_of_terms(query, doc):\n",
    "    count=0\n",
    "    for token in query:\n",
    "        if doc in docs_with_token(token):\n",
    "            count+=1\n",
    "    return count\n",
    "\n",
    "def disj_search_simple_order(query):\n",
    "    set_of_docs = disjunction_search(query)\n",
    "    return sorted(set_of_docs, reverse = True, key = lambda x: num_of_terms(query,x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['com_err', 'hamlet', 'taming', 'merchant', 'a_and_c', 'm_for_m', 'titus', 'rich_iii', 'hen_iv_2', 'john', 'tempest', 'win_tale', 'coriolan', 'r_and_j', 'lear', 'hen_vi_2', 'hen_vi_1', 'hen_vi_3', 'othello', 'macbeth', 'rich_ii', 'cymbelin', 'all_well']\n",
      "com_err: 3\n",
      "hamlet: 3\n",
      "all_well: 1\n"
     ]
    }
   ],
   "source": [
    "print(disj_search_simple_order(['test', 'despair', 'fat', 'beast']))\n",
    "print('com_err:', num_of_terms(['test', 'despair', 'fat', 'beast'], 'com_err'))\n",
    "print('hamlet:', num_of_terms(['test', 'despair', 'fat', 'beast'], 'hamlet'))\n",
    "print('all_well:', num_of_terms(['test', 'despair', 'fat', 'beast'], 'all_well'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####8.3.2 Ordering by term frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def term_frequency(query, doc):\n",
    "    count = 0\n",
    "    for token in query:\n",
    "        if doc in index_with_freq[token]['counter']:\n",
    "            count += index_with_freq[token]['counter'][doc]\n",
    "    return count\n",
    "\n",
    "def disj_search_term_freq_order(query):\n",
    "    set_of_docs = disjunction_search(query)\n",
    "    return sorted(set_of_docs, reverse = True, key = lambda x: term_frequency(query,x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rich_iii 17\n",
      "hamlet 13\n",
      "hen_iv_2 8\n",
      "com_err 7\n",
      "taming 7\n",
      "rich_ii 6\n",
      "r_and_j 5\n",
      "lear 5\n",
      "hen_vi_3 5\n",
      "othello 5\n",
      "m_for_m 4\n",
      "john 4\n",
      "tempest 4\n",
      "win_tale 3\n",
      "merchant 3\n",
      "a_and_c 3\n",
      "hen_vi_2 3\n",
      "titus 3\n",
      "macbeth 3\n",
      "cymbelin 2\n",
      "coriolan 2\n",
      "all_well 2\n",
      "hen_vi_1 2\n"
     ]
    }
   ],
   "source": [
    "for doc in disj_search_term_freq_order(['test', 'despair', 'fat', 'beast']):\n",
    "    print (doc, term_frequency(['test', 'despair', 'fat', 'beast'], doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8.3.3 Ordering by number of matched terms and term frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tf_and_matchedNo_combo(query,doc):\n",
    "    return([num_of_terms(query, doc), term_frequency(query, doc)])\n",
    "\n",
    "def disj_search_term_freq_order(query):\n",
    "    set_of_docs = disjunction_search(query)\n",
    "    return sorted(set_of_docs, reverse = True, key = lambda x: tf_and_matchedNo_combo(query,x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for doc in disj_search_term_freq_order(['test', 'despair', 'fat', 'beast']):\n",
    "    print (doc, term_frequency(['test', 'despair', 'fat', 'beast'], doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8.4 Conjunctive search\n",
    "\n",
    "Input is, again, a list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conjunction_search(query):\n",
    "    if len(query) ==0:\n",
    "        return set()\n",
    "    result = docs_with_token(query[0])\n",
    "    for word in query[1:]:\n",
    "        result = result.intersection(docs_with_token(query[1]))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'m_for_m', 'tempest'}\n"
     ]
    }
   ],
   "source": [
    "print(conjunction_search(['test', 'despair']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8.5 Ordering of conjunctive search\n",
    "\n",
    "Conjunctive search could be ordered by the amount of times terms appear in a document. Simply taking the sum is an option, or one could choose to do it relative to the rarity of the tokens. So the ranking could be the total amount of times a word from the query appears in the document, divided by the total amount of times the word appears in the corpus. Taking the sum over all the words in the query in this way would give a fairly good ranking.\n",
    "\n",
    "Each document score could then possibibly be divided by the document length, to punish longer documents as they will obviously on average have more terms in them. One could, however, argue that these longer documents are better documents, so they should be ranked higher and not punished."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8.6 Relaxing the search\n",
    "\n",
    "Instead of removing documents that fail to contain a search word, we can add some form of counter to them. Only when the counter exceeds a specified value will it be deleted. Then what's left is to decide how many terms the document should at least match. We think three quarters would be a good threshold, so it would not be allowed to miss more than floor(N/4) where N is the amount of words in the query. This is easily adjustable to any other value.\n",
    "\n",
    "As for ranking, the documents could be ranked the same way as specified in 8.5, except the documents with the lowest counter would be placed highest. So the documents that match the most terms will end up at the top of the ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
