{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment  week 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook made by   (If not filled in correctly: 0 pts for assignment)\n",
    "\n",
    "__Name(s)__: Adriaan de Vries, Verna Dankers\n",
    "\n",
    "__Student id(s)__ : 10795227, 10761225\n",
    "\n",
    "__Email(s)__: adriaan.devries@student.uva.nl, verna.dankers@student.uva.nl\n",
    "\n",
    "### Pledge (taken from [Coursera's Honor Code](https://www.coursera.org/about/terms/honorcode) )\n",
    "\n",
    "\n",
    "\n",
    "Put here a selfie with your photo where you hold a signed paper with the following text: (if this is team work, put two selfies here). The link must be to some place on the web, not to a local file. **Assignments without the selfies will not be graded and receive 0 points.**\n",
    "\n",
    "> My answers to homework, quizzes and exams will be my own work (except for assignments that explicitly permit collaboration).\n",
    "\n",
    ">I will not make solutions to homework, quizzes or exams available to anyone else. This includes both solutions written by me, as well as any official solutions provided by the course staff.\n",
    "\n",
    ">I will not engage in any other activities that will dishonestly improve my results or dishonestly improve/hurt the results of others.\n",
    "\n",
    "<img width=30% src='http://oi63.tinypic.com/2q3srh3.jpg'/>\n",
    "<img src=\"http://i63.tinypic.com/2itj1ic.jpg\" border=\"0\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "#### Book\n",
    "\n",
    "* Chap 6: 6.8 to 6.13, 6.14 to 6.17. \n",
    " \n",
    "#### Programming\n",
    "\n",
    "* [MakeInvertedIndex](MakeInvertedIndex1516.html): Exercises 8.1 to 8.6 about Boolean Search Engines and Exercises 9.1 to 9.5 about Ranked Retrieval \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Book\n",
    "\n",
    "#### 6.8 \n",
    "\n",
    "$idf_t = log( \\frac{N}{df_t} )$\n",
    "\n",
    "For $df_t > 0$, the $idf_t$ will always stay between the lower and the upper bound. The lower bound for $idf_t$ will be reached when the $df_t = N$ and the upper bound will be reached when $df_t = 1$. So the $idf_t$ can only be one of the numbers from the finite set of the number between these two bounds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.9 \n",
    "\n",
    "The $idf_t$ of a term that appears in every document: $log(\\frac{N}{N}) = log(1) = 0$\n",
    "\n",
    "Such a word will act like a stop word when the $tf-idf$ weighting will be used, since its weight becomes zero. It will not be able to affect the search results, due to its zero weight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.10\n",
    "\n",
    "$tf idf_{t,d} = tf_{t,d} \\times idf_t$\n",
    "\n",
    "##### car\n",
    "$tf idf_{car,d1} = 27 \\times log(\\frac{3}{3}) = 0 $  \n",
    "\n",
    "$tf idf_{car,d2} = 4 \\times log(\\frac{3}{3}) = 0 $  \n",
    "\n",
    "$tf idf_{car,d3} = 24 \\times log(\\frac{3}{3}) = 0 $  \n",
    "\n",
    "##### auto\n",
    "\n",
    "$tf idf_{auto,d1} = 3 \\times log(\\frac{3}{2}) = 0.53 $  \n",
    "\n",
    "$tf idf_{auto,d2} = 33 \\times log(\\frac{3}{2}) = 5.81 $\n",
    "\n",
    "$tf idf_{auto,d3} = 0 \\times log(\\frac{3}{2}) = 0 $  \n",
    "\n",
    "##### insurance\n",
    "\n",
    "$tf idf_{insurance,d1} = 0 \\times log(\\frac{3}{2}) = 0 $  \n",
    "\n",
    "$tf idf_{insurance,d2} = 33 \\times log(\\frac{3}{2}) = 5.81 $  \n",
    "\n",
    "$tf idf_{insurance,d3} = 29 \\times log(\\frac{3}{2}) = 5.11 $  \n",
    "\n",
    "##### best\n",
    "\n",
    "$tf idf_{best,d1} = 14 \\times log(\\frac{3}{2}) = 2.47 $  \n",
    "\n",
    "$tf idf_{best,d2} = 0 \\times log(\\frac{3}{2}) = 0 $  \n",
    "\n",
    "$tf idf_{best,d3} = 17 \\times log(\\frac{3}{2}) = 2.99 $  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.11\n",
    "\n",
    "Yes, obivously. Dependend on the number of documents in the collection (N) the upper bound for $tf-idf$ is $log(\\frac{N}{1})$, which can definitely exceed one. This is illustrated by exercise 6.10. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 6.12\n",
    "\n",
    "Altering the base of a logarithm is equivalent to multiplying the result by a constant. This will obviously alter the specific values for the idf, and thus also for $Score(q,d)$ (6.9), but the ratios between scores of different documents will remain the same. For sintance, if in base A the score of Doc1 is 2, and it's 4 in base B, than Doc2 will also have a score twice as high in base B as opposed to A. It is, however, important to calculate the score for each document with a logarithm of the same base. If we change bases during the process, some documents could get unfairly high or low rankings. There is, however, absolutely no reason to change the base of the logarithm mid-computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.13\n",
    "\n",
    "As stated before, the result of a logarithm in base two for some value, is just a constant times the logarithm with another base of that value. There really cannot be an easy approximation that works in base 2 but not in other bases. However, if what is meant in the question is an easy approximation of the idf in base 2 given the idf in base 10, then one could get this value by multiplying the idf by $\\frac{log(10)}{log(2)}$. For these logarithms, any base gives literally the same exact solution, as they're divided by eachother. Also, this is not so much an approximation as it is an exact way to calculate the idf in base 2 from base 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.15\n",
    "\n",
    "Both the term frequency and the inverse document frequency would have to see jealous and jealousy as the same term. For the term frequency, it's sufficient to simply add the term frequencies of the two previously seperate terms. In the calculation of the inverse document frequency, we can't simply add the document frequencies of the two terms as there may be overlap. Instead, we'd have to take the sets of documents that the terms appear in, take the union of those sets, and the cardinality of this set is the new document frequency of jealous(y) that can be used normally for the calculation of the idf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.15\n",
    "\n",
    "Vectors from exercise 6.10:\n",
    "\n",
    "1. d1 = $[0,0.53,0,2.47]^T$\n",
    "2. d2 = $[0,5.81,5.81,0]^T$\n",
    "3. d3 = $[0,0,5.11,2.99]^T$\n",
    "\n",
    "\n",
    "Normalized vectors:\n",
    "\n",
    "1. d1 = $\\frac{ [0,0.53,0,2.47]^T }{\\sqrt{0.53^2 + 2.47^2}} = \\frac{[0,0.53,0,2.47]^T}{2.53} = [0,0.21, 0  , 0.98]^T$\n",
    "2. d2 = $\\frac{ [0,5.81,5.81,0]^T }{\\sqrt{5.81^2 + 5.81^2}} = \\frac{[0,5.81,5.81,0]^T}{8.22} = [0,0.71,0.71, 0   ]^T$\n",
    "3. d3 = $\\frac{ [0,0,5.11,2.99]^T }{\\sqrt{5.11^2 + 2.99^2}} = \\frac{[0,0,5.11,2.99]^T}{5.92} = [0,   0,0.86, 0.51]^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.16\n",
    "\n",
    "1. length d1: $\\sqrt{0.21^2 + 0.98^2} = 1.002$\n",
    "2. length d2: $\\sqrt{0.71^2 + 0.71^2} = 1.004$\n",
    "3. length d3: $\\sqrt{0.86^2 + 0.51^2} = 1.000$ \n",
    "\n",
    "Now the vectors have length 1 since we divided the original vectors by their length in exercise 6.15."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.17\n",
    "\n",
    "1. The weight of a term is 1 if present in the query, 0 otherwise.\n",
    "\n",
    "query: car insurance\n",
    "\n",
    "1. d1 = $[0,0.21, 0  , 0.98]^T \\cdot [1,0,1,0]^T = 0$\n",
    "2. d2 = $[0,0.71,0.71, 0   ]^T \\cdot [1,0,1,0]^T = 0.71$\n",
    "3. d3 = $[0,   0,0.86, 0.51]^T \\cdot [1,0,1,0]^T = 0.86$\n",
    "\n",
    "2. Euclidean normalized idf.\n",
    "\n",
    "1. d1 = $[0,0.21, 0  , 0.98]^T \\cdot [0,0,0.18,0]^T = 0$\n",
    "2. d2 = $[0,0.71,0.71, 0   ]^T \\cdot [0,0,0.18,0]^T = 0.13$\n",
    "3. d3 = $[0,   0,0.86, 0.51]^T \\cdot [0,0,0.18,0]^T = 0.15$\n",
    "\n",
    "If we normalize the query, the dot product will be $\\frac{1}{0.18}$ times as high for all documents.\n",
    "\n",
    "For both ways of calculating the third document is the best hit for the query, followed by the second document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Programming\n",
    "\n",
    "For the programming part, we'll first copy and paste what we need from Assignment 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from collections import Counter, defaultdict\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "CORPUS = \"Shakespeare\"\n",
    "\n",
    "def normalize(word, keep_capitals):\n",
    "    \"\"\"\n",
    "    Normalize capital letters and punctuation\n",
    "    \"\"\"\n",
    "    if (keep_capitals and not word == word.upper()) or not keep_capitals:\n",
    "        word = word.lower()\n",
    "    word = re.sub(r'[^\\w\\s]', '', word)\n",
    "    return word\n",
    "\n",
    "def prepare_data(folder, new_folder, keep_capitals, byte):\n",
    "    \"\"\"\n",
    "    Prepare a text corpus for the process of\n",
    "    making an inverted index.\n",
    "    \"\"\"\n",
    "    # Create a directory for the processed files\n",
    "    if not os.path.exists(new_folder):\n",
    "        os.makedirs(new_folder)\n",
    "\n",
    "    # Prepare each file one by one\n",
    "    for filename in os.listdir(folder):\n",
    "        if not os.path.exists(os.path.join(new_folder, filename[:-4] + \".pkl\")):\n",
    "            # Strip xml tags\n",
    "            if byte:\n",
    "                xmlfile = BeautifulSoup(open(os.path.join(folder, filename), 'rb'),\"xml\")\n",
    "            else:\n",
    "                xmlfile = BeautifulSoup(open(os.path.join(folder, filename), 'r'),\"xml\")\n",
    "            textfile = xmlfile.get_text()\n",
    "\n",
    "            # Split the text on whitespaces\n",
    "            beautifile = WhitespaceTokenizer().tokenize(textfile)\n",
    "\n",
    "            # Normalize text for capital letters, non-alphabetical tokens and whitespace\n",
    "            finalfile = [ normalize(word, keep_capitals)\n",
    "                          for word \n",
    "                          in beautifile \n",
    "                         ]\n",
    "            new_filename = os.path.join(new_folder, filename[:-4] + \".pkl\")\n",
    "            savefile = open(new_filename, 'wb')\n",
    "            pickle.dump(finalfile, savefile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def index_collection(folder):\n",
    "    \"\"\"\n",
    "    Create an inverted index given a folder \n",
    "    containig a text corpus.\n",
    "    \"\"\"\n",
    "    index= defaultdict(Counter) # initialize MyIndex\n",
    "    for filename in os.listdir(folder):\n",
    "        text = pickle.load(open(os.path.join(folder,filename),'rb'))\n",
    "        \n",
    "        # Update the index with each token\n",
    "        for w in text:    \n",
    "            index[w][filename[:-4]]+=1   \n",
    "    if '' in index.keys():\n",
    "        del index['']\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prepare_data(CORPUS, CORPUS + \"New\", 0, 0)\n",
    "index_org = index_collection(CORPUS + \"New\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_doc_freq(index, word):\n",
    "    \"\"\"\n",
    "    Calculate in how many documents a given \n",
    "    term occurs, based on the inverted index.\n",
    "    \"\"\"\n",
    "    return len(index[word])\n",
    "    \n",
    "def calculate_corp_freq(index, word):\n",
    "    \"\"\"\n",
    "    Calculate how often a term occurs in a \n",
    "    corpus, based on the inverted index.\n",
    "    \"\"\"\n",
    "    corpus_frequency = 0\n",
    "    \n",
    "    for document in index[word]:\n",
    "        corpus_frequency += index[word][document]\n",
    "        \n",
    "    return corpus_frequency\n",
    "\n",
    "def add_frequencies(index):\n",
    "    for word in index:\n",
    "        original_counter = index[word]\n",
    "        document_frequency = calculate_doc_freq(index, word)\n",
    "        corpus_frequency = calculate_corp_freq(index, word)\n",
    "        index[word] = {\"counter\" : original_counter, \n",
    "                       \"doc_freq\" : document_frequency, \n",
    "                       \"corp_freq\" : corpus_frequency\n",
    "                      }\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "index_with_freq = add_frequencies(index_org)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making a list of all documents\n",
    "\n",
    "Will be useful for negation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_doc_set(index_with_frequencies):\n",
    "    \"\"\"\n",
    "    Creates a set of all documents from an inverted index\n",
    "    to which frequencies have been added\n",
    "    \"\"\"\n",
    "    \n",
    "    doc_list = []\n",
    "    for token in index_with_freq:\n",
    "        doc_list.extend(list(index_with_freq[token]['counter'].elements()))\n",
    "    return set(doc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hen_v', 'lear', 'all_well', 'cymbelin', 'a_and_c', 'titus', 'j_caesar', 'coriolan', 't_night', 'merchant', 'hen_iv_2', 'm_wives', 'hen_iv_1', 'two_gent', 'as_you', 'taming', 'timon', 'john', 'lll', 'win_tale', 'rich_iii', 'hen_vi_2', 'hen_viii', 'troilus', 'pericles', 'com_err', 'macbeth', 'hamlet', 'much_ado', 'dream', 'tempest', 'hen_vi_3', 'hen_vi_1', 'm_for_m', 'rich_ii', 'othello', 'r_and_j'}\n"
     ]
    }
   ],
   "source": [
    "doc_set = make_doc_set(index_with_freq)\n",
    "print(doc_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On to the actual exercises:\n",
    "\n",
    "We start with the code and the results for the Shakespeare corpus.\n",
    "\n",
    "##### 8.1 a search engine for negation (so useful)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def docs_with_token(token):\n",
    "    \"\"\"\n",
    "    Returns all documents containing a certain token\n",
    "    \"\"\"\n",
    "    if token in index_with_freq:\n",
    "        return set(index_with_freq[token]['counter'].elements())\n",
    "    else:\n",
    "        return set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def negation_search(word):\n",
    "    \"\"\"\n",
    "    Returns a set of all cosuments without the given term\n",
    "    \"\"\"\n",
    "    return doc_set.difference(docs_with_token(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hen_v', 'lear', 'all_well', 'cymbelin', 'a_and_c', 'titus', 'j_caesar', 'coriolan', 't_night', 'merchant', 'hen_iv_2', 'm_wives', 'hen_iv_1', 'two_gent', 'as_you', 'taming', 'timon', 'john', 'lll', 'win_tale', 'rich_iii', 'hen_vi_2', 'hen_viii', 'troilus', 'pericles', 'com_err', 'macbeth', 'much_ado', 'dream', 'hen_vi_3', 'hen_vi_1', 'rich_ii', 'r_and_j'}\n",
      "{'tempest', 'hamlet', 'othello', 'm_for_m'}\n",
      "{'lll', 'hen_v', 'm_wives', 'hen_iv_1', 'hen_viii', 'troilus', 'two_gent', 'othello', 'j_caesar', 'pericles', 'dream', 'much_ado', 'as_you', 'hamlet', 't_night', 'timon'}\n"
     ]
    }
   ],
   "source": [
    "print (negation_search('test'))\n",
    "print (docs_with_token('test'))\n",
    "print (negation_search('despair'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.2 Disjunction search\n",
    "\n",
    "Input as a list of words. Output is unordered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def disjunction_search(query):\n",
    "    \"\"\"\n",
    "    Returns a set of all documents in which any of the \n",
    "    terms from the query appear\n",
    "    \"\"\"\n",
    "    if len(query) ==0:\n",
    "        return set()\n",
    "    result = docs_with_token(query[0])\n",
    "    for word in query[1:]:\n",
    "        result = result.union(docs_with_token(query[1]))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a_and_c',\n",
       " 'all_well',\n",
       " 'com_err',\n",
       " 'coriolan',\n",
       " 'cymbelin',\n",
       " 'hamlet',\n",
       " 'hen_iv_2',\n",
       " 'hen_vi_1',\n",
       " 'hen_vi_2',\n",
       " 'hen_vi_3',\n",
       " 'john',\n",
       " 'lear',\n",
       " 'm_for_m',\n",
       " 'macbeth',\n",
       " 'merchant',\n",
       " 'othello',\n",
       " 'r_and_j',\n",
       " 'rich_ii',\n",
       " 'rich_iii',\n",
       " 'taming',\n",
       " 'tempest',\n",
       " 'titus',\n",
       " 'win_tale'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disjunction_search(['test', 'despair'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8.3.1 Ordering by number of terms matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def num_of_terms(query, doc):\n",
    "    \"\"\"\n",
    "    Returns the amount of query terms that\n",
    "    appear in the document\n",
    "    \"\"\"\n",
    "    count=0\n",
    "    for token in query:\n",
    "        if doc in docs_with_token(token):\n",
    "            count+=1\n",
    "    return count\n",
    "\n",
    "def disj_search_simple_order(query):\n",
    "    \"\"\"\n",
    "    Performs disjunction search, with documents ordered by\n",
    "    the amount of matching query terms\n",
    "    \"\"\"\n",
    "    set_of_docs = disjunction_search(query)\n",
    "    return sorted(set_of_docs, reverse = True, key = lambda x: num_of_terms(query,x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a_and_c', 'titus', 'merchant', 'hen_iv_2', 'taming', 'john', 'rich_iii', 'com_err', 'tempest', 'hamlet', 'm_for_m', 'lear', 'coriolan', 'win_tale', 'hen_vi_2', 'macbeth', 'hen_vi_3', 'hen_vi_1', 'othello', 'r_and_j', 'all_well', 'cymbelin', 'rich_ii']\n",
      "com_err: 3\n",
      "hamlet: 3\n",
      "all_well: 1\n"
     ]
    }
   ],
   "source": [
    "print(disj_search_simple_order(['test', 'despair', 'fat', 'beast']))\n",
    "print('com_err:', num_of_terms(['test', 'despair', 'fat', 'beast'], 'com_err'))\n",
    "print('hamlet:', num_of_terms(['test', 'despair', 'fat', 'beast'], 'hamlet'))\n",
    "print('all_well:', num_of_terms(['test', 'despair', 'fat', 'beast'], 'all_well'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8.3.2 Ordering by term frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def term_frequency(query, doc):\n",
    "    \"\"\"\n",
    "    Calculates the total term frequency for a query\n",
    "    and a document by summing te frequencies of the terms\n",
    "    in the document\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    for token in query:\n",
    "        if doc in index_with_freq[token]['counter']:\n",
    "            count += index_with_freq[token]['counter'][doc]\n",
    "    return count\n",
    "\n",
    "def disj_search_term_freq_order(query):\n",
    "    \"\"\"\n",
    "    Performs disjunctive search, ordering documents\n",
    "    by term frequency\n",
    "    \"\"\"\n",
    "    set_of_docs = disjunction_search(query)\n",
    "    return sorted(set_of_docs, reverse = True, key = lambda x: term_frequency(query,x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rich_iii 17\n",
      "hamlet 13\n",
      "hen_iv_2 8\n",
      "taming 7\n",
      "com_err 7\n",
      "rich_ii 6\n",
      "lear 5\n",
      "hen_vi_3 5\n",
      "othello 5\n",
      "r_and_j 5\n",
      "john 4\n",
      "tempest 4\n",
      "m_for_m 4\n",
      "a_and_c 3\n",
      "titus 3\n",
      "merchant 3\n",
      "win_tale 3\n",
      "hen_vi_2 3\n",
      "macbeth 3\n",
      "all_well 2\n",
      "cymbelin 2\n",
      "coriolan 2\n",
      "hen_vi_1 2\n"
     ]
    }
   ],
   "source": [
    "for doc in disj_search_term_freq_order(['test', 'despair', 'fat', 'beast']):\n",
    "    print (doc, term_frequency(['test', 'despair', 'fat', 'beast'], doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8.3.3 Ordering by number of matched terms and term frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tf_and_matchedNo_combo(query,doc):\n",
    "    \"\"\"\n",
    "    Returns both the term frequency and the amount of\n",
    "    matched terms for a given query and document\n",
    "    \"\"\"\n",
    "    return([num_of_terms(query, doc), term_frequency(query, doc)])\n",
    "\n",
    "def disj_search_combo_order(query):\n",
    "    \"\"\"\n",
    "    Performs disjunctive search, ordering documents first \n",
    "    on the amount of terms from the query matching the document,\n",
    "    and then on the sum of term frequencies.\n",
    "    \"\"\"\n",
    "    set_of_docs = disjunction_search(query)\n",
    "    return sorted(set_of_docs, reverse = True, key = lambda x: tf_and_matchedNo_combo(query,x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rich_iii [3, 17]\n",
      "hamlet [3, 13]\n",
      "hen_iv_2 [3, 8]\n",
      "taming [3, 7]\n",
      "com_err [3, 7]\n",
      "john [3, 4]\n",
      "tempest [3, 4]\n",
      "m_for_m [3, 4]\n",
      "a_and_c [3, 3]\n",
      "titus [3, 3]\n",
      "merchant [3, 3]\n",
      "lear [2, 5]\n",
      "hen_vi_3 [2, 5]\n",
      "othello [2, 5]\n",
      "r_and_j [2, 5]\n",
      "win_tale [2, 3]\n",
      "hen_vi_2 [2, 3]\n",
      "macbeth [2, 3]\n",
      "coriolan [2, 2]\n",
      "hen_vi_1 [2, 2]\n",
      "rich_ii [1, 6]\n",
      "all_well [1, 2]\n",
      "cymbelin [1, 2]\n"
     ]
    }
   ],
   "source": [
    "for doc in disj_search_combo_order(['test', 'despair', 'fat', 'beast']):\n",
    "    print (doc, tf_and_matchedNo_combo(['test', 'despair', 'fat', 'beast'], doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8.4 Conjunctive search\n",
    "\n",
    "Input is, again, a list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conjunction_search(query):\n",
    "    \"\"\"\n",
    "    Returns a set of documents that contain all the tokens in the query\n",
    "    \"\"\"\n",
    "    if len(query) ==0:\n",
    "        return set()\n",
    "    result = docs_with_token(query[0])\n",
    "    for word in query[1:]:\n",
    "        result = result.intersection(docs_with_token(query[1]))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tempest', 'm_for_m'}\n"
     ]
    }
   ],
   "source": [
    "print(conjunction_search(['test', 'despair']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8.5 Ordering of conjunctive search\n",
    "\n",
    "Conjunctive search could be ordered by the amount of times terms appear in a document. Simply taking the sum is an option, or one could choose to do it relative to the rarity of the tokens. So the ranking could be the total amount of times a word from the query appears in the document, divided by the total amount of times the word appears in the corpus. Taking the sum over all the words in the query in this way would give a fairly good ranking.\n",
    "\n",
    "Each document score could then possibibly be divided by the document length, to punish longer documents as they will obviously on average have more terms in them. One could, however, argue that these longer documents are better documents, so they should be ranked higher and not punished."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8.6 Relaxing the search\n",
    "\n",
    "Instead of removing documents that fail to contain a search word, we can add some form of counter to them. Only when the counter exceeds a specified value will it be deleted. Then what's left is to decide how many terms the document should at least match. We think three quarters would be a good threshold, so it would not be allowed to miss more than floor(N/4) where N is the amount of words in the query. This is easily adjustable to any other value.\n",
    "\n",
    "As for ranking, the documents could be ranked the same way as specified in 8.5, except the documents with the lowest counter would be placed highest. So the documents that match the most terms will end up at the top of the ranking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### 9.1 TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from math import log\n",
    "def TF_IDF(term):\n",
    "    \"\"\"\n",
    "    Returns a sorted list of documents with their TF_IDF\n",
    "    score for the given term. Sorted by score\n",
    "    \"\"\"\n",
    "    rated_docs = []\n",
    "    IDF = log(len(doc_set) / len(docs_with_token(term)))\n",
    "    for doc in doc_set:\n",
    "        if doc in index_with_freq[term]['counter']:\n",
    "            rated_docs.append([index_with_freq[term]['counter'][doc]*IDF,doc])\n",
    "        else:\n",
    "            rated_docs.append([0,doc])\n",
    "    return sorted(rated_docs, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[7.9295366488912205, 'rich_iii'],\n",
       " [3.398372849524809, 'rich_ii'],\n",
       " [2.265581899683206, 'hen_vi_3'],\n",
       " [1.6991864247624044, 'lear'],\n",
       " [1.132790949841603, 'win_tale'],\n",
       " [1.132790949841603, 'r_and_j'],\n",
       " [1.132790949841603, 'macbeth'],\n",
       " [1.132790949841603, 'john'],\n",
       " [1.132790949841603, 'hen_vi_2'],\n",
       " [1.132790949841603, 'cymbelin'],\n",
       " [1.132790949841603, 'all_well'],\n",
       " [0.5663954749208014, 'titus'],\n",
       " [0.5663954749208014, 'tempest'],\n",
       " [0.5663954749208014, 'taming'],\n",
       " [0.5663954749208014, 'merchant'],\n",
       " [0.5663954749208014, 'm_for_m'],\n",
       " [0.5663954749208014, 'hen_vi_1'],\n",
       " [0.5663954749208014, 'hen_iv_2'],\n",
       " [0.5663954749208014, 'coriolan'],\n",
       " [0.5663954749208014, 'com_err'],\n",
       " [0.5663954749208014, 'a_and_c'],\n",
       " [0, 'two_gent'],\n",
       " [0, 'troilus'],\n",
       " [0, 'timon'],\n",
       " [0, 't_night'],\n",
       " [0, 'pericles'],\n",
       " [0, 'othello'],\n",
       " [0, 'much_ado'],\n",
       " [0, 'm_wives'],\n",
       " [0, 'lll'],\n",
       " [0, 'j_caesar'],\n",
       " [0, 'hen_viii'],\n",
       " [0, 'hen_v'],\n",
       " [0, 'hen_iv_1'],\n",
       " [0, 'hamlet'],\n",
       " [0, 'dream'],\n",
       " [0, 'as_you']]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TF_IDF('despair')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### 9.2 TF-IDF scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getKey(list):\n",
    "    \"\"\"\n",
    "    Returns second term of list or tuple; used for sorting\n",
    "    \"\"\"\n",
    "    return list[1]\n",
    "\n",
    "def TF_IDF_search(query):\n",
    "    \"\"\"\n",
    "    Returns a list of tuples of all documents with their sum\n",
    "    of TF-IDF scores for the whole query. Sorted by score\n",
    "    \"\"\"\n",
    "    dic = {}\n",
    "    for doc in doc_set:\n",
    "        dic[doc]=0\n",
    "    for term in query:\n",
    "        TFIDF = TF_IDF(term)\n",
    "        for elem in TFIDF:\n",
    "            if elem[0]==0:\n",
    "                break\n",
    "            else:\n",
    "                dic[elem[1]]+=elem[0]\n",
    "    return sorted(dic.items(), reverse=True, key=getKey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('hen_iv_1', 9.793193846706075)\n",
      "('rich_iii', 8.983176289127522)\n",
      "('m_wives', 7.448460637417568)\n",
      "('hamlet', 7.084728896995646)\n",
      "('lll', 4.20839040710336)\n",
      "('hen_iv_2', 4.20839040710336)\n",
      "('rich_ii', 3.398372849524809)\n",
      "('tempest', 3.278263191760636)\n",
      "('m_for_m', 3.278263191760636)\n",
      "('othello', 3.199111882155336)\n",
      "('com_err', 2.9964481476564573)\n",
      "('taming', 2.9964481476564573)\n",
      "('timon', 2.679842909235256)\n",
      "('hen_vi_3', 2.5092039823409564)\n",
      "('lear', 2.1864305900779053)\n",
      "('john', 1.9428085074201549)\n",
      "('troilus', 1.9428085074201549)\n",
      "('r_and_j', 1.8636571978148544)\n",
      "('as_you', 1.8636571978148544)\n",
      "('win_tale', 1.6991864247624044)\n",
      "('macbeth', 1.3764130324993533)\n",
      "('a_and_c', 1.3764130324993533)\n",
      "('titus', 1.3764130324993533)\n",
      "('merchant', 1.3764130324993533)\n",
      "('hen_vi_2', 1.3764130324993533)\n",
      "('dream', 1.2972617228940528)\n",
      "('all_well', 1.132790949841603)\n",
      "('cymbelin', 1.132790949841603)\n",
      "('hen_vi_1', 1.132790949841603)\n",
      "('hen_v', 1.0536396402363024)\n",
      "('j_caesar', 1.0536396402363024)\n",
      "('coriolan', 0.810017557578552)\n",
      "('two_gent', 0.810017557578552)\n",
      "('t_night', 0.5663954749208014)\n",
      "('much_ado', 0.487244165315501)\n",
      "('hen_viii', 0)\n",
      "('pericles', 0)\n"
     ]
    }
   ],
   "source": [
    "returnval = TF_IDF_search(['test', 'despair', 'fat', 'beast'])\n",
    "for elem in returnval:\n",
    "    print (elem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 9.3 euclidean length of docs\n",
    "\n",
    "Function will return a dict, as that seems useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "def euclidean_lengths():\n",
    "    \"\"\"\n",
    "    Returns a dictionary matching every document\n",
    "    with the euclidean length of their vector\n",
    "    \"\"\"\n",
    "    dic={}\n",
    "    for doc in doc_set:\n",
    "        dic[doc]=0\n",
    "    for key in index_with_freq:\n",
    "        for doc in index_with_freq[key]['counter']:\n",
    "            dic[doc] += index_with_freq[key]['counter'][doc]**2\n",
    "    for key in dic:\n",
    "        dic[key] = sqrt(dic[key])\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'john': 1771.741798344217, 'lll': 1852.4702426759789, 'hen_v': 2310.7985632676855, 'macbeth': 1460.7621298486622, 'rich_iii': 2529.0595089874814, 'hen_vi_3': 2102.5584415183325, 'troilus': 2117.9992917845843, 'lear': 2162.9942209816463, 'all_well': 2011.9169962997976, 'win_tale': 2088.30337834329, 'hen_viii': 2117.5880146997433, 'cymbelin': 2259.077909236421, 'a_and_c': 2062.9069295535364, 'titus': 1752.0644965297367, 'j_caesar': 1714.6008281813001, 'pericles': 1527.190230455918, 'much_ado': 1900.3881182537425, 'hamlet': 2665.07917330799, 'merchant': 1917.1390142605726, 't_night': 1792.8792485831275, 'hen_vi_2': 2173.9512414035416, 'hen_iv_2': 2310.8329667026997, 'coriolan': 2372.4721283926606, 'tempest': 1355.7916506602332, 'm_wives': 2020.1821700034875, 'dream': 1387.939840194812, 'hen_iv_1': 2173.7451552562457, 'com_err': 1462.9962405966735, 'hen_vi_1': 1822.1355602698718, 'm_for_m': 1917.6334373388465, 'rich_ii': 1956.5198695643242, 'two_gent': 1501.6590824817729, 'r_and_j': 1986.4090716667602, 'as_you': 1955.6847905529153, 'othello': 2297.0711786969077, 'taming': 1843.2970460563322, 'timon': 1492.1628597442036}\n"
     ]
    }
   ],
   "source": [
    "print(euclidean_lengths())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 9.4 Cosine Similarity\n",
    "\n",
    "*If a word appears multiple times in the query, the length of the query vector is not just the square root of the length of the query. However, as this would change all results by multiplying them with a constant, it would not interfere with the ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "def cosine_similarity_search(query):\n",
    "    \"\"\"\n",
    "    Performs a search with given query, returning documents\n",
    "    in order of their cosine similarity to the query\n",
    "    \"\"\"\n",
    "    lengths = euclidean_lengths()\n",
    "    \n",
    "    # for querylength we assume the query to consist of unique words, for justification check markdown box above\n",
    "    querylength = sqrt(len(query))\n",
    "    \n",
    "    TF_IDF_results = TF_IDF_search(query)\n",
    "    final_result = []\n",
    "    for result in TF_IDF_results:\n",
    "        final_result.append([result[0],result[1] / (lengths[result[0]] * querylength)])\n",
    "    #sort again\n",
    "    return sorted(final_result, reverse=True, key=getKey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hen_iv_1 0.00225260855050684\n",
      "m_wives 0.001843512121831248\n",
      "rich_iii 0.001775991481656351\n",
      "hamlet 0.0013291779411194436\n",
      "tempest 0.0012089848724780876\n",
      "lll 0.001135886102284738\n",
      "com_err 0.0010240792370164859\n",
      "hen_iv_2 0.0009105786674638504\n",
      "timon 0.000897972661541332\n",
      "rich_ii 0.0008684738914206772\n",
      "m_for_m 0.0008547679467641046\n",
      "taming 0.0008127957873277263\n",
      "othello 0.0006963458319933607\n",
      "hen_vi_3 0.0005967025536110594\n",
      "john 0.0005482764218905397\n",
      "lear 0.0005054175755230688\n",
      "as_you 0.0004764717726541089\n",
      "macbeth 0.00047112839399866986\n",
      "r_and_j 0.000469102065731882\n",
      "dream 0.00046733355629879767\n",
      "troilus 0.00045864238835112707\n",
      "win_tale 0.00040683418951091696\n",
      "titus 0.00039279747841063344\n",
      "merchant 0.0003589758025529063\n",
      "a_and_c 0.0003336100656749558\n",
      "hen_vi_2 0.00031656943501886376\n",
      "hen_vi_1 0.00031084156814156793\n",
      "j_caesar 0.0003072550832003015\n",
      "all_well 0.00028152029927799383\n",
      "two_gent 0.0002697075411550291\n",
      "cymbelin 0.00025071976163595254\n",
      "hen_v 0.00022798171527905862\n",
      "coriolan 0.00017071171203333278\n",
      "t_night 0.00015795694979692894\n",
      "much_ado 0.0001281959618236372\n",
      "hen_viii 0.0\n",
      "pericles 0.0\n"
     ]
    }
   ],
   "source": [
    "result = cosine_similarity_search(['test', 'despair', 'fat', 'beast'])\n",
    "for [doc,similarity] in result:\n",
    "    print (doc, similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 9.5 Okapi BM25\n",
    "\n",
    "We will need to do some calculations with the values of k1, b, the term frequency, the document vector length and the average document vector length. All of these are calculable except for k1 and b, which are free variables. The wikipedia page suggests putting b=0.75 and k1 being between 1.2 and 2, so we'll take b=0.75 and k1=1.6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "def Okapi_BM25_search(query):\n",
    "    \"\"\"\n",
    "    Performs a search using the given query, using the \n",
    "    Okapi BM25 algorithm to rank the results\n",
    "    \"\"\"\n",
    "    k1=1.6\n",
    "    b=0.75\n",
    "    \n",
    "    lengths = euclidean_lengths()\n",
    "    querylength = sqrt(len(query))\n",
    "    \n",
    "    # calculating the average doc length\n",
    "    s=0\n",
    "    counter=0\n",
    "    for key in lengths:\n",
    "        s += lengths[key]\n",
    "        counter +=1\n",
    "    avg_len = s/counter\n",
    "    \n",
    "    result = []\n",
    "    for doc in doc_set:\n",
    "        subresult = [doc, 0]\n",
    "        for term in query:\n",
    "            if doc in index_with_freq[term]['counter']:\n",
    "                tf = index_with_freq[term]['counter'][doc]\n",
    "                val = (tf * (k1+1)) / (tf + k1 * (1-b+b*(lengths[doc]/avg_len)))\n",
    "                subresult[1]+=val\n",
    "        result.append(subresult)\n",
    "    return sorted(result, reverse=True, key=getKey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "com_err 4.757900788908586\n",
      "hamlet 4.735788265712869\n",
      "rich_iii 4.475000185681662\n",
      "taming 4.466287992275215\n",
      "m_wives 4.218962695507708\n",
      "hen_iv_1 3.991643004338016\n",
      "tempest 3.935030785476478\n",
      "hen_iv_2 3.8369575226705264\n",
      "john 3.5779035786569837\n",
      "m_for_m 3.4678816094848157\n",
      "lll 3.1547609456526753\n",
      "titus 3.1474160300055054\n",
      "as_you 3.1373801507638284\n",
      "r_and_j 3.12291745368682\n",
      "lear 3.042348710037171\n",
      "merchant 3.0234778718563797\n",
      "dream 2.9869408622156985\n",
      "a_and_c 2.9218776135644076\n",
      "hen_vi_3 2.791647389921127\n",
      "othello 2.7129781812814624\n",
      "macbeth 2.707204937926539\n",
      "troilus 2.6201123101368444\n",
      "j_caesar 2.563982455029559\n",
      "win_tale 2.3793607597775392\n",
      "hen_vi_2 2.3408220560348383\n",
      "timon 2.321752809048462\n",
      "hen_v 2.281829347310559\n",
      "two_gent 2.23740263821172\n",
      "hen_vi_1 2.0623911143031286\n",
      "rich_ii 2.051539523818834\n",
      "coriolan 1.818167029385026\n",
      "much_ado 1.4567858550578259\n",
      "all_well 1.429302949090377\n",
      "cymbelin 1.3719446855480477\n",
      "t_night 1.0386120618788026\n",
      "hen_viii 0\n",
      "pericles 0\n"
     ]
    }
   ],
   "source": [
    "result = Okapi_BM25_search(['test', 'despair', 'fat', 'beast'])\n",
    "for [doc, Okapi_score] in result:\n",
    "    print (doc, Okapi_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now on to the Motie files. \n",
    "We start with preparing the corpus, the index and the list of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "global name 'index_with_freq' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-949595eda3fa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mindex_with_freq_motions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madd_frequencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex_motions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mdoc_set_motions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_doc_set\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex_with_freq_motions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc_set_motions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-768e8aedb724>\u001b[0m in \u001b[0;36mmake_doc_set\u001b[1;34m(index_with_frequencies)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mdoc_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mindex_with_freq\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m         \u001b[0mdoc_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex_with_freq\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'counter'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0melements\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: global name 'index_with_freq' is not defined"
     ]
    }
   ],
   "source": [
    "MOTIONS = \"Moties/XML/MOT\"\n",
    "\n",
    "# prepare the files\n",
    "prepare_data(MOTIONS, MOTIONS + \"New\", 0, 1)\n",
    "\n",
    "# create the dinex\n",
    "index_motions = index_collection(MOTIONS + \"New\")\n",
    "\n",
    "index_with_freq_motions = add_frequencies(index_motions)\n",
    "\n",
    "doc_set_motions = make_doc_set(index_with_freq_motions)\n",
    "print(doc_set_motions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### 8.1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print (negation_search('test'))\n",
    "print (docs_with_token('test'))\n",
    "print (negation_search('despair'))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.3.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
