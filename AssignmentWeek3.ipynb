{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment  week 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook made by   (If not filled in correctly: 0 pts for assignment)\n",
    "\n",
    "__Name(s)__: Adriaan de Vries, Verna Dankers\n",
    "\n",
    "__Student id(s)__ : 10795227, 10761225\n",
    "\n",
    "### Pledge (taken from [Coursera's Honor Code](https://www.coursera.org/about/terms/honorcode) )\n",
    "\n",
    "\n",
    "\n",
    "Put here a selfie with your photo where you hold a signed paper with the following text: (if this is team work, put two selfies here). The link must be to some place on the web, not to a local file. **Assignments without the selfies will not be graded and receive 0 points.**\n",
    "\n",
    "> My answers to homework, quizzes and exams will be my own work (except for assignments that explicitly permit collaboration).\n",
    "\n",
    ">I will not make solutions to homework, quizzes or exams available to anyone else. This includes both solutions written by me, as well as any official solutions provided by the course staff.\n",
    "\n",
    ">I will not engage in any other activities that will dishonestly improve my results or dishonestly improve/hurt the results of others.\n",
    "\n",
    "<img width=30% src='http://oi63.tinypic.com/2q3srh3.jpg'/>\n",
    "<img src=\"http://i63.tinypic.com/2itj1ic.jpg\" border=\"0\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Book exercises\n",
    "In this section you will make the exercises from the book, which is freely available online as a PDF at [nlp.stanford.edu/IR-book/](http://nlp.stanford.edu/IR-book/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2.1\n",
    "\n",
    "Are the following statements true or false?\n",
    "\n",
    "1. In a Boolean retrieval system, stemming never lowers precision.\n",
    "2. In a Boolean retrieval system, stemming never lowers recall.\n",
    "3. Stemming increases the size of the vocabulary.\n",
    "4. Stemming should be invoked at indexing time but not while processing a query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1. False, when \"catching\" more documents for a query because of the application of stemming, the number of false positives may increase and the precision may become lower.\n",
    "> 2. True, the application of stemming maps multiple words to one term, one stem. Therefore the same number of documents or more documents will be retrieved when such a word is in the query, but the number of documents retrieved will never become lower.\n",
    "> 3. False, it decreases the size of the vocabulary since multiple words in the text will refer to one stem in the vocabulary.\n",
    "> 4. False, you want to search for the words from the query in your index. When you do not stem the words from the query, it may not be possible to find those words in your index. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2\n",
    "\n",
    "Suggest what normalized form should be used for these words (including the word\n",
    "itself as a possibility):\n",
    "1. ’Cos\n",
    "2. Shi’ite\n",
    "3. cont’d\n",
    "4. Hawai’i\n",
    "5. O’Rourke"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3\n",
    "\n",
    "The following pairs of words are stemmed to the same form by the Porter stemmer.\n",
    "Which pairs would you argue shouldn’t be conflated. Give your reasoning.\n",
    "1. abandon/abandonment\n",
    "2. absorbency/absorbent\n",
    "3. marketing/markets\n",
    "4. university/universe\n",
    "5. volume/volumes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We think that the third pair shouldn't be conflated: marketing refers to the act of selling, \"The action or business of promoting and selling products or services, including market research and advertising\", where market(s) refers to an actual place where people gather to purchase and sell.\n",
    "\n",
    "> We think that the fourth pair shouldn't be conflated: the universe includes all galaxies with all of their planets, but a university is only a building on one of the tiny tiny planets of the universe. Therefore these words have very different meanings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4\n",
    "\n",
    "For the Porter stemmer rule group shown in (2.1):\n",
    "1. What is the purpose of including an identity rule such as SS → SS?\n",
    "2. Applying just this rule group, what will the following words be stemmed to?\n",
    "   > _circus_ _canaries_ _boss_\n",
    "3. What rule should be added to correctly stem pony?\n",
    "4. The stemming for ponies and pony might seem strange. Does it have a deleterious effect on retrieval? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8\n",
    "\n",
    "Assume a biword index. Give an example of a document which will be returned\n",
    "for a query of New York University but is actually a false positive which should not be\n",
    "returned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> For this query the index will be searched for \"New York\" and \"York University\". That can give hits like the following two websites, which are about events in New York organised by the York University (which is in Toronto, and not in New York).  http://alumniandfriends.yorku.ca/event/new-york-alumni-reception/, http://alumniandfriends.yorku.ca/event/new-york-alumni-reception/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.9\n",
    "\n",
    "Shown below is a portion of a positional index in the format:\n",
    "> term: doc1: $\\langle$ position1, position2, . . . $\\rangle$; doc2: $\\langle$ position1, position2, . . .$\\rangle$; etc.\n",
    "\n",
    "angels: 2: $\\langle$36,174,252,651$\\rangle$; 4: $\\langle$12,22,102,432$\\rangle$; 7: $\\langle$17$\\rangle$;<br/>\n",
    "fools: 2: $\\langle$1,17,74,222$\\rangle$; 4: $\\langle$8,78,108,458$\\rangle$; 7: $\\langle$3,13,23,193$\\rangle$;<br/>\n",
    "fear: 2: $\\langle$87,704,722,901$\\rangle$; 4: $\\langle$13,43,113,433$\\rangle$; 7: $\\langle$18,328,528$\\rangle$;<br/>\n",
    "in: 2: $\\langle$3,37,76,444,851$\\rangle$; 4: $\\langle$10,20,110,470,500$\\rangle$; 7: $\\langle$5,15,25,195$\\rangle$;<br/>\n",
    "rush: 2: $\\langle$2,66,194,321,702$\\rangle$; 4: $\\langle$9,69,149,429,569$\\rangle$; 7: $\\langle$4,14,404$\\rangle$;<br/>\n",
    "to: 2: $\\langle$47,86,234,999$\\rangle$; 4: $\\langle$14,24,774,944$\\rangle$; 7: $\\langle$199,319,599,709$\\rangle$;<br/>\n",
    "tread: 2: $\\langle$57,94,333$\\rangle$; 4: $\\langle$15,35,155$\\rangle$; 7: $\\langle$20,320$\\rangle$;<br/>\n",
    "where: 2: $\\langle$67,124,393,1001$\\rangle$; 4: $\\langle$11,41,101,421,431$\\rangle$; 7: $\\langle$16,36,736$\\rangle$;\n",
    "\n",
    "Which document(s) if any match each of the following queries, where each expression\n",
    "within quotes is a phrase query?\n",
    "1. “fools rush in”\n",
    "2. “fools rush in” AND “angels fear to tread”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Documents 2, 4 and 7 all match 'fools rush in'. Only document 4 also matches 'angels fear to tread' and thus also matches \"'fools rush in' AND 'angels fear to tread'\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2.10\n",
    "\n",
    "Consider the following fragment of a positional index with the format:\n",
    "\n",
    "> word: document: $\\langle$position, position, . . .$\\rangle$; document: $\\langle$position, . . .$\\rangle$<br />\n",
    ". . .\n",
    "\n",
    ">Gates: 1: $\\langle$3$\\rangle$; 2: $\\langle$6$\\rangle$; 3: $\\langle$2,17$\\rangle$; 4: $\\langle$1$\\rangle$;<br />\n",
    "IBM: 4: $\\langle$3$\\rangle$; 7: $\\langle$14$\\rangle$;<br />\n",
    "Microsoft: 1: $\\langle$1$\\rangle$; 2: $\\langle$1,21$\\rangle$; 3: $\\langle$3$\\rangle$; 5: $\\langle$16,22,51$\\rangle$;\n",
    "\n",
    "The $/k$ operator, word1 $/k$ word2 finds occurrences of word1 within $k$ words of word2 (on either side), where $k$ is a positive integer argument. Thus $k = 1$ demands that word1 be adjacent to word2.\n",
    "1. Describe the set of documents that satisfy the query Gates $/2$ Microsoft.\n",
    "2. Describe each set of values for $k$ for which the query Gates $/k$ Microsoft returns a different set of documents as the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1 Documents 1 and 3\n",
    "\n",
    "> 2\n",
    "- k = 1 --> document 3 \n",
    "- 2 $\\leq$ k $\\leq$ 4 --> documents 1 and 3 \n",
    "- k $>$ 4 --> documents 1, 2 and 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1\n",
    "An IR system returns 8 relevant documents, and 10 nonrelevant documents. There are a total of 20 relevant documents in the collection. What is the precision of the system on this search, and what is its recall?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $precision = \\frac{8}{10} = \\frac{4}{5}$\n",
    "\n",
    "> $recall = \\frac{8}{20} = \\frac{2}{5}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2\n",
    "The balanced F measure (a.k.a. $F_1$) is defined as the harmonic mean of precision and recall. What is the advantage of using the harmonic mean rather than “averaging” (using the arithmetic mean)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3\n",
    "Derive the equivalence between the two formulas for F measure shown in Equation (8.5), given that $\\alpha = 1/(\\beta^2 + 1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4\n",
    "What are the possible values for interpolated precision at a recall level of 0?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.8\n",
    "Consider an information need for which there are 4 relevant documents in the collection. Contrast two systems run on this collection. Their top 10 results are judged for relevance as follows (the leftmost item is the top ranked search result):\n",
    "\n",
    "| System   | R for relevant, N for nonrelevant |\n",
    "|----------|:--------------------|\n",
    "| System 1 | R N R N N N N N R R |\n",
    "| System 2 | N R N N R R R N N N |\n",
    "\n",
    "1. What is the MAP of each system? Which has a higher MAP?\n",
    "2. Does this result intuitively make sense? What does it say about what is important in getting a good MAP score?\n",
    "3. What is the R-precision of each system? (Does it rank the systems the same as MAP?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.9\n",
    "\n",
    "The following list of Rs and Ns represents relevant (R) and nonrelevant (N) returned documents in a ranked list of 20 documents retrieved in response to a query from a collection of 10,000 documents. The top of the ranked list (the document the system thinks is most likely to be relevant) is on the left of the list. This list shows 6 relevant documents. Assume that there are 8 relevant documents in total in the collection.\n",
    "\n",
    " > R R N N N $\\quad$ N N N R N $\\quad$ R N N N R $\\quad$ N N N N R\n",
    "\n",
    "1. What is the precision of the system on the top 20?\n",
    "2. What is the F1 on the top 20?\n",
    "3. What is the uninterpolated precision of the system at 25% recall?\n",
    "4. What is the interpolated precision at 33% recall?\n",
    "5. Assume that these 20 documents are the complete result set of the system. What is the MAP for the query?<br/><br/>\n",
    "Assume, now, instead, that the system returned the entire 10,000 documents in a ranked list, and these are the first 20 results returned.<br/><br/>\n",
    "6. What is the largest possible MAP that this system could have?\n",
    "7. What is the smallest possible MAP that this system could have?\n",
    "8. In a set of experiments, only the top 20 results are evaluated by hand. The result in (e) is used to approximate the range (f)–(g). For this example, how large (in absolute terms) can the error for the MAP be by calculating (e) instead of (f) and (g) for this query?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.10\n",
    "Below is a table showing how two human judges rated the relevance of a set of 12 documents to a particular information need (0 = nonrelevant, 1 = relevant). Let us assume that you’ve written an IR system that for this query returns the set of documents {4, 5, 6, 7, 8}\n",
    "\n",
    "| docId | Judge 1 | Judge 2|\n",
    "|------:|:--------|:-------|\n",
    "| 1     | 0       | 0      |\n",
    "| 2     | 0       | 0      |\n",
    "| 3     | 1       | 1      |\n",
    "| 4     | 1       | 1      |\n",
    "| 5     | 1       | 0      |\n",
    "| 6     | 1       | 0      |\n",
    "| 7     | 1       | 0      |\n",
    "| 8     | 1       | 0      |\n",
    "| 9     | 0       | 1      |\n",
    "| 10    | 0       | 1      |\n",
    "| 11    | 0       | 1      |\n",
    "| 12    | 0       | 1      |\n",
    "\n",
    "1. Calculate the kappa measure between the two judges.\n",
    "2. Calculate precision, recall, and F1 of your system if a document is considered relevant only if the two judges agree.\n",
    "3. Calculate precision, recall, and F1 of your system if a document is considered relevant if either judge thinks it is relevant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1. \n",
    "> 2. There are only two relevant documents, one of which is returned. \n",
    "    - Precision: $\\frac{1}{5}$\n",
    "    - Recall: $\\frac{1}{2}$\n",
    "    - F1: $\\frac{2 * 0.2 * 0.5}{0.2+0.5} = \\frac{0.2}{0.7} = \\frac{2}{7}$\n",
    "> 3. Now there are 10 relevant documents, 5 of which are returned\n",
    "    - Precision: $\\frac{5}{5} = 1$\n",
    "    - Recall: $\\frac{5}{10} = 0.5$\n",
    "    - F1: $ \\frac{2 * 0.5 * 1}{0.5 + 1} = \\frac{1}{1.5} = \\frac{2}{3}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming (Inverted Index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10 Evaluation\n",
    "\n",
    "Using your code from last week's assignment, implement two search engines for the set of moties: one using cosine similarity and the other using BM25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from collections import Counter, defaultdict\n",
    "from math import sqrt, log\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "CORPUS = \"Shakespeare\"\n",
    "\n",
    "def normalize(word, keep_capitals):\n",
    "    \"\"\"\n",
    "    Normalize capital letters and punctuation.\n",
    "    \"\"\"\n",
    "    if (keep_capitals and not word == word.upper()) or not keep_capitals:\n",
    "        word = word.lower()\n",
    "    word = re.sub(r'[^\\w\\s]', '', word)\n",
    "    return word\n",
    "\n",
    "def prepare_data(folder, new_folder, keep_capitals, byte):\n",
    "    \"\"\"\n",
    "    Prepare a text corpus for the process of making an inverted index.\n",
    "    \"\"\"\n",
    "    # Create a directory for the processed files\n",
    "    if not os.path.exists(new_folder):\n",
    "        os.makedirs(new_folder)\n",
    "\n",
    "    # Prepare each file one by one\n",
    "    for filename in os.listdir(folder):\n",
    "        if not os.path.exists(os.path.join(new_folder, filename[:-4] + \".pkl\")):\n",
    "            # Strip xml tags\n",
    "            if byte:\n",
    "                xmlfile = BeautifulSoup(open(os.path.join(folder, filename), 'rb'),\"xml\")\n",
    "            else:\n",
    "                xmlfile = BeautifulSoup(open(os.path.join(folder, filename), 'r'),\"xml\")\n",
    "            textfile = xmlfile.get_text()\n",
    "\n",
    "            # Split the text on whitespaces\n",
    "            beautifile = WhitespaceTokenizer().tokenize(textfile)\n",
    "\n",
    "            # Normalize text for capital letters, non-alphabetical tokens and whitespace\n",
    "            finalfile = [ normalize(word, keep_capitals)\n",
    "                          for word \n",
    "                          in beautifile \n",
    "                         ]\n",
    "            \n",
    "            savefile = open(os.path.join(new_folder, filename[:-4] + \".pkl\"), 'wb')\n",
    "            pickle.dump(finalfile, savefile)\n",
    "\n",
    "def index_collection(folder):\n",
    "    \"\"\"\n",
    "    Create an inverted index given a folder \n",
    "    containig a text corpus.\n",
    "    \"\"\"\n",
    "    index = defaultdict(Counter) # initialize MyIndex\n",
    "    for filename in os.listdir(folder):\n",
    "        text = pickle.load(open(os.path.join(folder,filename),'rb'))\n",
    "        \n",
    "        # Update the index with each token\n",
    "        for w in text:    \n",
    "            index[w][filename[:-4]]+=1  \n",
    "    \n",
    "    index = add_frequencies(index)\n",
    "    \n",
    "    if '' in index.keys():\n",
    "        del index['']\n",
    "    return index\n",
    "\n",
    "def make_doc_set(index):\n",
    "    \"\"\"\n",
    "    Create a set containing all documents that\n",
    "    are present in a given index.\n",
    "    \"\"\"\n",
    "    doc_list = []\n",
    "    for token in index:\n",
    "        doc_list.extend(list(index[token]['counter'].elements()))\n",
    "    return set(doc_list)\n",
    "\n",
    "def add_frequencies(index):\n",
    "    \"\"\"\n",
    "    Add the document and corpus frequency to a \n",
    "    word in an inverted index.\n",
    "    \"\"\"\n",
    "    for word in index:\n",
    "        original_counter = index[word]\n",
    "        document_frequency = len(index[word])\n",
    "        corpus_frequency = sum([index[word][document] \n",
    "                                for document \n",
    "                                in index[word]])\n",
    "        index[word] = {\"counter\" : original_counter, \n",
    "                       \"doc_freq\" : document_frequency, \n",
    "                       \"corp_freq\" : corpus_frequency\n",
    "                      }\n",
    "    return index\n",
    "\n",
    "def TF_IDF(term, doc_set, index):\n",
    "    \"\"\"\n",
    "    Return the tf-idf weight of a given term.\n",
    "    \"\"\"\n",
    "    # catch off terms that aren't in the index\n",
    "    badresult = []\n",
    "    \n",
    "    if not term in index:\n",
    "        for doc in doc_set:\n",
    "            badresult.append([0, doc])\n",
    "        return badresult\n",
    "    \n",
    "    rated_docs = []\n",
    "    IDF = 0\n",
    "    length_termdocs = len(docs_with_token(index, term))\n",
    "    if not length_termdocs == 0:\n",
    "        IDF = log(len(doc_set) / length_termdocs)\n",
    "    for doc in doc_set:\n",
    "        if doc in index[term]['counter']:\n",
    "            rated_docs.append([index[term]['counter'][doc]*IDF,doc])\n",
    "        else:\n",
    "            rated_docs.append([0,doc])\n",
    "    return sorted(rated_docs, reverse=True)\n",
    "\n",
    "def docs_with_token(index, token):\n",
    "    \"\"\"\n",
    "    Return all documents that contain a\n",
    "    certain token.\n",
    "    \"\"\"\n",
    "    if token in index:\n",
    "        return set(index[token]['counter'].elements())\n",
    "    else:\n",
    "        return set()\n",
    "\n",
    "def getKey(list_):\n",
    "    return list_[1]\n",
    "\n",
    "def TF_IDF_search(query, doc_set, index):\n",
    "    \"\"\"\n",
    "    For all documents calculate how they\n",
    "    score compared to the query, by tf-idf weighting.\n",
    "    \"\"\"\n",
    "    dic = {}\n",
    "    for doc in doc_set:\n",
    "        dic[doc]=0\n",
    "    for term in query:\n",
    "        TFIDF = TF_IDF(term, doc_set, index)\n",
    "        for elem in TFIDF:\n",
    "            if elem[0]==0:\n",
    "                break\n",
    "            else:\n",
    "                dic[elem[1]]+=elem[0]\n",
    "    return sorted(dic.items(), reverse=True, key=getKey)\n",
    "\n",
    "def euclidean_lengths(doc_set, index):\n",
    "    \"\"\"\n",
    "    Compute for each document its Euclidean length.\n",
    "    \"\"\"\n",
    "    dic={}\n",
    "    for doc in doc_set:\n",
    "        dic[str(doc)] = 0\n",
    "    for key in index:\n",
    "        for doc in index[key]['counter']:\n",
    "            dic[str(doc)] += index[key]['counter'][str(doc)]**2\n",
    "    for key in dic:\n",
    "        dic[key] = sqrt(dic[key])\n",
    "    return dic\n",
    "\n",
    "def cosine_similarity_search(query, doc_set, index):\n",
    "    \"\"\"\n",
    "    For given query, return the top 10 documents using Cosine Similarity\n",
    "    \"\"\"\n",
    "    lengths = euclidean_lengths(doc_set, index)\n",
    "    \n",
    "    # For querylength we assume the query to consist of unique words, \n",
    "    # for justification check markdown box above\n",
    "    querylength = sqrt(len(query))\n",
    "    \n",
    "    TF_IDF_results = TF_IDF_search(query, doc_set, index)\n",
    "    final_result = []\n",
    "    for result in TF_IDF_results:\n",
    "        final_result.append([result[0],result[1] / (lengths[result[0]] * querylength)])\n",
    "    \n",
    "    # Sort again\n",
    "    return sorted(final_result, reverse=True, key=getKey)[:10]\n",
    "\n",
    "def Okapi_BM25_search(query, doc_set, index):\n",
    "    \"\"\"\n",
    "    For given query, return the top 10 documents using BM25\n",
    "    \"\"\"\n",
    "    k1 = 1.6\n",
    "    b = 0.75\n",
    "    \n",
    "    lengths = euclidean_lengths(doc_set, index)\n",
    "    querylength = sqrt(len(query))\n",
    "    \n",
    "    # Calculating the average doc length\n",
    "    s = 0\n",
    "    counter = 0\n",
    "    for key in lengths:\n",
    "        s += lengths[key]\n",
    "        counter += 1\n",
    "    avg_len = s/counter\n",
    "    \n",
    "    result = []\n",
    "    for doc in doc_set:\n",
    "        subresult = [str(doc), 0]\n",
    "        for term in query:\n",
    "            if term in index:\n",
    "                if doc in index[term]['counter']:\n",
    "                    tf = index[term]['counter'][str(doc)]\n",
    "                    val = (tf * (k1+1)) / (tf + k1 * (1-b+b*(lengths[str(doc)]/avg_len)))\n",
    "                    subresult[1]+=val\n",
    "        result.append(subresult)\n",
    "    return sorted(result, reverse=True, key=getKey)[:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1 Creating a test set\n",
    "\n",
    "Create a test collection in order to evaluate your two search engines (cosine similarity and BM25) on the set of moties. Create five information needs, which you express as queries. Use multi term queries only. For each information need you specify\n",
    "\n",
    " * The information need in natural language\n",
    " * The search query\n",
    " * A description when a document is considered relevant and when it is not. This should be a very clear description. A person assessing relevance may only use this together with the information need to determine relevance of a document. In particular the judge should not see the query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1.\n",
    "- Natural language: we want information about motions about the alcohol consumption of youngsters.\n",
    "- Search query: [alcoholconsumptie, alcohol, jeugd, jongeren]\n",
    "- A document is considered relevant if it is a motion containing information about the alcohol consumption of people aged 0-25. The document is not considered relevant if there is only information about youngsters, or if there is only information about alcohol consumption of older people or when the age of the consumers is not specified.\n",
    "\n",
    "> 2.\n",
    "- Natural language: we want information about motions about the financing of institutes of education.\n",
    "- Search query: [financiering, geld, school, onderwijs, hogeschool, mbo, universiteit]\n",
    "- A document is considered relevant if it is a motion about the financing of all kinds of dutch institutes of education, including elemtary school, high school, colleges and universities. A document is not considered relevant if it is about just financing institutes, or about just institutes of education.\n",
    "\n",
    "> 3.\n",
    "- Natural language: We want information about European regulations concerning cows and their milk.\n",
    "- Search query: [cow, milk, european, europe, regulations]\n",
    "- Relevance: The document is considered relevant if the motion mentions some European regulation for the keeping of cows and their milk. If it only mentions cows but not milk, it's not relevant. If it mentions cattle and milk, but not cows specifically it is considered relevant, as cows are implied.\n",
    "\n",
    "> 4.\n",
    "- Natural language: Information is needed on proposed changes to the Dutch voting system.\n",
    "- Search query: [regulering, stemmen, democratie, stemhokje, potlood, democratie, stemgerechtigden, verandering]\n",
    "- Relevance: The document is considered relevant if the motion proposes some form, however small, of change to the working of the Dutch democracy. For example, a proposed change in the color of the pencil used would be relevant. On the other hand, a motion proposing to keep things the way they are will not be considered relevant.\n",
    "\n",
    "> 5.\n",
    "- Natural language: We need information on advances in medical science.\n",
    "- Search query: [medisch, geneeskunde, gezondheid, wetenschap, wetenschappelijk, onderzoek, vooruitgang]\n",
    "- Relevance: The document is considered relevant if the motion mentions advances, however small, in medical science. Other scientific advances are not considered relevant.\n",
    "\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2 Assess search engine results\n",
    "\n",
    "Assess for your two search engines, for all five information needs the first 10 results given back by the systems. Let another student do the same task as well.\n",
    "1. Using your own relevance judgments, compute for each system, the precision at 10 for each query, and the average P@10. Discuss the results.\n",
    "2. You have now two times at least 50 relevance judgements, and probably more. Compute the kappa measure between the two judges as in exercise 8.10, using pooled marginals as in Table 8.2. Discuss your results and try to explain them.\n",
    "3. Now compute P@10 for both systems again, once using the relevance judgments of the other assessor, once in which you say that a document is relevant if BOTH judges said it was relevant, and once in which you say that a document is relevant if AT LEAST ONE of the judges said it was relevant.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MOTIONS = \"Moties/XML/MOT\"\n",
    "\n",
    "prepare_data(MOTIONS, MOTIONS + \"New\", 0, 1)\n",
    "index_motions = dict(index_collection(MOTIONS + \"New\"))\n",
    "\n",
    "doc_set_motions = make_doc_set(index_motions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 \t6235623\t\t6235623\n",
      "2 \t6174873\t\t6206569\n",
      "3 \t6174893\t\t6206861\n",
      "4 \t6206569\t\t6174873\n",
      "5 \t2008473\t\t6174893\n",
      "6 \t6235595\t\t1804628\n",
      "7 \t1804628\t\t2008473\n",
      "8 \t6206576\t\t6235595\n",
      "9 \t2008494\t\t6206576\n",
      "10 \t6235557\t\t6235557\n"
     ]
    }
   ],
   "source": [
    "query_alcohol = ['alcoholconsumptie', 'alcohol', 'jeugd', 'jongeren']\n",
    "cos_top10_alcohol = cosine_similarity_search(query_alcohol, doc_set_motions, index_motions)\n",
    "okapi_top10_alcohol = Okapi_BM25_search(query_alcohol, doc_set_motions, index_motions)\n",
    "\n",
    "for i in range(10):\n",
    "    print(i+1, '\\t'+cos_top10_alcohol[i][0] + '\\t\\t' + okapi_top10_alcohol[i][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12 Positional inverted index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjust your software making an inverted index for the Shakespeare collection: the output should be a positional index. A positional index is an index that also stores the position of the occurrences of a word in a document. The following figure shows an example of a positional index. Other data structures/formats are also allowed.\n",
    "\n",
    "<img src=\"http://nlp.stanford.edu/IR-book/html/htmledition/img121.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def positional_index(folder):\n",
    "    \"\"\"\n",
    "    Create an inverted index given a folder \n",
    "    containig a text corpus.\n",
    "    \"\"\"\n",
    "    index = defaultdict() # Initialize MyIndex\n",
    "    \n",
    "    for filename in os.listdir(folder):\n",
    "        text = pickle.load(open(os.path.join(folder,filename),'rb'))\n",
    "        unique_words = set(text)\n",
    "        \n",
    "        # Update the index with each token\n",
    "        for i, w in enumerate(text):  \n",
    "            if not w in index:\n",
    "                index[w] = defaultdict()\n",
    "            if not filename[:-4] in index[w]:\n",
    "                index[w][filename[:-4]] = dict({\"term_freq\" : 0, \"pos\" : []})\n",
    "            index[w][filename[:-4]][\"term_freq\"] += 1\n",
    "            index[w][filename[:-4]][\"pos\"].append(i)\n",
    "    \n",
    "    index = add_frequencies(index)\n",
    "    \n",
    "    if '' in index.keys():\n",
    "        del index['']\n",
    "    return index\n",
    "\n",
    "def add_frequencies(index):\n",
    "    \"\"\"\n",
    "    Add the document and corpus frequency to a \n",
    "    word in an inverted index.\n",
    "    \"\"\"\n",
    "    for word in index:\n",
    "        original_counter = index[word]\n",
    "        document_frequency = len(index[word])\n",
    "        corpus_frequency = sum([index[word][document][\"term_freq\"] \n",
    "                                for document \n",
    "                                in index[word]])\n",
    "        index[word] = {\"counter\" : original_counter, \n",
    "                       \"doc_freq\" : document_frequency, \n",
    "                       \"corp_freq\" : corpus_frequency\n",
    "                      }\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CORPUS = \"Shakespeare\"\n",
    "prepare_data(CORPUS, CORPUS + \"New\", 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Creating the positional index took 1.9019665718078613 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "pos_index = positional_index(CORPUS + \"New\")\n",
    "end = time.time()\n",
    "\n",
    "print(\"\\n Creating the positional index took \" + str(end-start) + \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'corp_freq': 62, 'doc_freq': 1, 'counter': defaultdict(None, {'a_and_c': {'pos': [75, 12854, 12861, 12862, 12872, 12890, 12977, 13011, 13043, 15257, 15259, 15291, 15309, 15366, 15393, 15425, 18624, 18627, 18637, 18641, 18643, 18719, 18751, 18936, 18983, 19081, 19103, 19584, 19585, 20720, 20721, 20822, 20884, 20997, 21000, 21005, 21062, 21090, 21100, 21153, 21171, 21294, 21351, 21369, 21460, 21461, 21463, 21464, 21532, 21584, 21607, 21649, 21680, 21720, 21743, 21763, 21789, 21800, 21801, 21826, 21837, 21870], 'term_freq': 62}})}\n"
     ]
    }
   ],
   "source": [
    "print(pos_index['eros'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'corp_freq': 528, 'doc_freq': 14, 'counter': defaultdict(None, {'j_caesar': {'pos': [4, 62, 64, 75, 95, 140, 161, 448, 798, 820, 825, 827, 833, 848, 851, 882, 889, 899, 900, 913, 934, 936, 947, 962, 980, 981, 997, 1277, 1433, 1593, 1630, 1698, 1735, 1756, 1897, 1954, 1960, 2005, 2022, 2270, 2299, 2367, 2370, 2371, 2409, 2421, 2549, 2571, 2600, 2860, 2869, 2906, 2933, 2962, 3126, 3434, 3501, 3541, 3808, 4155, 4215, 4353, 4422, 5037, 5097, 5378, 6140, 6157, 6160, 6194, 6230, 6247, 6270, 6273, 6388, 6401, 6415, 6478, 6643, 7650, 7654, 7676, 7685, 7712, 7727, 7728, 7749, 7754, 7850, 7862, 7876, 7891, 7910, 7989, 7998, 8016, 8024, 8046, 8103, 8134, 8140, 8149, 8197, 8199, 8224, 8232, 8248, 8402, 8439, 8484, 8492, 8497, 8518, 8564, 8565, 8581, 8602, 8606, 8635, 8636, 8681, 8701, 8730, 8753, 8801, 8828, 8858, 9015, 9090, 9120, 9133, 9139, 9186, 9217, 9258, 9318, 9334, 9346, 9352, 9374, 9384, 9389, 9390, 9402, 9406, 9429, 9456, 9482, 9504, 9530, 9570, 9590, 9600, 9615, 9626, 9662, 9716, 9762, 9775, 9780, 9781, 9933, 9934, 9944, 9945, 9965, 9966, 9972, 10268, 10380, 10398, 10419, 10431, 10551, 10671, 10758, 10838, 10935, 11080, 11099, 11145, 11168, 11194, 11314, 11367, 11605, 11615, 11644, 11963, 11978, 11987, 11998, 12007, 12016, 12142, 12279, 12445, 12496, 12522, 12529, 12544, 12614, 12623, 12742, 12772, 12862, 12945, 13049, 13069, 13145, 13191, 13290, 13343, 13375, 13390, 13430, 13507, 13738, 13799, 13858, 13867, 13917, 14094, 15717, 16104, 17554, 18032, 18102, 18167, 18193, 18266, 18276, 19318, 19737, 20318, 20621, 20796], 'term_freq': 237}, 'rich_iii': {'pos': [13534, 13653, 25415], 'term_freq': 3}, 'othello': {'pos': [9670], 'term_freq': 1}, 'hen_vi_3': {'pos': [11579, 24565], 'term_freq': 2}, 'hen_vi_1': {'pos': [773, 2936], 'term_freq': 2}, 'm_for_m': {'pos': [5549, 12114], 'term_freq': 2}, 'macbeth': {'pos': [7557], 'term_freq': 1}, 'hamlet': {'pos': [15647, 28017], 'term_freq': 2}, 'a_and_c': {'pos': [65, 92, 101, 118, 139, 371, 420, 669, 945, 1428, 2207, 3245, 3254, 3374, 3543, 3568, 3583, 3648, 3715, 3849, 3883, 3908, 3941, 4183, 4228, 4506, 4512, 4527, 4540, 4725, 4749, 4933, 4947, 5083, 5176, 5179, 5195, 5284, 5293, 5300, 5321, 5371, 5377, 5416, 5538, 5658, 5691, 5778, 5799, 5823, 5830, 6039, 6093, 6095, 6117, 6139, 6265, 6268, 6302, 6306, 6356, 6458, 6472, 6515, 6552, 6568, 6988, 7139, 7217, 7222, 7989, 8020, 8027, 8094, 8574, 8697, 8724, 8805, 8891, 8968, 8988, 9039, 9082, 9164, 9228, 9278, 9336, 9435, 9698, 9815, 10046, 10063, 10345, 10780, 10782, 10808, 10964, 11280, 11498, 11530, 11542, 11561, 11577, 11579, 11597, 11625, 11665, 11677, 11772, 11815, 11875, 11922, 11968, 12012, 12062, 12081, 12873, 12891, 13015, 13055, 13060, 13143, 13218, 13235, 13236, 13291, 13350, 13367, 13374, 13376, 13396, 13549, 13570, 13595, 13695, 13836, 14157, 14294, 14319, 14345, 14586, 14594, 14600, 14701, 14989, 15089, 15654, 15660, 15672, 15711, 15745, 15822, 15871, 15941, 15945, 15966, 16128, 16197, 16234, 16281, 16286, 16300, 16408, 16415, 16445, 16459, 16556, 16627, 16846, 17181, 17212, 17355, 17456, 17759, 17767, 17772, 17801, 17822, 17856, 18957, 19080, 19156, 19164, 19184, 19189, 19221, 19270, 19279, 19498, 20213, 20407, 20412, 20662, 21158, 21522, 21576, 21640, 21982, 22084, 22468, 22656, 22677, 22691, 23046, 23059, 23080, 23093, 23160, 23172, 23181, 23186, 23236, 23294, 23358, 23376, 23546, 23572, 23586, 23649, 23654, 23669, 23754, 23812, 24088, 24222, 24319, 24330, 24362, 24696, 24745, 24748, 24759, 24775, 24804, 24869, 24971, 25063, 25077, 25146, 25155, 25197, 25285, 25322, 25414, 25421, 25516, 25566, 26251, 26446, 26566, 26600, 26645, 26675, 26678, 26700, 26748, 26754, 26784, 26860], 'term_freq': 259}, 'cymbelin': {'pos': [8996, 10502, 10509, 10672, 10837, 10934, 10949, 11009, 11010, 11057, 11112, 16838, 28700, 28802], 'term_freq': 14}, 'all_well': {'pos': [14019], 'term_freq': 1}, 'm_wives': {'pos': [2765], 'term_freq': 1}, 'hen_vi_2': {'pos': [18308, 21420], 'term_freq': 2}, 'hen_v': {'pos': [23246], 'term_freq': 1}})}\n"
     ]
    }
   ],
   "source": [
    "print(pos_index['caesar'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MOT_CORPUS = \"Moties/XML/MOT\"\n",
    "prepare_data(MOT_CORPUS, MOT_CORPUS + \"New\", 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating the positional index took 39.617246866226196 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "pos_index_moties = positional_index(MOT_CORPUS + \"New\")\n",
    "end = time.time()\n",
    "\n",
    "print(\"\\nCreating the positional index took \" + str(end-start) + \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
