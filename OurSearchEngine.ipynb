{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ElasticSearch\n",
    "\n",
    "In this notebook we have created a search engine using ElasticSearch. \n",
    "\n",
    "For our own reference:\n",
    "* Literature: <https://www.elastic.co/guide/en/elasticsearch/guide/current/index.html>\n",
    "* Telegraaf XML documents: http://data.politicalmashup.nl/arjan/telegraaf/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a search engine for the telegraaf newspaper collection using eg ElasticSearch. Make facets for years and document types. Pay attention to telephone numbers (in mini advertisements). Hieronder een voorbeeld van 1 document (= 1 artikeltje).\n",
    "Je ziet dat er zelfs een link naar de bron tekst (als plaatje) instaat. De URL linked door naar de nieuwe url http://www.delpher.nl/nl/kranten/view?identifier=ddd%3A010563762%3Ampeg21%3Aa0005&coll=ddd ElasticSearch gebruikt een JSON formaat als invoer, en dit is dus triviaal om te zetten naar JSON.\n",
    "\n",
    "Each of the following points must be addressed. Create a seperate page on the wiki for each point. Make sure these pages can be found from the menu of your wiki. Explain what you did, and exemplify with links to screenshots/a working system.\n",
    "\n",
    "* Search as we know it from Google. Give a result page (SERP), with links to the documents and some description of each hit.\n",
    "* Advanced search. Let a user be able to search in several fields, also in several fields simulteanously. Queries like \"return kamervragen by Wilders about XXX with an answer about YYY in the period ZZZ\" should be possible. (For the \"Telegraaf\" collectie, let the user search in both title and tekst fields)\n",
    "* Do one of the following:\n",
    "    1. Represent the hits of a query with a wordcloud of 25-50 informative words. The wordcloud should somehow summarise what the collection has to say about the query. You may think of these words as words that you could add to the query in order to improve recall (blind relevance feedback/query expansion).\n",
    "    2. Represent each document (a kamervraag) with a word-cloud. Also make word-clouds for the question and for the answer. EXAMPLE: The html files in http://data.politicalmashup.nl/arjan/odeii/data_as_html/ contain such wordcloud summaries, which work rather well.   \n",
    "\n",
    "You can use several techniques to get rid of high frequency, but meaningless words: of course IDF, but also mutual information (see 13.5.1), or of course the technique from the paper by Kaptein et al on wordclouds.\n",
    "\n",
    "* Give next to a traditional list of results, a timeline in which you indicate how many hits there are over time.\n",
    "* Give next to the traditional list of results, a table with the number of hits for each political party. Link the party names, which should result in only selecting the hits \"ingediend\" by members of that party. (Faceted Search) (For the \"Telegraaf\" collectie, use the dc:subject element as facet values.)\n",
    "* Evaluate your results Let 2 persons assess the relevancy of the top 10 documents for 5 different queries. Compute Cohen's kappa. Determine the average precision at 10 for your system based on these 10 queries, and the two relevance assesments. Also plot the P@10 (for both judges) for each query, showing differences in hard and easy queries. Describe clearly how you solved differences in judgements. \n",
    "Create your queries in the following format:\n",
    "\n",
    "                    <topic number=\"6\"  >\n",
    "          <query>kcs</query>\n",
    "          <description>Find information on the Kansas City Southern railroad.\n",
    "          </description>\n",
    "           \n",
    "        </topic>\n",
    "\n",
    "        <topic number=\"16\"  >\n",
    "          <query>arizona game and fish</query>\n",
    "          <description>I'm looking for information about fishing and hunting\n",
    "          in Arizona.\n",
    "          </description>\n",
    "           \n",
    "        </topic>\n",
    "                \n",
    "\n",
    "So, both provide the actual query, and a description of the information need that was behind the query.\n",
    "Give a small set of clear guidelines for judging the results, and let your judges follow these guidelines.\n",
    "It is far more interesting to have difficult queries (both for the search engine and for the judges) than to have queries on which all ten retrieved documents are relevant. So, try to create a good list of information needs.\n",
    "\n",
    "* Change the ranking of your system, compute the average precision at 10 using your 10 queries, compare the results to your old system, and EXPLAIN what is going on.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Search Engine\n",
    "\n",
    "Before running ES run: \n",
    "\n",
    "    export ES_HEAP_SIZE=Half_RAM\n",
    "\n",
    "where Half_RAM is half your ram\n",
    "\n",
    "AND: \n",
    "in /config/elasticsaerch.yml add \n",
    "indices.memory.index_buffer_size: 50% \n",
    "(Still need to check if this makes a difference)\n",
    "\n",
    "To start the Elastic searh serive, please run the following code in commandline:\n",
    "\n",
    "    ./elasticsearch-2.4.1/bin/elasticsearch --node.name telegraaf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiate connection to the Elastic Search engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "\n",
    "HOST = 'http://localhost:9200/'\n",
    "es = Elasticsearch(hosts=[HOST],retry_on_timeout=True)\n",
    "\n",
    "# If code runs, the connection is made"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator to read Telegraaf XML and add them to the ES database\n",
    "\n",
    "A generator makes it possible to immediately put the XML files/documents into the ES databse\n",
    "\n",
    "* Remove high frequency, but meaningless words\n",
    "\n",
    "You can use several techniques to get rid of high frequency, but meaningless words: of course IDF, but also mutual information (see 13.5.1), or of course the technique from the paper by Kaptein et al on wordclouds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./Telegraaf/telegraaf-1918.xml', './Telegraaf/telegraaf-1922.xml', './Telegraaf/telegraaf-1923.xml', './Telegraaf/telegraaf-1951.xml', './Telegraaf/telegraaf-1961.xml', './Telegraaf/telegraaf-1962.xml', './Telegraaf/telegraaf-1963.xml', './Telegraaf/telegraaf-1965.xml', './Telegraaf/telegraaf-1966.xml', './Telegraaf/telegraaf-1968.xml', './Telegraaf/telegraaf-1969.xml', './Telegraaf/telegraaf-1970.xml', './Telegraaf/telegraaf-1971.xml', './Telegraaf/telegraaf-1972.xml', './Telegraaf/telegraaf-1973.xml', './Telegraaf/telegraaf-1974.xml', './Telegraaf/telegraaf-1975.xml', './Telegraaf/telegraaf-1976.xml', './Telegraaf/telegraaf-1977.xml', './Telegraaf/telegraaf-1978.xml', './Telegraaf/telegraaf-1979.xml', './Telegraaf/telegraaf-1980.xml', './Telegraaf/telegraaf-1981.xml', './Telegraaf/telegraaf-1982.xml', './Telegraaf/telegraaf-1983.xml', './Telegraaf/telegraaf-1984.xml', './Telegraaf/telegraaf-1985.xml', './Telegraaf/telegraaf-1986.xml', './Telegraaf/telegraaf-1987.xml', './Telegraaf/telegraaf-1988.xml', './Telegraaf/telegraaf-1989.xml', './Telegraaf/telegraaf-1990.xml', './Telegraaf/telegraaf-1991.xml', './Telegraaf/telegraaf-1992.xml', './Telegraaf/telegraaf-1993.xml', './Telegraaf/telegraaf-1994.xml']\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "from os import listdir\n",
    "from os.path import isfile\n",
    "import xml.etree.ElementTree as etree\n",
    "from lxml import etree\n",
    "\n",
    "\n",
    "def read(year):\n",
    "    '''\n",
    "    return a generator for the date, subject(type), \n",
    "    title, and text for each item in the given year. \n",
    "    '''\n",
    "    count = 0\n",
    "    date = ''\n",
    "    subject =''\n",
    "    identifier = ''\n",
    "    text = ''\n",
    "    title = ''\n",
    "    for event, elem in etree.iterparse(year,events=(\"start\", \"end\")):\n",
    "        if event =='end':\n",
    "            if elem.tag == '{http://purl.org/dc/elements/1.1/}date':\n",
    "                date = elem.text\n",
    "                elem.clear()\n",
    "            elif elem.tag == '{http://purl.org/dc/elements/1.1/}subject':\n",
    "                subject = elem.text\n",
    "                elem.clear()\n",
    "            elif elem.tag == '{http://purl.org/dc/elements/1.1/}identifier':\n",
    "                identifier = elem.text\n",
    "                elem.clear()\n",
    "            elif elem.tag == 'title':\n",
    "                if elem.text == None:\n",
    "                    title = ''\n",
    "                else:\n",
    "                    title = elem.text\n",
    "                elem.clear()\n",
    "            elif elem.tag == 'p':\n",
    "                if elem.text == None:\n",
    "                    text = ''\n",
    "                else:\n",
    "                    text = elem.text\n",
    "                elem.clear()\n",
    "            elif elem.tag == '{http://www.politicalmashup.nl}root':\n",
    "                elem.clear()\n",
    "                yield (date,subject,identifier,title,text)\n",
    "            else:\n",
    "                elem.clear()\n",
    "                \n",
    "#     soup = BeautifulSoup(open(year,'r'),'xml')\n",
    "#     for date,subject, title, text, identifier in zip(soup.find_all('date'), soup.find_all('subject'), \n",
    "#                                                      soup.find_all('title'), soup.find_all('text'),\n",
    "#                                                      soup.find_all('identifier')):\n",
    "#             yield (date.text,subject.text,title.text,text.text,identifier.text)\n",
    "\n",
    "documents = ['./Telegraaf/'+i for i in listdir('./Telegraaf') if not isfile(i)]\n",
    "\n",
    "print(documents)\n",
    "\n",
    "# Create the generator for the bulk importer\n",
    "# I'm not sure if it's a good idea to use _type here as a subject (which is artcle or advertisement, or more...)\n",
    "# The score calculation for the Elastic Search database uses whole-index statistics. \n",
    "# If you're searching a subsection this will alter the scores! WE WILL NEED TO KEEP THIS IN MIND.\n",
    "#k = ({'_type':subject, '_index':'telegraaf','_source':{'year':date[:4], 'date':date[5:], 'title':title, 'text':text}} \n",
    "#    for year in documents for (date,subject,title,text) in read(year))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate ES database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "health status index                   pri rep docs.count docs.deleted store.size pri.store.size \r\n",
      "yellow open   .marvel-es-1-2016.10.26   1   1       7007           88      4.4mb          4.4mb \r\n",
      "yellow open   .marvel-es-1-2016.10.25   1   1       6010          100      2.1mb          2.1mb \r\n",
      "yellow open   .marvel-es-1-2016.10.24   1   1      45705           72     16.6mb         16.6mb \r\n",
      "yellow open   .marvel-es-1-2016.10.23   1   1       4619           96      1.9mb          1.9mb \r\n",
      "yellow open   .kibana                   1   1          2            0       19kb           19kb \r\n",
      "yellow open   .marvel-es-data-1         1   1         29            6     26.4kb         26.4kb \r\n",
      "yellow open   telegraaf                 5   1    5655966            0     10.9gb         10.9gb \r\n"
     ]
    }
   ],
   "source": [
    "# List of all indices\n",
    "! curl 'localhost:9200/_cat/indices?v'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'acknowledged': True}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# delete any pre-excisting index\n",
    "#es.indices.delete(index='telegraaf', ignore=[404,400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'acknowledged': True}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the telegraaf index in our telegraaf node\n",
    "es.indices.create(index='telegraaf', ignore=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'acknowledged': True}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# turn refresh to a less frequent rate to speed up bulk import\n",
    "es.indices.put_settings(index='telegraaf',body={\"index\" : \n",
    "                                            {\"refresh_interval\" : \"30s\"\n",
    "                                            }\n",
    "                                       })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting time: 1477420650.39\n",
      "Document: ./Telegraaf/telegraaf-1971.xml\n",
      "[Done] 97.113476038\n",
      "Succesful injections:  174873\n",
      "Failed injections: 0\n",
      "\n",
      "\n",
      "Document: ./Telegraaf/telegraaf-1979.xml\n",
      "[Done] 97.8079650402\n",
      "Succesful injections:  149075\n",
      "Failed injections: 0\n",
      "\n",
      "\n",
      "Document: ./Telegraaf/telegraaf-1974.xml\n",
      "[Done] 93.9970080853\n",
      "Succesful injections:  137155\n",
      "Failed injections: 0\n",
      "\n",
      "\n",
      "Document: ./Telegraaf/telegraaf-1994.xml\n",
      "[Done] 150.787832022\n",
      "Succesful injections:  251526\n",
      "Failed injections: 0\n",
      "\n",
      "\n",
      "Document: ./Telegraaf/telegraaf-1990.xml\n",
      "[Done] 138.377987146\n",
      "Succesful injections:  228202\n",
      "Failed injections: 0\n",
      "\n",
      "\n",
      "Document: ./Telegraaf/telegraaf-1981.xml\n",
      "[Done] 124.811862946\n",
      "Succesful injections:  203119\n",
      "Failed injections: 0\n",
      "\n",
      "\n",
      "Document: ./Telegraaf/telegraaf-1963.xml\n",
      "[Done] 81.1194469929\n",
      "Succesful injections:  120901\n",
      "Failed injections: 0\n",
      "\n",
      "\n",
      "Document: ./Telegraaf/telegraaf-1976.xml\n",
      "[Done] 129.643987179\n",
      "Succesful injections:  179814\n",
      "Failed injections: 0\n",
      "\n",
      "\n",
      "Document: ./Telegraaf/telegraaf-1989.xml\n",
      "[Done] 157.363436937\n",
      "Succesful injections:  230444\n",
      "Failed injections: 0\n",
      "\n",
      "\n",
      "Document: ./Telegraaf/telegraaf-1962.xml\n",
      "[Done] 139.618813992\n",
      "Succesful injections:  180677\n",
      "Failed injections: 0\n",
      "\n",
      "\n",
      "Document: ./Telegraaf/telegraaf-1973.xml\n",
      "[Done] 115.262254\n",
      "Succesful injections:  117418\n",
      "Failed injections: 0\n",
      "\n",
      "\n",
      "Document: ./Telegraaf/telegraaf-1985.xml\n",
      "[Done] 177.812840939\n",
      "Succesful injections:  173750\n",
      "Failed injections: 0\n",
      "\n",
      "\n",
      "Document: ./Telegraaf/telegraaf-1992.xml\n",
      "[Done] 130.971071005\n",
      "Succesful injections:  193719\n",
      "Failed injections: 0\n",
      "\n",
      "\n",
      "Document: ./Telegraaf/telegraaf-1966.xml\n",
      "[Done] 113.816015959\n",
      "Succesful injections:  144359\n",
      "Failed injections: 0\n",
      "\n",
      "\n",
      "Document: ./Telegraaf/telegraaf-1988.xml\n",
      "[Done] 37.0302460194\n",
      "Succesful injections:  62618\n",
      "Failed injections: 0\n",
      "\n",
      "\n",
      "Document: ./Telegraaf/telegraaf-1970.xml\n",
      "[Done] 220.756904125\n",
      "Succesful injections:  238958\n",
      "Failed injections: 0\n",
      "\n",
      "\n",
      "Document: ./Telegraaf/telegraaf-1993.xml\n",
      "[Done] 161.174058914\n",
      "Succesful injections:  265515\n",
      "Failed injections: 0\n",
      "\n",
      "\n",
      "Document: ./Telegraaf/telegraaf-1923.xml\n",
      "[Done] 0.206234931946\n",
      "Succesful injections:  228\n",
      "Failed injections: 0\n",
      "\n",
      "\n",
      "Document: ./Telegraaf/telegraaf-1978.xml\n",
      "[Done] 193.86420083\n",
      "Succesful injections:  211211\n",
      "Failed injections: 0\n",
      "\n",
      "\n",
      "Document: ./Telegraaf/telegraaf-1983.xml\n",
      "[Done] 142.792152882\n",
      "Succesful injections:  135899\n",
      "Failed injections: 0\n",
      "\n",
      "\n",
      "Document: ./Telegraaf/telegraaf-1961.xml\n",
      "[Done] 99.2728590965\n",
      "Succesful injections:  106988\n",
      "Failed injections: 0\n",
      "\n",
      "\n",
      "Document: ./Telegraaf/telegraaf-1965.xml\n",
      "[Done] 49.7844350338\n",
      "Succesful injections:  46140\n",
      "Failed injections: 0\n",
      "\n",
      "\n",
      "Document: ./Telegraaf/telegraaf-1918.xml\n",
      "[Done] 10.4150841236\n",
      "Succesful injections:  6263\n",
      "Failed injections: 0\n",
      "\n",
      "\n",
      "Document: ./Telegraaf/telegraaf-1982.xml\n",
      "[Done] 203.53243804\n",
      "Succesful injections:  170929\n",
      "Failed injections: 0\n",
      "\n",
      "\n",
      "Document: ./Telegraaf/telegraaf-1984.xml\n",
      "[Done] 160.43089509\n",
      "Succesful injections:  174923\n",
      "Failed injections: 0\n",
      "\n",
      "\n",
      "Document: ./Telegraaf/telegraaf-1975.xml\n",
      "[Done] 227.07770896\n",
      "Succesful injections:  202926\n",
      "Failed injections: 0\n",
      "\n",
      "\n",
      "Document: ./Telegraaf/telegraaf-1987.xml\n",
      "[Done] 334.698056936\n",
      "Succesful injections:  271758\n",
      "Failed injections: 0\n",
      "\n",
      "\n",
      "Document: ./Telegraaf/telegraaf-1972.xml\n",
      "[Done] 222.377041101\n",
      "Succesful injections:  203708\n",
      "Failed injections: 0\n",
      "\n",
      "\n",
      "Document: ./Telegraaf/telegraaf-1968.xml\n",
      "[Done] 134.227461815\n",
      "Succesful injections:  154713\n",
      "Failed injections: 0\n",
      "\n",
      "\n",
      "Document: ./Telegraaf/telegraaf-1986.xml\n",
      "[Done] 170.369179964\n",
      "Succesful injections:  157296\n",
      "Failed injections: 0\n",
      "\n",
      "\n",
      "Document: ./Telegraaf/telegraaf-1951.xml\n",
      "[Done] 33.0668420792\n",
      "Succesful injections:  37375\n",
      "Failed injections: 0\n",
      "\n",
      "\n",
      "Document: ./Telegraaf/telegraaf-1980.xml\n",
      "[Done] 148.400204897\n",
      "Succesful injections:  142228\n",
      "Failed injections: 0\n",
      "\n",
      "\n",
      "Document: ./Telegraaf/telegraaf-1969.xml\n",
      "[Done] 196.722019911\n",
      "Succesful injections:  184627\n",
      "Failed injections: 0\n",
      "\n",
      "\n",
      "Document: ./Telegraaf/telegraaf-1922.xml\n",
      "[Done] 6.94085502625\n",
      "Succesful injections:  6692\n",
      "Failed injections: 0\n",
      "\n",
      "\n",
      "Document: ./Telegraaf/telegraaf-1991.xml\n",
      "[Done] 152.802206993\n",
      "Succesful injections:  156259\n",
      "Failed injections: 0\n",
      "\n",
      "\n",
      "Document: ./Telegraaf/telegraaf-1977.xml\n",
      "[Done] 272.449551105\n",
      "Succesful injections:  233680\n",
      "Failed injections: 0\n",
      "\n",
      "\n",
      "Finished 4927.18455219\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "#Import the information into the database\n",
    "# bundeling same type together could speed up bulk process\n",
    "# I\n",
    "def bulk_per_doc(documents):\n",
    "    start = time.time()\n",
    "    print \"Starting time:\", start\n",
    "    for doc in documents:\n",
    "        print \"Document:\", doc\n",
    "        failed = 0\n",
    "        succes = 0\n",
    "        start_doc = time.time()\n",
    "        k = ({'_index':'telegraaf', '_type':subject, '_id':identifier, '_source':{'year':date[:4], \n",
    "               'date':date[5:], 'title':title, 'text':text}} for (date,subject,identifier,title,text) in read(doc))\n",
    "        for (ok, res) in helpers.parallel_bulk(es,k, chunk_size=4000,max_chunk_bytes=15728640, request_timeout=10000):\n",
    "            if not ok:\n",
    "                failed +=1\n",
    "            else:\n",
    "                succes += 1\n",
    "        end_doc = time.time()\n",
    "        print '[Done]', end_doc-start_doc\n",
    "        print \"Succesful injections: \", succes\n",
    "        print \"Failed injections:\", failed\n",
    "        print '\\n'\n",
    "    end =time.time()\n",
    "    print \"Finished\", (end_doc-start)\n",
    "\n",
    "bulk_per_doc(documents)    \n",
    "\n",
    "# It is better to use the per doc populate method, because otherwise the bulk queue will overflow. \n",
    "# def bulk_per_doc(documents):\n",
    "#     start = time.time()\n",
    "#     print \"Starting time:\", start\n",
    "\n",
    "#     for i in documents[:1]:\n",
    "#         start_doc = time.time()\n",
    "#         k = ({'_type':subject, '_id':identifier, '_index':'telegraaf','_source':{'year':date[:4], \n",
    "#              'date':date[5:], 'title':title, 'text':text}}\n",
    "#              for (date,subject,title,text,identifier) in read(i))\n",
    "#         for ok in helpers.parallel_bulk(es,k,chunk_size=500,max_chunk_bytes=15728640):\n",
    "#             continue\n",
    "#         end_doc =time.time()\n",
    "#         print \"Finished\", (end_doc-start_doc)\n",
    "#         end = time.time()\n",
    "#     print \"Done:\", end - start\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'acknowledged': True}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the refresh rate back to default\n",
    "es.indices.put_settings(index='telegraaf',body={\"index\" : \n",
    "                                            {\"refresh_interval\" : \"1s\"\n",
    "                                            }\n",
    "                                       })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Speed improvements/performance: \n",
    "# - bootstrap.mlockall: true in config of the file \n",
    "# (make sure  ES_HEAP_SIZE is large enough) \n",
    "# Parsing whole document xml.cElementTree.parse()\n",
    "# Streaming the xml document: xml.sax.reader.html\n",
    "\n",
    "# import xml.etree.ElementTree as etree\n",
    "# for event, elem in etree.iterparse(xmL, events=('start', 'end', 'start-ns', 'end-ns')):\n",
    "#  print event, elem\n",
    "# http://boscoh.com/programming/reading-xml-serially.html\n",
    "# Event handlers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query system \n",
    "\n",
    "* Normalise query\n",
    "* Get right tokens from the query. Use patterns to split up the query in parts? \n",
    "* Put them in the right representation for ES search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def search(query, advanced=False):\n",
    "    '''\n",
    "    Given a query it returns a SERP with rakings based on the score\n",
    "    '''\n",
    "    if advanced:\n",
    "        must = []\n",
    "        if query[0]:\n",
    "            for i in query[0]:\n",
    "                must.append({\"term\": {\"_type\": i}})\n",
    "    \n",
    "        q = {\"query\": \n",
    "                {\"filtered\": \n",
    "                    {\"query\": {\n",
    "                        \"multi_match\": \n",
    "                            {\"query\" : query[1],\n",
    "                             \"type\" : \"cross_fields\",  # with 'and' operator this is strict\n",
    "                             \"fields\" : query[2],\n",
    "                             \"operator\" : 'and'\n",
    "                            }\n",
    "                        },\n",
    "                     \"filter\": \n",
    "                        [{\"bool\" : \n",
    "                            {\"should\" : must\n",
    "                            }\n",
    "                        },{'range':\n",
    "                            {\"year\":\n",
    "                             {\"gte\": query[3][0], \n",
    "                              \"lte\": query[3][1]}\n",
    "                                \n",
    "                            }}]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        res = es.search(index='telegraaf', size=10, body=q)\n",
    "        return res\n",
    "    else:\n",
    "        # filter_path can help reduce the amount of data that is returned by the es.search\n",
    "        # The query context is for how well the document fits the query\n",
    "        # The filter context is a boolean context. Does it match or not.\n",
    "        # example: Does this timestamp fall into the range 2015 to 2016?\n",
    "        #\n",
    "        \n",
    "        # The outer 'query': is necessary to show that this is the query.  \n",
    "        q = {'query':\n",
    "                {'multi_match':\n",
    "                    {'query' : query,\n",
    "                     'type' : 'cross_fields',  # with 'and' operator \n",
    "                     'fields' : ['title', 'text'],\n",
    "                     'operator' : 'and'\n",
    "                     }\n",
    "                 }\n",
    "             }\n",
    "        # In other words, all terms must be present in at least one field for a document to match.\n",
    "        res = es.search(index='telegraaf', size=10, body=q)\n",
    "        return res\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res = search('stoomschip engelsche')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res['hits']['total']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result page function\n",
    "\n",
    "* Take query output and use score to order result on a Search Engine Result Page (SERP).\n",
    "* Return title, link, and description of each hit\n",
    "\n",
    "-> The description can be a word cloud of 20-25 most informative words. Represent the hits of a query with a wordcloud of 25-50 informative words. The wordcloud should somehow summarise what the collection has to say about the query. You may think of these words as words that you could add to the query in order to improve recall (blind relevance feedback/query expansion). \n",
    "\n",
    "\n",
    "Additions\n",
    "* A timeline with the amount of hits over time\n",
    "* A table with the number of hits for each political party. Link the party names, which should result in only selecting the hits \"ingediend\" by members of that party. (Faceted Search) (For the \"Telegraaf\" collectie, use the dc:subject element as facet values.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Search\n",
    "\n",
    "The query system will have to be changed to implement this\n",
    "\n",
    "* Make multiple fields searchable:\n",
    "    * Title \n",
    "    * Tekst\n",
    "    * Year?\n",
    "    \n",
    "Let a user be able to search in several fields, also in several fields simulteanously. Queries like \"return kamervragen by Wilders about XXX with an answer about YYY in the period ZZZ\" should be possible. (For the \"Telegraaf\" collectie, let the user search in both title and tekst fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# unique_years = list(set( document.date.get_text()[:4]\n",
    "#                     for document in soup_documents ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'1918', u'1922', u'1923', u'1951', u'1961', u'1962', u'1963', u'1965', u'1966', u'1968', u'1969', u'1970', u'1971', u'1972', u'1973', u'1974', u'1975', u'1976', u'1977', u'1978', u'1979', u'1980', u'1981', u'1982', u'1983', u'1984', u'1985', u'1986', u'1987', u'1988', u'1989', u'1990', u'1991', u'1992', u'1993', u'1994']\n"
     ]
    }
   ],
   "source": [
    "# Determine values for the year facets\n",
    "agg={\n",
    "    \"aggs\" : {\n",
    "        \"_source\" : {\n",
    "            \"terms\" : { \"field\" : \"year\", \"size\" : len(documents) }\n",
    "            \n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "agg2={\n",
    "    \"aggs\" : {\n",
    "        \"_type\" : {\n",
    "            \"terms\" : { \"field\" : \"_type\" }\n",
    "            \n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Get field values for the year\n",
    "res = es.search(index='telegraaf', body=agg)\n",
    "unique_years_string = sorted([ \"%s (%d documents)\" % (item['key'], item['doc_count']) \n",
    "                      for item in res['aggregations']['_source']['buckets']])\n",
    "unique_years = sorted([item['key'] for item in res['aggregations']['_source']['buckets']])\n",
    "\n",
    "# Get field values for document type\n",
    "res = es.search(index='telegraaf', body=agg2)\n",
    "unique_doc_types_string = [ \"%s (%d documents)\" % (item['key'], item['doc_count']) \n",
    "                            for item in res['aggregations']['_type']['buckets']]\n",
    "unique_doc_types = [item['key'] for item in res['aggregations']['_type']['buckets']]\n",
    "\n",
    "print unique_years\n",
    "\n",
    "\n",
    "# # Determine values for the document type facets\n",
    "# doc_types = list(es.indices.get_mapping(index='telegraaf')['telegraaf']['mappings'])\n",
    "# unique_doc_types = set(doc_types)\n",
    "\n",
    "# a = es.indices.get_mapping(index='telegraaf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import formlayout \n",
    "\n",
    "def query_database():\n",
    "    query = formlayout.fedit([('Document type',[0]+unique_doc_types_string),\n",
    "                   ('Zoektermen',''),\n",
    "                   ('Jaar publicatie',[0]+unique_years_string)],\n",
    "                   title=\"Telegraaf zoekmachine\", \n",
    "                   comment=\"Wat voor krantenartikel zoek je?\")\n",
    "\n",
    "    query[0] = unique_doc_types[query[0]]\n",
    "\n",
    "    res = search(query, advanced=True)\n",
    "    serp = result_page(query[1], res['hits']['hits'], \"http://kranten.kb.nl/view/article/id/\", 50, 15)\n",
    "    results = formlayout.fedit(serp, title=\"Telegraaf results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def position_sentences(positions, text, m):\n",
    "    \"\"\"\n",
    "    Return a sentence in which multiple m words \n",
    "    from the text occur, based on a list of positions.\n",
    "    \"\"\"\n",
    "    mini = positions[0]\n",
    "    maxi = mini\n",
    "    for i in positions[1:]:\n",
    "        if i > mini and i <= mini + m:\n",
    "            maxi = i\n",
    "    diff = int(math.floor((m - (maxi - mini)) / 2))\n",
    "    return '...'+' '.join(text[mini-diff:maxi+diff])+'...'\n",
    "\n",
    "\n",
    "def extract_description(query, text, m):\n",
    "    \"\"\"\n",
    "    Given a query, select m words from the text that\n",
    "    contain words from the query.\n",
    "    \"\"\"\n",
    "    query = query.split()\n",
    "    stext = text.split(' ')\n",
    "    positions = []\n",
    "    # get the word position \n",
    "    for word in query: \n",
    "        for i,term in enumerate(stext):\n",
    "            if word in term:\n",
    "                positions.append(i)\n",
    "    positions = [i for i in sorted(positions) if i > 7]\n",
    "\n",
    "    # If word(s) appeared in text, return these sentences\n",
    "    if positions:\n",
    "        description = position_sentences(positions, stext, m)\n",
    "    # If the word only occured in title, return first sentence/part of first sentence\n",
    "    else:\n",
    "        description = ''.join(text.split('. ')[0]) + '...'\n",
    "    return description\n",
    "\n",
    "\n",
    "def result_page(query,total_hits, hits):\n",
    "    total = widgets.HTML('Total hits: '+str(total_hits)+\" Shown: 10\")\n",
    "    results = []\n",
    "    descriptions = []\n",
    "    for elem in hits:\n",
    "        if elem['_source']['title'] == '':\n",
    "            results.append(widgets.HTML('<h3><a href=\"http://kranten.kb.nl/view/article/id/'+ str(elem['_id'])+'\" target=\"_blank\">No Title Available</a></h3>'))\n",
    "        else:\n",
    "            results.append(widgets.HTML(value = '<h3><a href=\"http://kranten.kb.nl/view/article/id/'+ str(elem['_id'])+'\" target=\"_blank\">'+elem['_source']['title']+'</a></h3>'))\n",
    "        results.append(widgets.HTML(extract_description(query, elem['_source']['text'],15)))\n",
    "    return results\n",
    "\n",
    "# titles, description = result_page('car', res['hits']['total'], res['hits']['hits'])\n",
    "# for t, d in zip(titles,description):\n",
    "#     display(t)\n",
    "#     display(d)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, clear_output\n",
    "from IPython.core.display import HTML\n",
    "from ipywidgets import widgets, interact\n",
    "# res = search('stoomschip')\n",
    "# print res['hits']['hits'][0]['_source']\n",
    "\n",
    "def get_count(ty, query, fields):\n",
    "    return es.count(index='telegraaf',body={'query':{\"filtered\":{\"query\":\n",
    "                                                                     {\"multi_match\": \n",
    "                                                                        {\"query\" : query,\n",
    "                                                                         \"type\" : \"cross_fields\",  # with 'and' operator this is strict\n",
    "                                                                         \"fields\" : fields,\n",
    "                                                                         \"operator\" : 'and'\n",
    "                                                                        }\n",
    "                                                                    },\n",
    "                                                                    \"filter\":\n",
    "                                                                    {\"bool\":\n",
    "                                                                        {\"must\": {\"term\": {\"_type\": ty}}}\n",
    "                                                                     }\n",
    "                                                                    }\n",
    "                                                        }\n",
    "                                               })['count']\n",
    "\n",
    "\n",
    "\n",
    "SERP = ''\n",
    "\n",
    "# Create de search field and the search button\n",
    "text = widgets.Text(placeholder='Vul een zoekterm in')\n",
    "search_button = widgets.Button(description=\"Search\")\n",
    "advanced = widgets.ToggleButton(description=\"Toggle Advanced Search\", width='220px', button_style='success', value=True)\n",
    "container = widgets.HBox((widgets.HTML('',width='130px'),\n",
    "                          widgets.HTML('Zoektermen:'),\n",
    "                          text, \n",
    "                          widgets.HTML(''), \n",
    "                          search_button,\n",
    "                          widgets.HTML('',width='50px'),\n",
    "                          advanced))\n",
    "container.layout.align_items='center'\n",
    "HBox_layout = widgets.Layout(align_items='center'\n",
    "                            ,align_content='center')\n",
    "\n",
    "#Text field checkbox\n",
    "text_check = widgets.Checkbox(value=True)\n",
    "textfield = widgets.HBox(children=(widgets.HTML('',width='20px'),\n",
    "                                   widgets.HTML('Text'),\n",
    "                                   text_check))\n",
    "textfield.layout.align_items='center'\n",
    "\n",
    "#Title field checkbox\n",
    "title_check = widgets.Checkbox(value=True)\n",
    "titlefield = widgets.HBox(children=(widgets.HTML('',width='20px'),\n",
    "                                    widgets.HTML('Title'),\n",
    "                                    title_check))\n",
    "titlefield.layout.align_items='center'\n",
    "# Container for searchfields\n",
    "c_textfields = widgets.VBox(children=(widgets.HTML('Welke zoekvelden?'),\n",
    "                                      textfield, \n",
    "                                      titlefield))\n",
    "titlefield.layout.align_items='center'\n",
    "\n",
    "\n",
    "# The type fields: \n",
    "class doc_widget:\n",
    "    \n",
    "    def __init__(self,name,value):\n",
    "        self.value = value\n",
    "        self.name = name\n",
    "        self.widget = widgets.Text(str(self.name)+' ('+str(self.value)+')', width='220px', disabled=True)\n",
    "        \n",
    "    def set_value(self, value):\n",
    "        self.value = value\n",
    "        self.widget = widgets.Text(str(self.name)+' ('+str(self.value)+')', width='220px', disabled=True)\n",
    "    \n",
    "doc_types = {}\n",
    "for i in unique_doc_types:\n",
    "    doc_types[i]  = doc_widget(i,0)\n",
    "\n",
    "ad_check = widgets.Checkbox(value=True, width='20px')\n",
    "ar_check = widgets.Checkbox(value=True, width='20px')\n",
    "io_check = widgets.Checkbox(value=True, width='20px')\n",
    "fb_check = widgets.Checkbox(value=True, width='20px')\n",
    "\n",
    "row1 = widgets.HBox(children=(ad_check,\n",
    "                              doc_types['advertentie'].widget, \n",
    "                              ar_check,\n",
    "                              doc_types['artikel'].widget))\n",
    "row1.layout.align_items='center'\n",
    "\n",
    "row2 = widgets.HBox(children=(io_check,\n",
    "                              doc_types['illustratie met onderschrift'].widget,\n",
    "                              fb_check,\n",
    "                              doc_types['familiebericht'].widget))\n",
    "row2.layout.align_items='center'\n",
    "c_types = widgets.VBox(children=(widgets.HTML('Welke type documenten?'),\n",
    "                                 row1,\n",
    "                                 row2))\n",
    "\n",
    "# The time period range:\n",
    "years = widgets.IntRangeSlider(value=[int(unique_years[0]), int(unique_years[-1])], \n",
    "                               min=int(unique_years[0]),\n",
    "                               max= int(unique_years[-1]), \n",
    "                               step=1)\n",
    "\n",
    "c_slide = widgets.VBox(children=(widgets.HTML(\"Kies tijdsperiode:\"), \n",
    "                                 years))\n",
    "\n",
    "\n",
    "# Advanced settings container\n",
    "container_adv = widgets.HBox((c_textfields,   \n",
    "                              c_types,     # misschien een extra beetje ruimte tussen plaatsen   \n",
    "                              c_slide))    # Slider to select the number of years\n",
    "container_adv.visible=True\n",
    "\n",
    "\n",
    "def advanced_options(sender):\n",
    "    if sender['new'] == True:\n",
    "        container_adv.visible = True\n",
    "        # Show the advanced settings\n",
    "        \n",
    "    elif sender['new'] == False:\n",
    "        container_adv.visible = False\n",
    "        # Hide the advanced settings\n",
    "\n",
    "        \n",
    "    \n",
    "        \n",
    "def handle_submit(sender):\n",
    "    '''\n",
    "    This function handles the search after the button has been presed or enter in the text field\n",
    "    '''\n",
    "    global SERP \n",
    "    if SERP != '':\n",
    "        SERP.close()\n",
    "        \n",
    "    # The simple search function were it just searches for the keywords\n",
    "    if container_adv.visible == False:\n",
    "        display('Zoeken..')\n",
    "        res = search(text.value)\n",
    "        clear_output()\n",
    "        results = result_page(text.value, res['hits']['total'], res['hits']['hits'])\n",
    "        if results == []:\n",
    "            SERP = widgets.VBox(children=(widgets.HTML('', height='20px'),\n",
    "                                          widgets.HTML(\"<center>Geen zoekresultaten zijn gevonden. Probeer het opnieuw</center>\")))\n",
    "        else:\n",
    "            SERP = widgets.VBox(children=(tuple([i for i in results])))\n",
    "        display(SERP)\n",
    "        \n",
    "    # The advanced search\n",
    "    else:\n",
    "        display('Zoeken..')\n",
    "        \n",
    "        # Get which types are checked for the search\n",
    "        types = []\n",
    "        if ad_check.value:\n",
    "            types.append('advertentie')\n",
    "        if ar_check.value:\n",
    "            types.append('artikel')\n",
    "        if io_check.value:\n",
    "            types.append('illustratie met onderschrift')\n",
    "        if fb_check.value:\n",
    "            types.append('familiebericht')\n",
    "        \n",
    "        fields = []\n",
    "        # Which text fields? \n",
    "        if title_check:\n",
    "            fields.append('title')\n",
    "        if text_check:\n",
    "            fields.append('text')\n",
    "        \n",
    "        # Get values from slider\n",
    "        \n",
    "        \n",
    "        # Search \n",
    "        res = search([types,text.value,fields,years.value], advanced=True)\n",
    "        clear_output()\n",
    "        results = result_page(text.value, res['hits']['total'], res['hits']['hits'])\n",
    "        \n",
    "        # Print the outputs\n",
    "        if results == []:\n",
    "            SERP = widgets.VBox(children=(widgets.HTML('', height='20px'),\n",
    "                                          widgets.HTML(\"<center>Geen zoekresultaten zijn gevonden. Probeer het opnieuw</center>\")))\n",
    "        else:\n",
    "            SERP = widgets.VBox(children=(tuple([i for i in results])))\n",
    "        display(SERP)\n",
    "        \n",
    "        # update the type doc numbers\n",
    "        for i in doc_types:\n",
    "            doc_types[i].widget.value= doc_types[i].name+' ('+str(get_count(i,text.value,fields))+')'\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "    \n",
    "    \n",
    "display(container)\n",
    "display(container_adv)\n",
    "text.on_submit(handle_submit)\n",
    "search_button.on_click(handle_submit)\n",
    "advanced.observe(advanced_options,names='value')\n",
    "# advanced.on_click(handle_submit)\n",
    "# help(advanced)\n",
    "# Submit the search \n",
    "\n",
    "\n",
    "\n",
    "# Show the advanced search options\n",
    "# def advanced_options():\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result page function\n",
    "\n",
    "* Take query output and use score to order result on a Search Engine Result Page (SERP).\n",
    "* Return title, link, and description of each hit\n",
    "\n",
    "### SERP & Worcloud\n",
    "Since stopwords have high frequencies, they are likely to occupy most places in the word cloud. We therefore remove an extensive stopword list consisting of 571 common English words. Only single words (unigrams) are included in the cloud and stemming is applied. To create a word cloud all terms in the document are sorted by their probabilities and a fixed number of the 25 top ranked terms are kept. The top 10 documents retrieved by a language model run are concatenated and treated as one long document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named stop_words",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-c250d37b77e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mstop_words\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_stop_words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msnowballstemmer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mwordcloud\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named stop_words"
     ]
    }
   ],
   "source": [
    "from stop_words import get_stop_words\n",
    "import snowballstemmer\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "def result_page(query, hits, url, n, m):\n",
    "    \"\"\"\n",
    "    Return the SERP information from a given list of\n",
    "    search results.\n",
    "    \"\"\"\n",
    "    if hits:\n",
    "        serp = []\n",
    "        total_text = []\n",
    "        years = []\n",
    "        dates = []\n",
    "        for i, hit in enumerate(hits[:10]):\n",
    "            year = hit['_source']['year']\n",
    "            years.append(year)\n",
    "            dates.append(hit['_source']['date'])\n",
    "            identifier = url+str(hit['_id'])\n",
    "            serp.append((None,None))\n",
    "            serp.append((None,\"Jaar publicatie: %s, Onderwerp: %s\" % (year,hit['_type'])))\n",
    "            serp.append((None, \"URL: <a href='%s'>%s</a>\" % (identifier,identifier)))\n",
    "            serp.append((None,hit['_source']['title']))\n",
    "            text = hit['_source']['text'].split()\n",
    "            total_text = total_text + text\n",
    "            serp.append((None,\"Beschrijving: \" + extract_description(query,text,m)))\n",
    "        wordcloud = create_wordcloud(text, n)\n",
    "        timeline = create_timeline(years,dates)\n",
    "    else:\n",
    "        serp = [(None,'\\n\\n\\n\\nEr zijn helaas geen zoekresultaten.\\n\\n\\n\\n')]\n",
    "    return serp\n",
    "\n",
    "def create_wordcloud(text, n):\n",
    "    \"\"\"\n",
    "    Display a wordcloud with at most n words, generated\n",
    "    from the given text.\n",
    "    \"\"\"\n",
    "    # Filter words to use for the wordcloud, by stemming and stop words removal\n",
    "    stop_words = get_stop_words(\"dutch\")\n",
    "    stemmer = snowballstemmer.stemmer(\"dutch\")\n",
    "    text = [word for word in text if word.lower() not in stop_words]\n",
    "    text = stemmer.stemWords(text)\n",
    "\n",
    "    # Plot wordcloud\n",
    "    wordcloud = WordCloud(max_font_size=40, background_color=\"white\",\n",
    "                          max_words = n).generate(\" \".join(text))\n",
    "    plt.figure()\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "def extract_description(query, text, m):\n",
    "    \"\"\"\n",
    "    Given a query, select m words from the text that\n",
    "    contain words from the query.\n",
    "    \"\"\"\n",
    "    query = query.split()\n",
    "    positions = sorted([text.index(word) for word in query \n",
    "                        if word in text ])\n",
    "    \n",
    "    positions = [i for i in positions if i > 7]\n",
    "\n",
    "    # If word(s) appeared in text, return these sentences\n",
    "    if positions:\n",
    "        description = position_sentences(positions, text, m)\n",
    "    # If the word only occured in title, return first sentence\n",
    "    else:\n",
    "        description = ' '.join(text[0:m]) + '...'\n",
    "    \n",
    "    return description\n",
    "\n",
    "def position_sentences(positions, text, m):\n",
    "    \"\"\"\n",
    "    Return a sentence in which multiple m words \n",
    "    from the text occur, based on a list of positions.\n",
    "    \"\"\"\n",
    "    mini = positions[0]\n",
    "    maxi = mini\n",
    "    for i in positions[1:]:\n",
    "        if i > mini and i <= mini + m:\n",
    "            maxi = i\n",
    "    diff = int(math.floor((m - (maxi - mini)) / 2))\n",
    "    return '...'+' '.join(text[mini-diff:maxi+diff])+'...'\n",
    "\n",
    "def create_timeline(years, dates):\n",
    "    \"\"\"\n",
    "    Display the years or dates from the hits\n",
    "    on a timeline.\n",
    "    \"\"\"\n",
    "\n",
    "    ax = plt.figure(figsize=(15,4)).gca()\n",
    "    ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    \n",
    "    if len(set(years)) == 1:\n",
    "        xlabel = \"Months of the year\" + years[0]\n",
    "        bins = [ int(date[:2]) for date in dates ]\n",
    "        #counted = Counter(months)\n",
    "        names = ['           Januari','             Februari','         Maart',\n",
    "                 '        April', '        Mei','        Juni','        Juli',\n",
    "                 '               Augustus','                 September',\n",
    "                 '               Oktober','                November',\n",
    "                 '                December']\n",
    "        xlim = [1,13]\n",
    "        plt.xticks(range(1,13),names)\n",
    "        bin_range = range(1,13)\n",
    "    else: \n",
    "        bins = years \n",
    "        bin_range = range(min(years),max(years)+1)\n",
    "        xlabel = \"Years\"\n",
    "        xlim = [min(years),max(years)]\n",
    "\n",
    "    ax.set_xlim(xlim)\n",
    "    \n",
    "    # Mooi roze is niet leelijk\n",
    "    plt.hist(bins, bins=bin_range, color='Crimson')\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(\"Number of documents\")\n",
    "    plt.show()\n",
    "    \n",
    "#create_timeline([1922,1992],['11-12','10-31','11-6'])        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "* Manual relevance check\n",
    "* P@10\n",
    "* Change the ranking of the system + explain what is going on and why it is improving/decreasing\n",
    "\n",
    "Evaluate your results Let 2 persons assess the relevancy of the top 10 documents for 5 different queries. Compute Cohen's kappa. Determine the average precision at 10 for your system based on these 10 queries, and the two relevance assesments. Also plot the P@10 (for both judges) for each query, showing differences in hard and easy queries. Describe clearly how you solved differences in judgements. \n",
    "So, both provide the actual query, and a description of the information need that was behind the query.\n",
    "Give a small set of clear guidelines for judging the results, and let your judges follow these guidelines.\n",
    "It is far more interesting to have difficult queries (both for the search engine and for the judges) than to have queries on which all ten retrieved documents are relevant. So, try to create a good list of information needs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'familiebericht', u'overlijden', u'1981']\n"
     ]
    }
   ],
   "source": [
    "query_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "def cohens_kappa(bins):\n",
    "    '''\n",
    "    Given bins made by bin_evaluations, returns cohen\\'s kappa\n",
    "    '''\n",
    "    \n",
    "    total = bins[0]+bins[1]+bins[2]+bins[3]\n",
    "    po = (bins[0]+bins[3]) / total\n",
    "    \n",
    "    marginala = ((bins[3] + bins[2]) * (bins[3] + bins[1])) / total\n",
    "    marginalb = ((bins[0] + bins[1]) * (bins[0] + bins[2])) / total\n",
    "    pe = (marginala + marginalb) / total\n",
    "    \n",
    "    cohens_kappa = (po - pe) / (1 - pe)\n",
    "    return cohens_kappa\n",
    "    \n",
    "def precision(bins, agreement_necessary):\n",
    "    '''\n",
    "    Given bins made by bin_evaluations, returns precision\n",
    "    If second argument is True, document is only seen as\n",
    "    relevant if both judges think so; otherwise one of the \n",
    "    judges is enough\n",
    "    '''\n",
    "    if agreement_necessary:\n",
    "        correct = bins[3]\n",
    "    else:\n",
    "        correct = bins[3] + bins[2] + bins[1]\n",
    "    total = bins[0]+bins[1]+bins[2]+bins[3]\n",
    "    return correct/total\n",
    "    \n",
    "    \n",
    "def bin_evaluations(evaluations):\n",
    "    '''\n",
    "    Given a list of tuples of binary values which contain\n",
    "    relevance judgements by two judges in which 1 means \n",
    "    relevant, and 0 means non-relevant, returns a list\n",
    "    in which the 4 elements represent the amount of times\n",
    "    any combination has occurred.\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    # bins are 00, 01, 10, 11 in that order\n",
    "    bins = [0,0,0,0]\n",
    "    for evaluation in evaluations:\n",
    "        b = 0\n",
    "        if evaluation[0]:\n",
    "            b += 2\n",
    "        if evaluation[1]:\n",
    "            b += 1\n",
    "        bins[b] += 1\n",
    "    return bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_search_results(number_of_queries):\n",
    "    '''\n",
    "    Asks the user to submit a query using query_database() some amount \n",
    "    of times after which two judges can give their relevancy \n",
    "    assessments. Returns evaluations in a format that can be used for \n",
    "    bin_evaluations.\n",
    "    '''\n",
    "\n",
    "    print('After the query is resolved, you will be asked to rate the documents on relevancy.')\n",
    "    print('To do this, enter the numbers of the relevant results sperated by whitespace.')\n",
    "    print('For example, if only the first and third documents were relevant, enter \\'1 3\\' without the quotes.')\n",
    "    print('If more than ten documents are returned, ignore all but the first 10.\\n')\n",
    "    evaluations = []\n",
    "    for _ in range(number_of_queries):\n",
    "        query_database()\n",
    "        print('Indicate which documents were relevant:')\n",
    "        judge1_input = raw_input('Judge 1 -->').split()\n",
    "        judge2_input = raw_input('Judge 2 -->').split()\n",
    "        for i in range(1, 11):\n",
    "            if str(i) in judge1_input:\n",
    "                assessment1 = 1\n",
    "            else:\n",
    "                assessment1 = 0\n",
    "            if str(i) in judge2_input:\n",
    "                assessment2 = 1\n",
    "            else:\n",
    "                assessment2 = 0\n",
    "            evaluations.append((assessment1, assessment2))\n",
    "    \n",
    "    bins = bin_evaluations(evaluations)\n",
    "    print('\\n The average P@10 if we require agreement for correctness: ' + str(precision(bins, 1)))\n",
    "    print(\"The average P@10 if we require a single \\'relevant\\' assessment for correctness: \" + str(precision(bins, 0)))\n",
    "    print(\"Cohen\\'s Kappa was: \" +str(cohens_kappa(bins)))\n",
    "    return evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After the query is resolved, you will be asked to rate the documents on relevancy.\n",
      "To do this, enter the numbers of the relevant results sperated by whitespace.\n",
      "For example, if only the first and third documents were relevant, enter '1 3' without the quotes.\n",
      "If more than ten documents are returned, ignore all but the first 10.\n",
      "\n",
      "Indicate which documents were relevant:\n",
      "Judge 1 -->1\n",
      "Judge 2 -->1\n",
      "Indicate which documents were relevant:\n",
      "Judge 1 -->2\n",
      "Judge 2 -->2\n",
      "\n",
      " The average P@10 if we require agreement for correctness: 0.1\n",
      "The average P@10 if we require a single 'relevant' assessment for correctness: 0.1\n",
      "Cohen's Kappa was: 1.0\n"
     ]
    }
   ],
   "source": [
    "evaluations = evaluate_search_results(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "widgets": {
   "state": {
    "3de4b0e08c3945b3a2ab99e0a9b06734": {
     "views": [
      {
       "cell_index": 29
      }
     ]
    },
    "4b4a682048b74becbe8b5f1eea77c880": {
     "views": [
      {
       "cell_index": 29
      }
     ]
    },
    "704ecd4a142e444f8290b30609b00c30": {
     "views": [
      {
       "cell_index": 29
      }
     ]
    },
    "73ccb7d487d44c758c4e88e3fee217ea": {
     "views": [
      {
       "cell_index": 29
      }
     ]
    },
    "8504188a417f480dbe769bd1a1e1ba61": {
     "views": [
      {
       "cell_index": 29
      }
     ]
    },
    "9e0c07ef58754a4eafdad5f7ba4df78a": {
     "views": [
      {
       "cell_index": 29
      }
     ]
    },
    "d2db8b9ec30d47ceb8fa55f68d1ea1b3": {
     "views": [
      {
       "cell_index": 29
      }
     ]
    },
    "f3e7f07a192e4f9b88e795f65f27a7ee": {
     "views": [
      {
       "cell_index": 29
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
