{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ElasticSearch\n",
    "\n",
    "In this notebook we have created a search engine using ElasticSearch. \n",
    "\n",
    "For our own reference:\n",
    "* Literature: <https://www.elastic.co/guide/en/elasticsearch/guide/current/index.html>\n",
    "* Telegraaf XML documents: http://data.politicalmashup.nl/arjan/telegraaf/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a search engine for the telegraaf newspaper collection using eg ElasticSearch. Make facets for years and document types. Pay attention to telephone numbers (in mini advertisements). Hieronder een voorbeeld van 1 document (= 1 artikeltje).\n",
    "Je ziet dat er zelfs een link naar de bron tekst (als plaatje) instaat. De URL linked door naar de nieuwe url http://www.delpher.nl/nl/kranten/view?identifier=ddd%3A010563762%3Ampeg21%3Aa0005&coll=ddd ElasticSearch gebruikt een JSON formaat als invoer, en dit is dus triviaal om te zetten naar JSON.\n",
    "\n",
    "Each of the following points must be addressed. Create a seperate page on the wiki for each point. Make sure these pages can be found from the menu of your wiki. Explain what you did, and exemplify with links to screenshots/a working system.\n",
    "\n",
    "* Search as we know it from Google. Give a result page (SERP), with links to the documents and some description of each hit.\n",
    "* Advanced search. Let a user be able to search in several fields, also in several fields simulteanously. Queries like \"return kamervragen by Wilders about XXX with an answer about YYY in the period ZZZ\" should be possible. (For the \"Telegraaf\" collectie, let the user search in both title and tekst fields)\n",
    "* Do one of the following:\n",
    "    1. Represent the hits of a query with a wordcloud of 25-50 informative words. The wordcloud should somehow summarise what the collection has to say about the query. You may think of these words as words that you could add to the query in order to improve recall (blind relevance feedback/query expansion).\n",
    "    2. Represent each document (a kamervraag) with a word-cloud. Also make word-clouds for the question and for the answer. EXAMPLE: The html files in http://data.politicalmashup.nl/arjan/odeii/data_as_html/ contain such wordcloud summaries, which work rather well.   \n",
    "\n",
    "You can use several techniques to get rid of high frequency, but meaningless words: of course IDF, but also mutual information (see 13.5.1), or of course the technique from the paper by Kaptein et al on wordclouds.\n",
    "\n",
    "* Give next to a traditional list of results, a timeline in which you indicate how many hits there are over time.\n",
    "* Give next to the traditional list of results, a table with the number of hits for each political party. Link the party names, which should result in only selecting the hits \"ingediend\" by members of that party. (Faceted Search) (For the \"Telegraaf\" collectie, use the dc:subject element as facet values.)\n",
    "* Evaluate your results Let 2 persons assess the relevancy of the top 10 documents for 5 different queries. Compute Cohen's kappa. Determine the average precision at 10 for your system based on these 10 queries, and the two relevance assesments. Also plot the P@10 (for both judges) for each query, showing differences in hard and easy queries. Describe clearly how you solved differences in judgements. \n",
    "Create your queries in the following format:\n",
    "\n",
    "                    <topic number=\"6\"  >\n",
    "          <query>kcs</query>\n",
    "          <description>Find information on the Kansas City Southern railroad.\n",
    "          </description>\n",
    "           \n",
    "        </topic>\n",
    "\n",
    "        <topic number=\"16\"  >\n",
    "          <query>arizona game and fish</query>\n",
    "          <description>I'm looking for information about fishing and hunting\n",
    "          in Arizona.\n",
    "          </description>\n",
    "           \n",
    "        </topic>\n",
    "                \n",
    "\n",
    "So, both provide the actual query, and a description of the information need that was behind the query.\n",
    "Give a small set of clear guidelines for judging the results, and let your judges follow these guidelines.\n",
    "It is far more interesting to have difficult queries (both for the search engine and for the judges) than to have queries on which all ten retrieved documents are relevant. So, try to create a good list of information needs.\n",
    "\n",
    "* Change the ranking of your system, compute the average precision at 10 using your 10 queries, compare the results to your old system, and EXPLAIN what is going on.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Search Engine\n",
    "\n",
    "Before running ES run: \n",
    "\n",
    "    export ES_HEAP_SIZE=Half_RAM\n",
    "\n",
    "where Half_RAM is half your ram\n",
    "\n",
    "AND: \n",
    "in /config/elasticsaerch.yml add \n",
    "indices.memory.index_buffer_size: 50% \n",
    "(Still need to check if this makes a difference)\n",
    "\n",
    "To start the Elastic searh serive, please run the following code in commandline:\n",
    "\n",
    "    ./elasticsearch-2.4.1/bin/elasticsearch --node.name telegraaf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiate connection to the Elastic Search engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "\n",
    "HOST = 'http://localhost:9200/'\n",
    "es = Elasticsearch(hosts=[HOST])\n",
    "\n",
    "# If code runs, the connection is made"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator to read Telegraaf XML and add them to the ES database\n",
    "\n",
    "A generator makes it possible to immediately put the XML files/documents into the ES databse\n",
    "\n",
    "* Remove high frequency, but meaningless words\n",
    "\n",
    "You can use several techniques to get rid of high frequency, but meaningless words: of course IDF, but also mutual information (see 13.5.1), or of course the technique from the paper by Kaptein et al on wordclouds.\n",
    "\n",
    "    Possibly also create an inverted index at this point? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./Telegraaf/telegraaf-1918.xml', './Telegraaf/telegraaf-1922.xml']\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "from os import listdir\n",
    "from os.path import isfile\n",
    "\n",
    "def read(year):\n",
    "    '''\n",
    "    return a generator for the date, subject(type), \n",
    "    title, and text for each item in the given year. \n",
    "    '''\n",
    "    soup = BeautifulSoup(open(year,'r'),'xml')\n",
    "    for date,subject, title, text, identifier in zip(soup.find_all('date'), soup.find_all('subject'), \n",
    "                                                     soup.find_all('title'), soup.find_all('text'),\n",
    "                                                     soup.find_all('identifier')):\n",
    "            yield (date.text,subject.text,title.text,text.text,identifier.text)\n",
    "\n",
    "documents = ['./Telegraaf/'+i for i in listdir('./Telegraaf') if not isfile(i)]\n",
    "\n",
    "print(documents)\n",
    "\n",
    "# Create the generator for the bulk importer\n",
    "# I'm not sure if it's a good idea to use _type here as a subject (which is artcle or advertisement, or more...)\n",
    "# The score calculation for the Elastic Search database uses whole-index statistics. \n",
    "# If you're searching a subsection this will alter the scores! WE WILL NEED TO KEEP THIS IN MIND.\n",
    "#k = ({'_type':subject, '_index':'telegraaf','_source':{'year':date[:4], 'date':date[5:], 'title':title, 'text':text}} \n",
    "#    for year in documents for (date,subject,title,text) in read(year))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate ES database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "health status index     pri rep docs.count docs.deleted store.size pri.store.size \r\n",
      "yellow open   telegraaf   5   1     157546            0      333mb          333mb \r\n",
      "yellow open   megacorp    5   1          0            0       800b           800b \r\n"
     ]
    }
   ],
   "source": [
    "# List of all indices\n",
    "! curl 'localhost:9200/_cat/indices?v'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'acknowledged': True}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# delete any pre-excisting index\n",
    "es.indices.delete(index='telegraaf', ignore=[404,400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'acknowledged': True}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the telegraaf index in our telegraaf node\n",
    "es.indices.create(index='telegraaf', ignore=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'acknowledged': True}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# turn refresh off to speed up bulk import\n",
    "es.indices.put_settings(index='telegraaf',body={\"index\" : \n",
    "                                            {\"refresh_interval\" : \"-1\"\n",
    "                                            }\n",
    "                                       })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test with chunk size = 500 and max_chunk_bytes = 15728640 \n",
      "Starting time: 1477241574.61\n",
      "Finished 17.2869999409\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "#Import the information into the database\n",
    "# The generator can only be used once. So this code will only work once. \n",
    "print \"Test with chunk size = 500 and max_chunk_bytes = 15728640 \"\n",
    "# helper.parallel_bulk might increase the speed even more!!!\n",
    "\n",
    "def bulk_all(documents):\n",
    "    start = time.time()\n",
    "    print \"Starting time:\", start\n",
    "\n",
    "    k = ({'_type':subject, '_id':identifier, '_index':'telegraaf','_source':{'year':date[:4], \n",
    "         'date':date[5:], 'title':title, 'text':text}}\n",
    "          for doc in documents[:5] for (date,subject,title,text,identifier) in read(doc))\n",
    "    for ok in helpers.parallel_bulk(es,k, chunk_size=500,max_chunk_bytes=15728640):\n",
    "        continue\n",
    "    end_doc =time.time()\n",
    "    print \"Finished\", (end_doc-start)\n",
    "\n",
    "def bulk_per_doc(documents):\n",
    "    start = time.time()\n",
    "    print \"Starting time:\", start\n",
    "\n",
    "    for i in documents[:1]:\n",
    "        start_doc = time.time()\n",
    "        k = ({'_type':subject, '_id':identifier, '_index':'telegraaf','_source':{'year':date[:4], \n",
    "             'date':date[5:], 'title':title, 'text':text}}\n",
    "             for (date,subject,title,text,identifier) in read(i))\n",
    "        for ok in helpers.parallel_bulk(es,k,chunk_size=500,max_chunk_bytes=15728640):\n",
    "            continue\n",
    "        end_doc =time.time()\n",
    "        print \"Finished\", (end_doc-start_doc)\n",
    "        end = time.time()\n",
    "    print \"Done:\", end - start\n",
    "        \n",
    "bulk_all(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'acknowledged': True}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the refresh rate back to default\n",
    "es.indices.put_settings(index='telegraaf',body={\"index\" : \n",
    "                                            {\"refresh_interval\" : \"1s\"\n",
    "                                            }\n",
    "                                       })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Speed improvements/performance: \n",
    "# - bootstrap.mlockall: true in config of the file \n",
    "# (make sure  ES_HEAP_SIZE is large enough) \n",
    "# Parsing whole document xml.cElementTree.parse()\n",
    "# Streaming the xml document: xml.sax.reader.html\n",
    "\n",
    "# import xml.etree.ElementTree as etree\n",
    "# for event, elem in etree.iterparse(xmL, events=('start', 'end', 'start-ns', 'end-ns')):\n",
    "#  print event, elem\n",
    "# http://boscoh.com/programming/reading-xml-serially.html\n",
    "# Event handlers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query system \n",
    "\n",
    "* Normalise query\n",
    "* Get right tokens from the query. Use patterns to split up the query in parts? \n",
    "* Put them in the right representation for ES search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def search(query, advanced=False):\n",
    "    '''\n",
    "    Given a query it returns a SERP with rakings based on the score\n",
    "    '''\n",
    "    if advanced:\n",
    "        must = []\n",
    "        if query[0]:\n",
    "            must.append({\"term\": {\"_type\": query[0]}})\n",
    "        if query[2]:\n",
    "            must.append({\"term\": {\"year\": query[2]}})\n",
    "        \n",
    "        q = {\"query\": \n",
    "                {\"filtered\": \n",
    "                    {\"query\": {\n",
    "                        \"multi_match\": \n",
    "                            {\"query\" : query[1],\n",
    "                             \"type\" : \"cross_fields\",  # with 'and' operator \n",
    "                             \"fields\" : ['title', 'text'],\n",
    "                             \"operator\" : 'and'\n",
    "                            }\n",
    "                        },\n",
    "                     \"filter\": \n",
    "                        {\"bool\" : \n",
    "                            {\"must\" : must\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        res = es.search(index='telegraaf', size=10, body=q)\n",
    "        return res\n",
    "    else:\n",
    "        # filter_path can help reduce the amount of data that is returned by the es.search\n",
    "        # The query context is for how well the document fits the query\n",
    "        # The filter context is a boolean context. Does it match or not.\n",
    "        # example: Does this timestamp fall into the range 2015 to 2016?\n",
    "        #\n",
    "        \n",
    "        # The outer 'query': is necessary to show that this is the query.  \n",
    "        q = {'query':\n",
    "                {'multi_match':\n",
    "                    {'query' : query,\n",
    "                     'type' : 'cross_fields',  # with 'and' operator \n",
    "                     'fields' : ['title', 'text'],\n",
    "                     'operator' : 'and'\n",
    "                     }\n",
    "                 }\n",
    "             }\n",
    "        # In other words, all terms must be present in at least one field for a document to match.\n",
    "        res = es.search(index='telegraaf', size=10, body=q)\n",
    "        return res\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#search('stoomschip engelsche')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result page function\n",
    "\n",
    "* Take query output and use score to order result on a Search Engine Result Page (SERP).\n",
    "* Return title, link, and description of each hit\n",
    "\n",
    "-> The description can be a word cloud of 20-25 most informative words. Represent the hits of a query with a wordcloud of 25-50 informative words. The wordcloud should somehow summarise what the collection has to say about the query. You may think of these words as words that you could add to the query in order to improve recall (blind relevance feedback/query expansion). \n",
    "\n",
    "\n",
    "Additions\n",
    "* A timeline with the amount of hits over time\n",
    "* A table with the number of hits for each political party. Link the party names, which should result in only selecting the hits \"ingediend\" by members of that party. (Faceted Search) (For the \"Telegraaf\" collectie, use the dc:subject element as facet values.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Search\n",
    "\n",
    "The query system will have to be changed to implement this\n",
    "\n",
    "* Make multiple fields searchable:\n",
    "    * Title \n",
    "    * Tekst\n",
    "    * Year?\n",
    "    \n",
    "Let a user be able to search in several fields, also in several fields simulteanously. Queries like \"return kamervragen by Wilders about XXX with an answer about YYY in the period ZZZ\" should be possible. (For the \"Telegraaf\" collectie, let the user search in both title and tekst fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Determine values for the document type facets\n",
    "unique_doc_types = list(set(es.indices.get_mapping(index='telegraaf')['telegraaf']['mappings']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import formlayout \n",
    "\n",
    "def query_database():\n",
    "    query = formlayout.fedit([('Document type',[0]+unique_doc_types),\n",
    "                   ('Zoektermen',''),\n",
    "                   ('Jaar publicatie','')],\n",
    "                   title=\"Telegraaf zoekmachine\", \n",
    "                   comment=\"Wat voor krantenartikel zoek je?\")\n",
    "\n",
    "    query[0] = unique_doc_types[query[0]]\n",
    "\n",
    "    res = search(query, advanced=True)\n",
    "    serp = result_page(query[1], res['hits']['hits'], \"http://kranten.kb.nl/view/article/id/\", 50, 15)\n",
    "    results = formlayout.fedit(serp, title=\"Telegraaf results\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result page function\n",
    "\n",
    "* Take query output and use score to order result on a Search Engine Result Page (SERP).\n",
    "* Return title, link, and description of each hit\n",
    "\n",
    "### SERP & Worcloud\n",
    "Since stopwords have high frequencies, they are likely to occupy most places in the word cloud. We therefore remove an extensive stopword list consisting of 571 common English words. Only single words (unigrams) are included in the cloud and stemming is applied. To create a word cloud all terms in the document are sorted by their probabilities and a fixed number of the 25 top ranked terms are kept. The top 10 documents retrieved by a language model run are concatenated and treated as one long document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from stop_words import get_stop_words\n",
    "import snowballstemmer\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import math\n",
    "\n",
    "def result_page(query, hits, url, n, m):\n",
    "    \"\"\"\n",
    "    Return the SERP information from a given list of\n",
    "    search results.\n",
    "    \"\"\"\n",
    "    if hits:\n",
    "        serp = []\n",
    "        total_text = []\n",
    "        for i, hit in enumerate(hits[:10]):\n",
    "            serp.append((None,None))\n",
    "            serp.append((None,\"Jaar publicatie: %s, Onderwerp: %s\" % (hit['_source']['year'],hit['_type'])))\n",
    "            serp.append((None, \"URL: <a href='%s'>%s</a>\" % (url+str(hit['_id']),url+str(hit['_id']))))\n",
    "            serp.append((None,hit['_source']['title']))\n",
    "            text = hit['_source']['text'].split()\n",
    "            total_text = total_text + text\n",
    "            serp.append((None,\"Beschrijving: \" + extract_description(query,text,m)))\n",
    "        wordcloud = create_wordcloud(text, n)\n",
    "    else:\n",
    "        serp = [(None,'\\n\\n\\n\\nEr zijn helaas geen zoekresultaten.\\n\\n\\n\\n')]\n",
    "    return serp\n",
    "\n",
    "def create_wordcloud(text, n):\n",
    "    \"\"\"\n",
    "    Display a wordcloud with at most n words, generated\n",
    "    from the given text.\n",
    "    \"\"\"\n",
    "    # Filter words to use for the wordcloud, by stemming and stop words removal\n",
    "    stop_words = get_stop_words(\"dutch\")\n",
    "    stemmer = snowballstemmer.stemmer(\"dutch\")\n",
    "    text = [word for word in text if word.lower() not in stop_words]\n",
    "    text = stemmer.stemWords(text)\n",
    "\n",
    "    # Plot wordcloud\n",
    "    wordcloud = WordCloud(max_font_size=40, background_color=\"white\",\n",
    "                          max_words = n).generate(\" \".join(text))\n",
    "    plt.figure()\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "def extract_description(query, text, m):\n",
    "    query = query.split()\n",
    "    positions = sorted([text.index(word) for word in query \n",
    "                        if word in text ])\n",
    "    \n",
    "    positions = [i for i in positions if i > 7]\n",
    "\n",
    "    # If word(s) appeared in text, return these sentences\n",
    "    if positions:\n",
    "        description = position_sentences(positions, text, m)\n",
    "    # If the word only occured in title, return first sentence\n",
    "    else:\n",
    "        description = ' '.join(text[0:m]) + '...'\n",
    "    \n",
    "    return description\n",
    "\n",
    "def position_sentences(positions, text, m):\n",
    "    \n",
    "    mini = positions[0]\n",
    "    maxi = mini\n",
    "    for i in positions[1:]:\n",
    "        if i > mini and i <= mini + m:\n",
    "            maxi = i\n",
    "    diff = int(math.floor((m - (maxi - mini)) / 2))\n",
    "    return '...'+' '.join(text[mini-diff:maxi+diff])+'...'\n",
    "    \n",
    "\n",
    "# res = search(query, advanced=True)\n",
    "# serp = result_page(query[1], res['hits']['hits'], \"http://kranten.kb.nl/view/article/id/\", 50, 15)\n",
    "# results = formlayout.fedit(serp, title=\"Telegraaf results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "* Manual relevance check\n",
    "* P@10\n",
    "* Change the ranking of the system + explain what is going on and why it is improving/decreasing\n",
    "\n",
    "Evaluate your results Let 2 persons assess the relevancy of the top 10 documents for 5 different queries. Compute Cohen's kappa. Determine the average precision at 10 for your system based on these 10 queries, and the two relevance assesments. Also plot the P@10 (for both judges) for each query, showing differences in hard and easy queries. Describe clearly how you solved differences in judgements. \n",
    "So, both provide the actual query, and a description of the information need that was behind the query.\n",
    "Give a small set of clear guidelines for judging the results, and let your judges follow these guidelines.\n",
    "It is far more interesting to have difficult queries (both for the search engine and for the judges) than to have queries on which all ten retrieved documents are relevant. So, try to create a good list of information needs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "def cohens_kappa(bins):\n",
    "    '''\n",
    "    Given bins made by bin_evaluations, returns cohen\\'s kappa\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    total = bins[0]+bins[1]+bins[2]+bins[3]\n",
    "    po = (bins[0]+bins[3]) / total\n",
    "    \n",
    "    marginala = ((bins[3] + bins[2]) * (bins[3] + bins[1])) / total\n",
    "    marginalb = ((bins[0] + bins[1]) * (bins[0] + bins[2])) / total\n",
    "    pe = (marginala + marginalb) / total\n",
    "    \n",
    "    cohens_kappa = (po - pe) / (1 - pe)\n",
    "    return cohens_kappa\n",
    "    \n",
    "def precision(bins, agreement_necessary):\n",
    "    '''\n",
    "    Given bins made by bin_evaluations, returns precision\n",
    "    If second argument is True, document is only seen as\n",
    "    relevant if both judges think so; otherwise one of the \n",
    "    judges is enough\n",
    "    '''\n",
    "    if agreement_necessary:\n",
    "        correct = bins[3]\n",
    "    else:\n",
    "        correct = bins[3] + bins[2] + bins[1]\n",
    "    total = bins[0]+bins[1]+bins[2]+bins[3]\n",
    "    return correct/total\n",
    "    \n",
    "    \n",
    "def bin_evaluations(evaluations):\n",
    "    '''\n",
    "    Given a list of tuples of binary values which contain\n",
    "    relevance judgements by two judges in which 1 means \n",
    "    relevant, and 0 means non-relevant, returns a list\n",
    "    in which the 4 elements represent the amount of times\n",
    "    any combination has occurred.\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    # bins are 00, 01, 10, 11 in that order\n",
    "    bins = [0,0,0,0]\n",
    "    for evaluation in evaluations:\n",
    "        b = 0\n",
    "        if evaluation[0]:\n",
    "            b += 2\n",
    "        if evaluation[1]:\n",
    "            b += 1\n",
    "        bins[b] += 1\n",
    "    return bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_search_results(number_of_queries):\n",
    "    '''\n",
    "    Asks the user to submit a query using query_database() some amount \n",
    "    of times after which two judges can give their relevancy \n",
    "    assessments. Returns evaluations in a format that can be used for \n",
    "    bin_evaluations.\n",
    "    '''\n",
    "\n",
    "    print('After the query is resolved, you will be asked to rate the documents on relevancy.')\n",
    "    print('To do this, enter the numbers of the relevant results sperated by whitespace.')\n",
    "    print('For example, if only the first and third documents were relevant, enter \\'1 3\\' without the quotes.')\n",
    "    print('If more than ten documents are returned, ignore all but the first 10.\\n')\n",
    "    evaluations = []\n",
    "    for _ in range(number_of_queries):\n",
    "        query_database()\n",
    "        print('Indicate which documents were relevant:')\n",
    "        judge1_input = raw_input('Judge 1 -->').split()\n",
    "        judge2_input = raw_input('Judge 2 -->').split()\n",
    "        for i in range(1, 11):\n",
    "            if str(i) in judge1_input:\n",
    "                assessment1 = 1\n",
    "            else:\n",
    "                assessment1 = 0\n",
    "            if str(i) in judge2_input:\n",
    "                assessment2 = 1\n",
    "            else:\n",
    "                assessment2 = 0\n",
    "            evaluations.append((assessment1, assessment2))\n",
    "    \n",
    "    bins = bin_evaluations(evaluations)\n",
    "    print('\\n The average P@10 if we require agreement for correctness: ' + str(precision(bins, 1)))\n",
    "    print(\"The average P@10 if we require a single \\'relevant\\' assessment for correctness: \" + str(precision(bins, 0)))\n",
    "    print(\"Cohen\\'s Kappa was: \" +str(cohens_kappa(bins)))\n",
    "    return evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After the query is resolved, you will be asked to rate the documents on relevancy.\n",
      "To do this, enter the numbers of the relevant results sperated by whitespace.\n",
      "For example, if only the first and third documents were relevant, enter '1 3' without the quotes.\n",
      "If more than ten documents are returned, ignore all but the first 10.\n",
      "\n",
      "Indicate which documents were relevant:\n",
      "Judge 1 -->1\n",
      "Judge 2 -->1\n",
      "Indicate which documents were relevant:\n",
      "Judge 1 -->2\n",
      "Judge 2 -->2\n",
      "\n",
      " The average P@10 if we require agreement for correctness: 0.1\n",
      "The average P@10 if we require a single 'relevant' assessment for correctness: 0.1\n",
      "Cohen's Kappa was: 1.0\n"
     ]
    }
   ],
   "source": [
    "evaluations = evaluate_search_results(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
