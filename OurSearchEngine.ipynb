{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ElasticSearch\n",
    "\n",
    "In this notebook we have created a search engine using ElasticSearch. \n",
    "\n",
    "For our own reference:\n",
    "* Literature: <https://www.elastic.co/guide/en/elasticsearch/guide/current/index.html>\n",
    "* Telegraaf XML documents: http://data.politicalmashup.nl/arjan/telegraaf/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a search engine for the telegraaf newspaper collection using eg ElasticSearch. Make facets for years and document types. Pay attention to telephone numbers (in mini advertisements). Hieronder een voorbeeld van 1 document (= 1 artikeltje).\n",
    "Je ziet dat er zelfs een link naar de bron tekst (als plaatje) instaat. De URL linked door naar de nieuwe url http://www.delpher.nl/nl/kranten/view?identifier=ddd%3A010563762%3Ampeg21%3Aa0005&coll=ddd ElasticSearch gebruikt een JSON formaat als invoer, en dit is dus triviaal om te zetten naar JSON.\n",
    "\n",
    "Each of the following points must be addressed. Create a seperate page on the wiki for each point. Make sure these pages can be found from the menu of your wiki. Explain what you did, and exemplify with links to screenshots/a working system.\n",
    "\n",
    "* Search as we know it from Google. Give a result page (SERP), with links to the documents and some description of each hit.\n",
    "* Advanced search. Let a user be able to search in several fields, also in several fields simulteanously. Queries like \"return kamervragen by Wilders about XXX with an answer about YYY in the period ZZZ\" should be possible. (For the \"Telegraaf\" collectie, let the user search in both title and tekst fields)\n",
    "* Do one of the following:\n",
    "    1. Represent the hits of a query with a wordcloud of 25-50 informative words. The wordcloud should somehow summarise what the collection has to say about the query. You may think of these words as words that you could add to the query in order to improve recall (blind relevance feedback/query expansion).\n",
    "    2. Represent each document (a kamervraag) with a word-cloud. Also make word-clouds for the question and for the answer. EXAMPLE: The html files in http://data.politicalmashup.nl/arjan/odeii/data_as_html/ contain such wordcloud summaries, which work rather well.   \n",
    "\n",
    "You can use several techniques to get rid of high frequency, but meaningless words: of course IDF, but also mutual information (see 13.5.1), or of course the technique from the paper by Kaptein et al on wordclouds.\n",
    "\n",
    "* Give next to a traditional list of results, a timeline in which you indicate how many hits there are over time.\n",
    "* Give next to the traditional list of results, a table with the number of hits for each political party. Link the party names, which should result in only selecting the hits \"ingediend\" by members of that party. (Faceted Search) (For the \"Telegraaf\" collectie, use the dc:subject element as facet values.)\n",
    "* Evaluate your results Let 2 persons assess the relevancy of the top 10 documents for 5 different queries. Compute Cohen's kappa. Determine the average precision at 10 for your system based on these 10 queries, and the two relevance assesments. Also plot the P@10 (for both judges) for each query, showing differences in hard and easy queries. Describe clearly how you solved differences in judgements. \n",
    "Create your queries in the following format:\n",
    "\n",
    "                    <topic number=\"6\"  >\n",
    "          <query>kcs</query>\n",
    "          <description>Find information on the Kansas City Southern railroad.\n",
    "          </description>\n",
    "           \n",
    "        </topic>\n",
    "\n",
    "        <topic number=\"16\"  >\n",
    "          <query>arizona game and fish</query>\n",
    "          <description>I'm looking for information about fishing and hunting\n",
    "          in Arizona.\n",
    "          </description>\n",
    "           \n",
    "        </topic>\n",
    "                \n",
    "\n",
    "So, both provide the actual query, and a description of the information need that was behind the query.\n",
    "Give a small set of clear guidelines for judging the results, and let your judges follow these guidelines.\n",
    "It is far more interesting to have difficult queries (both for the search engine and for the judges) than to have queries on which all ten retrieved documents are relevant. So, try to create a good list of information needs.\n",
    "\n",
    "* Change the ranking of your system, compute the average precision at 10 using your 10 queries, compare the results to your old system, and EXPLAIN what is going on.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Search Engine\n",
    "\n",
    "Before running ES run: \n",
    "\n",
    "    export ES_HEAP_SIZE=Half_RAM\n",
    "\n",
    "where Half_RAM is half your ram\n",
    "\n",
    "AND: \n",
    "in /config/elasticsaerch.yml add \n",
    "indices.memory.index_buffer_size: 50% \n",
    "(Still need to check if this makes a difference)\n",
    "\n",
    "To start the Elastic searh serive, please run the following code in commandline:\n",
    "\n",
    "    ./elasticsearch-2.4.1/bin/elasticsearch --node.name telegraaf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiate connection to the Elastic Search engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "\n",
    "HOST = 'http://localhost:9200/'\n",
    "es = Elasticsearch(hosts=[HOST])\n",
    "\n",
    "# If code runs, the connection is made"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator to read Telegraaf XML and add them to the ES database\n",
    "\n",
    "A generator makes it possible to immediately put the XML files/documents into the ES databse\n",
    "\n",
    "* Remove high frequency, but meaningless words\n",
    "\n",
    "You can use several techniques to get rid of high frequency, but meaningless words: of course IDF, but also mutual information (see 13.5.1), or of course the technique from the paper by Kaptein et al on wordclouds.\n",
    "\n",
    "    Possibly also create an inverted index at this point? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "from os import listdir\n",
    "from os.path import isfile\n",
    "\n",
    "def read(year):\n",
    "    '''\n",
    "    return a generator for the date, subject(type), \n",
    "    title, and text for each item in the given year. \n",
    "    '''\n",
    "    soup = BeautifulSoup(open(year,'r'),'xml')\n",
    "    for date,subject, title, text, identifier in zip(soup.find_all('date'), soup.find_all('subject'), \n",
    "                                                     soup.find_all('title'), soup.find_all('text'),\n",
    "                                                     soup.find_all('identifier')):\n",
    "            yield (date.text,subject.text,title.text,text.text,identifier.text)\n",
    "\n",
    "documents = ['./Telegraaf/'+i for i in listdir('./Telegraaf') if not isfile(i)]\n",
    "\n",
    "# Create the generator for the bulk importer\n",
    "# I'm not sure if it's a good idea to use _type here as a subject (which is artcle or advertisement, or more...)\n",
    "# The score calculation for the Elastic Search database uses whole-index statistics. \n",
    "# If you're searching a subsection this will alter the scores! WE WILL NEED TO KEEP THIS IN MIND.\n",
    "#k = ({'_type':subject, '_index':'telegraaf','_source':{'year':date[:4], 'date':date[5:], 'title':title, 'text':text}} \n",
    "#    for year in documents for (date,subject,title,text) in read(year))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate ES database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "health status index                   pri rep docs.count docs.deleted store.size pri.store.size \r\n",
      "yellow open   .marvel-es-data-1         1   1         18            2     42.6kb         42.6kb \r\n",
      "yellow open   .kibana                   1   1          2            0       19kb           19kb \r\n",
      "yellow open   telegraaf                 5   1     157546            0    335.9mb        335.9mb \r\n",
      "yellow open   .marvel-es-1-2016.10.21   1   1      12418          128      7.8mb          7.8mb \r\n"
     ]
    }
   ],
   "source": [
    "# List of all indices\n",
    "! curl 'localhost:9200/_cat/indices?v'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'acknowledged': True}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# delete any pre-excisting index\n",
    "es.indices.delete(index='telegraaf', ignore=[404,400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'acknowledged': True}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the telegraaf index in our telegraaf node\n",
    "es.indices.create(index='telegraaf', ignore=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'acknowledged': True}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# turn refresh off to speed up bulk import\n",
    "es.indices.put_settings(index='telegraaf',body={\"index\" : \n",
    "                                            {\"refresh_interval\" : \"-1\"\n",
    "                                            }\n",
    "                                       })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test with chunk size = 500 and max_chunk_bytes = 15728640 \n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'soup_documents' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-ea5c233c6ee9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Done:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mbulk_per_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoup_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'soup_documents' is not defined"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "#Import the information into the database\n",
    "# The generator can only be used once. So this code will only work once. \n",
    "print \"Test with chunk size = 500 and max_chunk_bytes = 15728640 \"\n",
    "# helper.parallel_bulk might increase the speed even more!!!\n",
    "\n",
    "def bulk_all(documents):\n",
    "    start = time.time()\n",
    "    print \"Starting time:\", start\n",
    "\n",
    "    k = ({'_type':subject, '_index':'telegraaf','_source':{'year':date[:4], \n",
    "         'date':date[5:], 'title':title, 'text':text}}\n",
    "          for doc in documents[:5] for (date,subject,title,text,identifier) in read(doc))\n",
    "    for ok in helpers.parallel_bulk(es,k, chunk_size=500,max_chunk_bytes=15728640):\n",
    "        continue\n",
    "    end_doc =time.time()\n",
    "    print \"Finished\", (end_doc-start)\n",
    "\n",
    "def bulk_per_doc(documents):\n",
    "    start = time.time()\n",
    "    print \"Starting time:\", start\n",
    "\n",
    "    for i in documents[:1]:\n",
    "        start_doc = time.time()\n",
    "        k = ({'_type':subject, '_id':identifier, '_index':'telegraaf','_source':{'year':date[:4], \n",
    "             'date':date[5:], 'title':title, 'text':text}}\n",
    "             for (date,subject,title,text,identifier) in read(i))\n",
    "        for ok in helpers.parallel_bulk(es,k,chunk_size=500,max_chunk_bytes=15728640):\n",
    "            continue\n",
    "        end_doc =time.time()\n",
    "        print \"Finished\", (end_doc-start_doc)\n",
    "        end = time.time()\n",
    "    print \"Done:\", end - start\n",
    "        \n",
    "bulk_per_doc(soup_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'acknowledged': True}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the refresh rate back to default\n",
    "es.indices.put_settings(index='telegraaf',body={\"index\" : \n",
    "                                            {\"refresh_interval\" : \"1s\"\n",
    "                                            }\n",
    "                                       })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Speed improvements/performance: \n",
    "# - bootstrap.mlockall: true in config of the file \n",
    "# (make sure  ES_HEAP_SIZE is large enough) \n",
    "# Parsing whole document xml.cElementTree.parse()\n",
    "# Streaming the xml document: xml.sax.reader.html\n",
    "\n",
    "# import xml.etree.ElementTree as etree\n",
    "# for event, elem in etree.iterparse(xmL, events=('start', 'end', 'start-ns', 'end-ns')):\n",
    "#  print event, elem\n",
    "# http://boscoh.com/programming/reading-xml-serially.html\n",
    "# Event handlers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query system \n",
    "\n",
    "* Normalise query\n",
    "* Get right tokens from the query. Use patterns to split up the query in parts? \n",
    "* Put them in the right representation for ES search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def search(query, advanced=False):\n",
    "    '''\n",
    "    Given a query it returns a SERP with rakings based on the score\n",
    "    '''\n",
    "    if advanced:\n",
    "        q = {\"query\": \n",
    "                {\"filtered\": \n",
    "                    {\"query\": {\n",
    "                        \"multi_match\": \n",
    "                            {\"query\" : query[1],\n",
    "                             \"type\" : \"cross_fields\",  # with 'and' operator \n",
    "                             \"fields\" : ['title', 'text'],\n",
    "                             \"operator\" : 'and'\n",
    "                            }\n",
    "                        },\n",
    "                     \"filter\": \n",
    "                        {\"bool\" : \n",
    "                            {\"must\" : [{\"term\": {\"year\": query[2]}},{\"term\": {\"_type\": query[0]}}]\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        res = es.search(index='telegraaf', size=10, body=q)\n",
    "        return res\n",
    "    else:\n",
    "        # filter_path can help reduce the amount of data that is returned by the es.search\n",
    "        # The query context is for how well the document fits the query\n",
    "        # The filter context is a boolean context. Does it match or not.\n",
    "        # example: Does this timestamp fall into the range 2015 to 2016?\n",
    "        #\n",
    "        \n",
    "        # The outer 'query': is necessary to show that this is the query.  \n",
    "        q = {'query':\n",
    "                {'multi_match':\n",
    "                    {'query' : query,\n",
    "                     'type' : 'cross_fields',  # with 'and' operator \n",
    "                     'fields' : ['title', 'text'],\n",
    "                     'operator' : 'and'\n",
    "                     }\n",
    "                 }\n",
    "             }\n",
    "        # In other words, all terms must be present in at least one field for a document to match.\n",
    "        res = es.search(index='telegraaf', size=10, body=q)\n",
    "        return res\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'hits': {u'hits': [{u'_score': 2.968566, u'_type': u'artikel', u'_id': u'AVfnbQZ86Aus4AtDOGsn', u'_source': {u'date': u'03-01', u'text': u't', u'year': u'1923', u'title': u'HEVIGE STORM AAN DE ENGELSCHE KUST. Een stoomschip door de hemanning verlaten.'}, u'_index': u'telegraaf'}, {u'_score': 2.0465457, u'_type': u'artikel', u'_id': u'AVfnbObB6Aus4AtDOEc0', u'_source': {u'date': u'04-20', u'text': u'IONDEX, 20 April. Aan het diner, dal werd aangeboden aan de Engsl*ch-Belgtsche Handelscommlssie, werd o\\xf6kT het woord gevoerd door lord Inch ca po, directeur ~ van TerschUlende belangrijke scbeepTaartmaatschappiJen. De spreker verklaarde, dat men er zich wel degelijk rekenschap tan geeft, dat het herstel van Belgi\\xeb, de . dulvelsche verwoesting, welke do Dutttrchers hebben aangericht, oen reuzentaak-Is. \\u2022 \"\\u2022 : . De DulUch\\xbb Invloed ln BeiglS, Inzonderheid ln Antwerpen, was v\\xf3\\xf3r den oorlog zeer belangrijk,\\'doch zal na. den oorlog ongetwijfeld niet groot zijn. Door d\\xbb activiteit der Duitven van Antwerpen verwijderd \\'gehouden: \\'Spr. gaf de verzekering, dat, wann\\xe9\\xe9r d\\xab Belgische \"ETh\\xe8\\u2014scheepvaartmaatschappijen. ~a\\xf3ge~gtSni7 alles-zouden d\\xf3\\xe9n \"o;ii~\\xe9Civ \"goede samenwerking met de Belgische maatschappijen tot stand te brengen cn mede zouden werken om de in- en uitvoer van- Kelglii oplil\\xeauwla Antwerpen to mncentreernn. . Als ds.Trede gesloten is. mogen dc Duitsche schepen echter niet uit Antwerpen worden seweerd, want dat zou gelijk \\xbbtaan wet het voortzetten- van den oorlog. Doch voortaan zullen de Engelsche en Belgische scheepvaartmaatschappijen samenwerken voor de behartiging van hun wederzijdsche belangen, zooals zll ook \\xabamen-hnn bioed hebben rergoten voor do gemeenschappelijk\\xbb zaak. (Toejuiching). \\u2022 \\u2022 - - . . Hoe lang dlo strijd nog zal duren Is ni\\xf3t te zeggen, maar-een rmiLvhc vrcd\\xbb\\'ia. indenkleven onmogelijk. Daarom zal do strijd wordwi rnortgezel tnolany i>r een\\' Rnift\\'\\xbbch schip op zee Is. Zelfs als wij uit Praukrij? zouden worden verdreven, wat niet zal gebeuren, dan nog eonden wij met ome vloten de Duitschers bestrijden tot zij bet opgeven.\\' \\' t, (Reuter). \\': - \\u2014 Dedulkbooloorlog. -\\' \\\\4 \\' I \\'\\u25a0 \\u25a0 BERLIJN, AprU. In iret versperde gebied tn de Middellandsche Zee werden door onze duikbootru 1 stoomschip.\\xabn vijf zeilschepen tot zinken ge.bracht. to zamen metende 26.600 ton, verder vernielden zU voor BUcrta de Fransche dulk- ton) cn \\xe8cn met tweo 7H tiU kanonnen bewapenden schoener, dlo ook van eenf motor en oen toestel voor draadloos\\xbb telegrafie was Drie leden der bemanning Werden gqvangengenomen. (Wolft)', u'year': u'1918', u'title': u'DE BELGISCHE EN ENGELSCHE SCHEEPVAART-MAATSCHAPPPIJEN.'}, u'_index': u'telegraaf'}, {u'_score': 0.753062, u'_type': u'advertentie', u'_id': u'AVfnbQIF6Aus4AtDOFYy', u'_source': {u'date': u'12-01', u'text': u\"I KOOPT 'S MORGENS HET IS UW VOORDEEL. j fIJNION CASTLELiJNi KONINKLIJKE ENGELSCHE POSTVAART IZUID-AFRIKA, WEST- & OOSTAFRIKA j \\u25a0_ voor Kaapkolonie. Natal, Transvaal, Rhodesia, Mozambiq jekust I Britsen Oost-Afrika, Angola, Zuid West Afrika, Mauritius via Madeira, Kanariache Eilanden, Ascensior en St. Hele 'M Vertrek van r Stoomschip Rotterdam Londen Southsmpton I GLOUCESTER CASTLE. 25 Nov. 7 DBC. \\u25a0 ; KILDONAN CASTLE . . 8 Dcc I f EOINBURGH CASTLE. . 15 Oec I BALMORAL CASTLE. . 22 Oec WALMER CASTLE ... - 29 Oec. I NORMAN 30 Dee. GOORKHA 23 Dee. 4 Jan. 1923 I Matistearner. o Via Suezkanaal. (Onvoorziene omstandigheden voorbehouden). I p flir''f*\\xab Tt \\xbbir\\xa9OT\\xbbtwnniTMemrntv\\xbbi, tor vor- I m-lit'j'lnp a;in!r\\xbbnom\\xbbQn ritior <i\\xab AcT^nt^n IDE VRIES I Co. REEREN6RACHI 448 AMSTERDAM 1 I lal ri inrrt-n omtreal paaaage word\\xb0n v. r\\xabttr- kt do fn I Oo Vries 4 Co. (Dam 6) I Trios. Cock & Son . Amsterdam Hoyman & Schuurman's Scheepsagentuur ' Kuyper, Van Dam 4 Smeer i D \\xbb.t-rH,_ Ch. Cornelder 4 Zonen's Scheepsagantuur ' koitb\\u2122\\xbb\\u2122 'aSJ Stoomvaart Maatschappij Zeeland, Vlissingen JU Eykensluyters 4 Tromp i ~ n\\u201e. ~.\\u201e. \\xbb i i i-5 \\u2022 \\xab. i s-uravennaqe A. J. Lindeman 4 Co. ' I \\xbba orders Agenten in Nederland. Ml) I\", u'year': u'1922', u'title': u''}, u'_index': u'telegraaf'}, {u'_score': 0.37239808, u'_type': u'artikel', u'_id': u'AVfnbQE96Aus4AtDOFIY', u'_source': {u'date': u'11-29', u'text': u'lSaXESlMSDSCWB 1L1VENS. l.lnmldcn. Aangek. IS Nov. Norderney, au. Ram do ; Krakatau. st.. Java. iSassiuLs. Aangek, 28 Nuv Her la v. Bilbao; Rtiaen v. Immiagham. Vertr. Stunda n. Narvik; Crenatula n. Amterdam; Valkenburg c. Newcastle, Karlsruhe n Hamburg; Western I\\'lains n. Haanburg; Channel Trader n. Huil; Outcnfels n. Hamb.; Talho Mam n. Antwerpen; Macrlesfield n. Grimsby: Jervatix Vbb.y ii. Hu!!; *7:osterdljk n. New Tork. Narnwedlrp. ITrt zee terug 28 Nov. BenanMn, at.. vermoede\\xfcjl: wegens slecht weer. miprm-AirDscHK STOOMVAARTLIJNEN. Krttatan (Nederl.) 28/11 v. Java L v. Londen te A\\'dam. r\\xbbitrla( (R. TJoyd) \\' uitrei\\xbb 28/11 v.m. 11 u. ae MaraellJa. Koepa* (11. O. A, lijn) uitreis, pass. 27/11 Perlm. TJiHwong (Jasa-Oilna-J-Tpan) 23/11 v. Shaoghae n. barna. Edam (1LAJ*) R\\'dam-Havana p. 28/11 rit. mefJML \\u25a0\\u25a0tejpa (K_SJEvli.) 28/11 v. Teliewln te Amsterdam. Gaastcxdijk (H-AJ\\xbb) 28/11 v. R\\'dam naar New Yf>rk. Heeraslitark (H.O.-Afr.-liJn) Hamburg-Port- Natal 27/11 te Port-Sald. Maan lam (U.A.L.) ICdam-New-Orleans ET/11 v. Tampico. taKM N.S M.\\xbb A\\'dam-Bordeaux p. 28/11 La Clou b re. yi B CITKN LANDtKTUK IL1VENS. BataiJer II 28/11 v.m. 8.50 v. R\\'dam te Gravesend. Drtndjc 24/11 v. Constant te Limn\\xef. TT!.-wout\\xabriljV 27/11 v. Lis Palmas te Hu\". FYieHnghitts 26 \\'11 v. R\\'dam te Hamburg. <Ilo*JcCMIcr Chiirie 27/11 v. R\\'dam te Hamburg Orootzigrn (sleepboot) R\\'dajn-Tandjong Prtok met twee sloepbooten pass. 27\\'11 Pr.rim. Maai R\\'dam-Abi) pass. 2o,ll Holtenan n.->amsdonLiccr Antwerpen 2 7/11 te Sas n its. Kijndijt IS 1! te Ramallo. Witte Zen 27/11 v. Methll n. Gent. \\' Mvl 27/11 v. Kotha n. R\\'dam. 7.wnrt\\xab Ze<-, slcepb.. 27/11 v. Corooaa te C\\'ardlff met s.a Cltta dl Palcrmo. Anwtefcunom 27/11 v. Adam te Hul!. Antoxtla SS/11 v. Montreal te Plymouth. i \\u2022 \\u2022\\u2022\\u2022!\\u2022\\u2022\\u2022\\xbb 23,11 v. Plate l:;ver te Bordeaux. Florapark Malta-Antwerpen p. 27/11 SagTe*. Lmkkos 20/11 v. Tanger te Rabat. Minu-h 2S/11 v. San Nicolaas te Avonmouth. *.uotustn\\xbb>m 27/11 v. Ilarlinseti te Huil. Oranjci>ol<Irr 27/11 v. R\\'dam te Huil Pu-tro tlorl R\\'dam-Gibraltar p. 27/11 Sagr\\'S. Sai.mra 27 \\'11 v. Nt\\xab York te Livcrpool. Scopa-t 20 11 v. Abadan te Kabang. Tenbergen Montrcal-R\\'dam p. 28/11 Lizard. yUuataw 27 11 v. A dam te Huil. Mali van Drirf Sr. 28/11 v. R\\'dam te Methil GEMENGDE BFUtlfTHTEN. Aatje. S\\'.avanger. 17 Nov. De Nederi. motorschoener Antje. is 7 Nov. met een lading haver van Norrkoping te Sandes (Stavangerfjord) aangekomen. Het schip, dat gedurende stormweer lek gesprongen is, werd 15 dezer onderzocht, waarbij bleek dat van de lading 22 ton door zeewater is beschadigd. Waarschijnlijk moet de schoener gereparoerd worden HJcmnymus. \\xefy%n.1\\' n, 28 Nov. Volgens te?\\'- gram uit Danizsr Is de aldaar thuisbehoorer.de. van Amsterdam komende motorschoener Hieronymus op Helapant gestrand en beeft de deladikng geworpen. Kr is een bergingtcontract gemaakt op den voet ,.no cure no pay\" lichters zullen worden gesonden, een berging?. boot is op de si randingsplaats aangcokmen. Kinderdijk. PanuunbttCO, 24 Nov. De Nederi. .\\xabrtoomhopper Kinderdijk heeft de hoofdzuigpijp verloren, deze is echter later gedeeltelijk geborgen, en kan hier gerepareerd wor. den. Er wordt aanbevolen hel Vaartuig voor bodemonderzoek. 1\\xbb dokken. waarvoor het noodzakelijk la, dat het naar Rio de Janeiro gaat. Lr wordt op nadere instructies gewacht.. San Antonio. Hamburg, 27 Nov De motor, schoener .San Antonio heeft te Kiel de schade geheel hersteld en de reis naar Ijl Roche voortgezet. iyedeataiy, Vltssingen. 21 Nor. -- Bel Deer.sohe stoomschip PiiiiIiiiIiiiib\". dat aan de werf der Kon. Mij. \\u201eDe Schelde\" van machine* en kerels Is voorzien, vertrok heden naar Veere. Morgen aal het schip de reis naar Hiia-aserrtam voortzetten en zal dan tevens d,- proeft\\xbbf:ht houdtie De ,.F>edenbor>r\" zal op de werf \\u201eDe Noord\\u2122, te Alblasserdam, alwaar het is gebouwd, worden opgelegd tot het voorjaar. . , \\' K<K!iima. Vliswingen. 27 Nov. Het st, Kod - ma was vermoedelijk in aanvaring met het Engelsche s*, Kosewarih dat met ingedrukte boegen naar Antwerpen opstoomde (Zie vurig avondblad Taf - en Rijm aart verkrrr to Amstnrdam. Van 18 Nor. tot en met \"4 Nov. 1922 werl -n door de schutsluizen te Zeeburg peschut: Krjnsehepen\\xbb) Oem. toniieninb. Naar de Lek . . . Ledig ... 2 IltS Geladen . . 22 11592 Totaal ... 24 i27is Naar Amsterdam . J/cdlj\\xbb ... \\u2014 Geladen . ._*\\xbb 1*144 \\'\\u25a0Gel ho?veeUi Totaal ... \\'h ***** * ij Gol. hosveelh. R5.0 i) Geladen hoeveelh. 14532 Rinnenscliepen. Naar de Lek . . . Zeilschepen 168 3S331 M.schepen. 145 11918 Totaal . . . 813 5C249 Nair Amsterdam . Zeilschepen 244 448i\\'8 fat.schepen. 158 1 3236 Totaal . . . 402 53064 hlocpboolen. Nasr de Lek 41 670 . Amsterdam ....... 4)0 \\xab42 \". lot*\\xbb*. ... 81 1712 (*) Ouder Rijnschepen worden in deze opra\\\\o vetAtaan: Rijnaken, alsmede schepen, Voorzien van een Rijnpatent ca met een lading komende van of bu:.;a:nd naar den Rijn.', u'year': u'1922', u'title': u'SCHEEPSTIJDINGEN.'}, u'_index': u'telegraaf'}], u'total': 4, u'max_score': 2.968566}, u'_shards': {u'successful': 5, u'failed': 0, u'total': 5}, u'took': 1273, u'timed_out': False}\n"
     ]
    }
   ],
   "source": [
    "search('stoomschip engelsche')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result page function\n",
    "\n",
    "* Take query output and use score to order result on a Search Engine Result Page (SERP).\n",
    "* Return title, link, and description of each hit\n",
    "\n",
    "-> The description can be a word cloud of 20-25 most informative words. Represent the hits of a query with a wordcloud of 25-50 informative words. The wordcloud should somehow summarise what the collection has to say about the query. You may think of these words as words that you could add to the query in order to improve recall (blind relevance feedback/query expansion). \n",
    "\n",
    "\n",
    "Additions\n",
    "* A timeline with the amount of hits over time\n",
    "* A table with the number of hits for each political party. Link the party names, which should result in only selecting the hits \"ingediend\" by members of that party. (Faceted Search) (For the \"Telegraaf\" collectie, use the dc:subject element as facet values.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "artikel\n",
      "advertentie\n",
      "familiebericht\n",
      "illustratie met onderschrift\n"
     ]
    }
   ],
   "source": [
    "res = es.indices.get_mapping(index='telegraaf')\n",
    "for i in res['telegraaf']['mappings']:\n",
    "    print i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Search\n",
    "\n",
    "The query system will have to be changed to implement this\n",
    "\n",
    "* Make multiple fields searchable:\n",
    "    * Title \n",
    "    * Tekst\n",
    "    * Year?\n",
    "    \n",
    "Let a user be able to search in several fields, also in several fields simulteanously. Queries like \"return kamervragen by Wilders about XXX with an answer about YYY in the period ZZZ\" should be possible. (For the \"Telegraaf\" collectie, let the user search in both title and tekst fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unique_years = list(set( document.date.get_text()[:4]\n",
    "                    for document in soup_documents ))\n",
    "\n",
    "unique_doc_types = list(set( subject.get_text()\n",
    "                       for document in soup_documents\n",
    "                       for subject in document.find_all('subject')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, u'mandaten', 1]\n",
      "[u'artikel', u'mandaten', u'1922']\n"
     ]
    }
   ],
   "source": [
    "from formlayout import fedit, FormDialog\n",
    "\n",
    "query = fedit([('Document type',[0]+unique_doc_types),\n",
    "               ('Zoektermen',''),\n",
    "               ('Jaar publicatie',[0]+unique_years)], \n",
    "               title=\"Telegraaf zoekmachine\", \n",
    "               comment=\"Wat voor krantenartikel zoek je?\")\n",
    "\n",
    "print query\n",
    "\n",
    "query[0] = unique_doc_types[query[0]]\n",
    "query[2] = unique_years[query[2]]\n",
    "\n",
    "print query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res = search(query, advanced=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2.9305606,\n",
       "  'http://kranten.kb.nl/view/article/id/ddd:010563557:mpeg21:p001:a0002',\n",
       "  u'Sovjet-Rusland en de mandaten over Syri\\xeb, Palestina en Mesopotami\\xeb.'),\n",
       " (1.3677711,\n",
       "  'http://kranten.kb.nl/view/article/id/ddd:010563560:mpeg21:p006:a0159',\n",
       "  u'De Landdag van Kowno.'),\n",
       " (0.9130335,\n",
       "  'http://kranten.kb.nl/view/article/id/ddd:010563575:mpeg21:p002:a0037',\n",
       "  u'De kwestie der nieuwe verkiezingen in Oostenrijk.'),\n",
       " (0.33850607,\n",
       "  'http://kranten.kb.nl/view/article/id/ddd:010563579:mpeg21:p006:a0144',\n",
       "  u'HET INT. VREDESCONGRES. ONDERWIJS ALS PROPAGANDA. SAMENWERKING BEPLEIT.')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def result_page(hits, url):\n",
    "    serp = [(hit['_score'], url+str(hit['_id']), hit['_source']['title']) \n",
    "            for hit in hits]\n",
    "    return serp\n",
    "\n",
    "result_page(res['hits']['hits'], \"http://kranten.kb.nl/view/article/id/\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, 'b'), (8, 'a')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([(8,'a'),(4,'b')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result page function\n",
    "\n",
    "* Take query output and use score to order result on a Search Engine Result Page (SERP).\n",
    "* Return title, link, and description of each hit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "* Manual relevance check\n",
    "* P@10\n",
    "* Change the ranking of the system + explain what is going on and why it is improving/decreasing\n",
    "\n",
    "Evaluate your results Let 2 persons assess the relevancy of the top 10 documents for 5 different queries. Compute Cohen's kappa. Determine the average precision at 10 for your system based on these 10 queries, and the two relevance assesments. Also plot the P@10 (for both judges) for each query, showing differences in hard and easy queries. Describe clearly how you solved differences in judgements. \n",
    "So, both provide the actual query, and a description of the information need that was behind the query.\n",
    "Give a small set of clear guidelines for judging the results, and let your judges follow these guidelines.\n",
    "It is far more interesting to have difficult queries (both for the search engine and for the judges) than to have queries on which all ten retrieved documents are relevant. So, try to create a good list of information needs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [NLTK]",
   "language": "python",
   "name": "Python [NLTK]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
