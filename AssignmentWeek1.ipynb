{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment  week 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook made by   (If not filled in correctly: 0 pts for assignment)\n",
    "\n",
    "__Name(s)__: Adriaan de Vries, Verna Dankers\n",
    "\n",
    "__Student id(s)__ : 10795227, 10761225\n",
    "\n",
    "__Email(s)__: adriaan.devries@student.uva.nl, verna.dankers@student.uva.nl\n",
    "\n",
    "### Pledge (taken from [Coursera's Honor Code](https://www.coursera.org/about/terms/honorcode) )\n",
    "\n",
    "\n",
    "\n",
    "Put here a selfie with your photo where you hold a signed paper with the following text: (if this is team work, put two selfies here). The link must be to some place on the web, not to a local file. **Assignments without the selfies will not be graded and receive 0 points.**\n",
    "\n",
    "> My answers to homework, quizzes and exams will be my own work (except for assignments that explicitly permit collaboration).\n",
    "\n",
    ">I will not make solutions to homework, quizzes or exams available to anyone else. This includes both solutions written by me, as well as any official solutions provided by the course staff.\n",
    "\n",
    ">I will not engage in any other activities that will dishonestly improve my results or dishonestly improve/hurt the results of others.\n",
    "\n",
    "<img src='link to your selfie'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Book Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 1.1\n",
    "\n",
    "<table width=50%>\n",
    "<tr> <td>forecasts</td><td>1</td> </tr>\n",
    "<tr> <td>home</td>     <td>1,2,3,4</td> </tr>\n",
    "<tr> <td>new</td>      <td>1,4</td> </tr>\n",
    "<tr> <td>sales</td>    <td>1,2,3,4</td> </tr>\n",
    "<tr> <td>top</td>      <td>1</td> </tr>\n",
    "<tr> <td>rise</td>     <td>2,4</td> </tr>\n",
    "<tr> <td>in</td>       <td>2,3</td> </tr>\n",
    "<tr> <td>july</td>     <td>2,3,4</td> </tr>\n",
    "<tr> <td>increase</td> <td>3</td> </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 1.2\n",
    "\n",
    "#### A\n",
    "\n",
    "<table width=50%>\n",
    "<tr>  <td></td>             <th>Doc 1</th> <th>Doc 2</th> <th>Doc3</th> <th>Doc 4</th>   </tr>\n",
    "<tr>  <td>breakthrough</td> <td>1</td> <td>0</td> <td>0</td> <td>0</td>   </tr>\n",
    "<tr>  <td>drug</td>         <td>1</td> <td>1</td> <td>0</td> <td>0</td>   </tr>\n",
    "<tr>  <td>for</td>          <td>1</td> <td>0</td> <td>1</td> <td>1</td>   </tr>\n",
    "<tr>  <td>schizophrenia</td><td>1</td> <td>1</td> <td>1</td> <td>1</td>   </tr>\n",
    "<tr>  <td>new</td>          <td>0</td> <td>1</td> <td>1</td> <td>1</td>   </tr>\n",
    "<tr>  <td>approach</td>     <td>0</td> <td>0</td> <td>1</td> <td>0</td>   </tr>\n",
    "<tr>  <td>treatment</td>    <td>0</td> <td>0</td> <td>1</td> <td>0</td>   </tr>\n",
    "<tr>  <td>of</td>           <td>0</td> <td>0</td> <td>1</td> <td>0</td>   </tr>\n",
    "<tr>  <td>hopes</td>        <td>0</td> <td>0</td> <td>0</td> <td>1</td>   </tr>\n",
    "<tr>  <td>patients</td>     <td>0</td> <td>0</td> <td>0</td> <td>1</td>   </tr>\n",
    "</table>\n",
    "\n",
    "#### B\n",
    "\n",
    "<table width=50%>\n",
    "<tr> <td>breakthrough</td> <td>1</td> </tr>\n",
    "<tr> <td>drug</td>         <td>1,2</td> </tr>\n",
    "<tr> <td>for</td>          <td>1,3,4</td> </tr>\n",
    "<tr> <td>schizophrenia</td><td>1,2,3,4</td> </tr>\n",
    "<tr> <td>new</td>          <td>2,3,4</td> </tr>\n",
    "<tr> <td>approach</td>     <td>3</td> </tr>\n",
    "<tr> <td>treatment</td>    <td>3</td> </tr>\n",
    "<tr> <td>of</td>           <td>3</td> </tr>\n",
    "<tr> <td>hopes</td>        <td>4</td> </tr>\n",
    "<tr> <td>patients</td>     <td>4</td> </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3\n",
    "\n",
    "#### A\n",
    "\n",
    "schizophrenia AND drug returns documents 1 and 2.\n",
    "\n",
    "#### B\n",
    "\n",
    "for AND NOT(drug OR approach) returns document 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4\n",
    "\n",
    "#### A\n",
    "\n",
    "By looping through caesar and removing from brutus the ids that appear in caesar the algorithm remains in O(x+y)\n",
    "\n",
    "#### B\n",
    "\n",
    "In one way or another we need to work with a list of size N - y where N is the number of documents, so it's not necessarily in O(x+y). We are still in O(N)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5\n",
    "\n",
    "A merge can always be completed in linear time, specifically O(N). Extending this to a full query, we can complete the algorithm in O(N*T) where T is the amout of terms in the query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6\n",
    "\n",
    "#### A\n",
    "\n",
    "(Brutus OR Caesar) AND NOT (Antony OR Cleopatra)\n",
    "\n",
    "(Brutus OR Caesar) AND (NOT Antony AND NOT Cleopatra)\n",
    "\n",
    "((NOT Antony AND NOT Cleopatra) AND Brutus) OR ((NOT Antony AND NOT Cleopatra) AND Caesar)\n",
    "\n",
    "(NOT Antony AND NOT Cleopatra AND Brutus) OR (NOT Antony AND NOT Cleopatra AND Caesar)\n",
    "\n",
    "\n",
    "#### B\n",
    "\n",
    "In disjunctive normal form we can keep the intermediate result small because we start by evaluating ANDs. For example, Brutus AND NOT Cleopatra would be smaller than Brutus, which would become even smaller after intersecting with NOT Antony. After doing the same thing on the other side, we only have to apply OR at the very end to two relatively small lists. In its original form, we start off with Brutus OR Caesar which is already larger than Brutus.\n",
    "\n",
    "On the other hand, in the normal form, more operations are necessary.\n",
    "\n",
    "Thus, which form can be evaluated faster depends on the exact contents, although it's likely disjunctive normal form is faster.\n",
    "\n",
    "#### C\n",
    "\n",
    "Conjunctive normal form will almost always have longer queries with more operations on shorter lists. It will be faster when the lists get much smaller, and slower when the lists stay roughly the same size. For example, if Brutus and Caesar have for a large part the same documents in which they appear, Brutus OR Caesar will be not much larger than Caesar, so the original query (1.6.a is taken as an example here) could be faster.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7\n",
    "\n",
    "We want to start with the smallest set after conjunction. The sum of the postings sizes of eyes and kaleidoscope is smaller than the other sums, and there should also be some overlap between kaleidoscope and eyes (documents that include kaleidoscope will often include eyes as well). So, we start with kaleidoscope OR eyes. Trees and tangerines have some overlap, and their postings size sum is smaller than the sum of the sizes of skies and marmelade, so tree OR tangerine should be the second smallest. Thus, we evaluate in this order: ((eyes OR kaleidoscope) AND (trees OR tangerine)) AND (skies OR marmelade)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8\n",
    "\n",
    "If the frequency of countrymen is very high, to the point where most documents include the term, we want to use it early in our evaluation to shrink the size of the intermediate result. If the frequency is low, we can shrink the list more by starting with friends AND romans.\n",
    "\n",
    "In general, the higher the frequency of a negated term in a conjunction, the earlier we want to evaluate it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.9\n",
    "\n",
    "It isn't guaranteed to be optimal. Here's a silly example where it doesn't work:\n",
    "\n",
    "Doc1: one\n",
    "\n",
    "Doc2: one\n",
    "\n",
    "Doc3: one\n",
    "\n",
    "Doc4: two three\n",
    "\n",
    "Doc5: two three four\n",
    "\n",
    "query: one AND two AND three AND four\n",
    "\n",
    "Going by size first evaluates four AND three which leaves Doc5, it then adds two which still leaves Doc5, and only when AND one is evaluated is it clear that no documents match the query. If we'd started with one AND four, we'd seen that immediately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.10\n",
    "\n",
    "UNITE(p1, p2)\n",
    "1  answer ← []\n",
    "2  while p1 =\\= NIL and p2 =\\= NIL\n",
    "3  do if docID(p1) = docID(p2)\n",
    "4    then ADD(answer, docID(p1))\n",
    "5      p1 ← next(p1)\n",
    "6      p2 ← next(p2)\n",
    "7    else if docID(p1) < docID(p2)\n",
    "8      then \n",
    "9        ADD(answer, docID(p1))\n",
    "10       p1 ← next(p1)\n",
    "11     else \n",
    "12       ADD(answer, docID(p2))\n",
    "13       p2 ← next(p2)\n",
    "14  return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.11\n",
    "\n",
    "In this case, p2 is the postings list of y in the query 'x AND NOT y'. p1 is as usual the postings list of x.\n",
    "\n",
    "INTERSECT_WITH_NEGATION(p1, p2)\n",
    "1  answer ← []\n",
    "2  while p1 =\\= NIL and p2 =\\= NIL\n",
    "3  do if docID(p1) = docID(p2)\n",
    "4    then\n",
    "5      p1 ← next(p1)\n",
    "6      p2 ← next(p2)\n",
    "7    else if docID(p1) < docID(p2)\n",
    "8      then \n",
    "9        ADD(answer, docID(p1))\n",
    "10       p1 ← next(p1)\n",
    "11     else \n",
    "12       p2 ← next(p2)\n",
    "13  return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.12\n",
    "\n",
    "(professor teacher lecturer) /s explain!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.13\n",
    "\n",
    "|                     \t| Google      \t| Yahoo      \t| Bing       \t|\n",
    "|---------------------\t|-------------\t|------------\t|------------\t|\n",
    "| burglar             \t| 36.400.000  \t| 8,240,000  \t| 8.270.000  \t|\n",
    "| burglar AND burglar \t| 22.900.000  \t| 11,800,000 \t| 9.980.000  \t|\n",
    "| burglar OR burglar  \t| 36.400.000  \t| 8,280,000  \t| 8.280.000  \t|\n",
    "| knight              \t| 359.000.000 \t| 15,500,000 \t| 16.200.000 \t|\n",
    "| conquer             \t| 66.900.000  \t| 12,600,000 \t| 12.700.000 \t|\n",
    "| knight OR conquer   \t| 394.000.000 \t| 16,200,000 \t| 16.900.000 \t|\n",
    "\n",
    "Of course, burglar AND burglar should give the same hits as burglar, which should give the same answer as burglar OR burglar. None of the engines are able to succesfully do that. They all do fairly well with OR, but they're off by a lot for AND. Google has fewer hits, and both Yahoo and Bing have more hits when using AND. Perhaps they use some heuristic for deciding which documents to even consider in the first place, which does not stay the same when using AND and OR.\n",
    "\n",
    "knight OR conquer should return an amount of hits that is higher than or equal to the highest number of hits from either knight or conquer, and lower than or equal to the sum of the number of hits of knight and conquer. All three search engines satisfy this bound."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming exercises\n",
    "\n",
    "* [MakeInvertedIndex](MakeInvertedIndex1516.html): vragen 1-7 | Best done in iPython notebook\n",
    "\n",
    "## Preparing the data\n",
    "\n",
    "### Step 1: Get a corpus\n",
    "The Skakespeare corpus used here was downloaded from http://xml.coverpages.org/bosakShakespeare200.html.\n",
    "\n",
    "### Step 2: Remove XML tags\n",
    "In the code shown below the XML are removed by using the BeautifulSoup module.\n",
    "\n",
    "### Step 3: Tokenize\n",
    "The data is whitespace normalized, therefore we can tokenize the data simply by applying a WhitespaceTokenizer from the nltk library.\n",
    "\n",
    "### Step 4: Normalize\n",
    "The data is normalized by changing capital letters into lowercase letters, by deleting non alphabetical characters and empty lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "CORPUS = \"Shakespeare\"\n",
    "PROCESSED_CORPUS = CORPUS + \"New\"\n",
    "\n",
    "def prepare_data(folder):\n",
    "    # Create a directory for the processed files\n",
    "    if not os.path.exists(PROCESSED_CORPUS):\n",
    "        os.makedirs(PROCESSED_CORPUS)\n",
    "\n",
    "    # Prepare each file one by one\n",
    "    for filename in os.listdir(CORPUS):\n",
    "        if not os.path.exists(os.path.join(PROCESSED_CORPUS, filename[:-4] + \".pkl\")):\n",
    "            # Strip xml tags\n",
    "            xmlfile = BeautifulSoup(open(os.path.join(CORPUS,filename), 'r'),\"xml\")\n",
    "            textfile = xmlfile.get_text()\n",
    "\n",
    "            # Split the text on whitespaces\n",
    "            beautifile = WhitespaceTokenizer().tokenize(textfile)\n",
    "\n",
    "            # Normalize text for capital letters, non-alphabetical tokens and whitespace\n",
    "            finalfile = [ re.sub(r'[^\\w\\s]', '', word.lower())\n",
    "                          for word \n",
    "                          in beautifile \n",
    "                         ]\n",
    "            new_filename = os.path.join(PROCESSED_CORPUS, filename[:-4] + \".pkl\")\n",
    "            savefile = open(new_filename, 'wb')\n",
    "            pickle.dump(finalfile, savefile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the Inverted Index\n",
    "\n",
    "### Step 1: Counting tokens\n",
    "\n",
    "### Step 2: Inverting the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def index_collection(folder):\n",
    "    \"\"\"\n",
    "    Create an inverted index given a folder \n",
    "    containig a text corpus.\n",
    "    \"\"\"\n",
    "    index= defaultdict(Counter) # initialize MyIndex\n",
    "    for filename in os.listdir(PROCESSED_CORPUS):\n",
    "        text = pickle.load(open(os.path.join(PROCESSED_CORPUS,filename),'rb'))\n",
    "        \n",
    "        # Update the index with each token\n",
    "        for w in text:    \n",
    "            index[w][filename[:-4]]+=1   \n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "index = index_collection(PROCESSED_CORPUS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'two_gent': 159, 'r_and_j': 135, 'as_you': 112, 'dream': 102, 'lll': 100, 'much_ado': 90, 'othello': 77, 't_night': 74, 'troilus': 67, 'hamlet': 67, 'rich_iii': 64, 'taming': 63, 'all_well': 60, 'merchant': 59, 'lear': 51, 'm_wives': 46, 'hen_vi_3': 41, 'hen_v': 40, 'john': 39, 'a_and_c': 38, 'timon': 34, 'j_caesar': 34, 'rich_ii': 32, 'm_for_m': 29, 'cymbelin': 28, 'coriolan': 28, 'hen_iv_1': 28, 'pericles': 26, 'hen_vi_1': 25, 'win_tale': 24, 'hen_viii': 23, 'titus': 23, 'hen_iv_2': 21, 'macbeth': 19, 'com_err': 18, 'hen_vi_2': 17, 'tempest': 12})"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index['love']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "#### 1. Count the total number of tokens in the works of Shakespeare and the total number of unique tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 27868\n",
      "Total number of tokens: 887663\n"
     ]
    }
   ],
   "source": [
    "# total number of unique tokens\n",
    "print(\"Number of unique tokens: \" + str(len(index)))\n",
    "\n",
    "# total number of tokens\n",
    "total = 0\n",
    "\n",
    "for word in index:\n",
    "    for work in index[word]:\n",
    "        total += index[word][work]\n",
    "        \n",
    "print(\"Total number of tokens: \" + str(total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Look at the vocabulary. Is it OK? Propose further normalization, or better tokenization. Also see Chapter 2. \n",
    "\n",
    "#### 3. Make a new index where you lower case all words except if each letter in a word is a capital. So 'Caesar' becomes 'caesar', but CAESAR remains the same.\n",
    "\n",
    "#### 4. Calculate the document frequency (in how many documents occurs the term) for each term and the corpus frequency (how often does the term occur in the corpus) for each term. Add these two values as new values in your inverted index. You can use several data-structures for this: a \"JSON like dict\", or a \"CSV-like\" triple. Choose what you like best. \n",
    "\n",
    "#### 5. Create a script which produces a table with the following information: \n",
    "size of the vocabulary\n",
    "nr of terms with corpus frequency 1\n",
    "nr of terms with document frequency 1\n",
    "nr of terms with document frequency  equal to the number of documents in the corpus\n",
    "nr of terms with document frequency  half or more than the number of documents in the corpus\n",
    "\n",
    "#### 6. Zipf law Look up what Zipf's law means in Wikipedia or in the book. Now show that it holds or not for our corpus. Do that as follows: (instead of a csv file you can of course use a Python datastructure and use Python's pyplot to do the plotting.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
