{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment  week 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook made by   (If not filled in correctly: 0 pts for assignment)\n",
    "\n",
    "__Name(s)__: Adriaan de Vries, Verna Dankers\n",
    "\n",
    "__Student id(s)__ : 10795227, 10761225\n",
    "\n",
    "__Email(s)__: adriaan.devries@student.uva.nl, verna.dankers@student.uva.nl\n",
    "\n",
    "### Pledge (taken from [Coursera's Honor Code](https://www.coursera.org/about/terms/honorcode) )\n",
    "\n",
    "\n",
    "\n",
    "Put here a selfie with your photo where you hold a signed paper with the following text: (if this is team work, put two selfies here). The link must be to some place on the web, not to a local file. **Assignments without the selfies will not be graded and receive 0 points.**\n",
    "\n",
    "> My answers to homework, quizzes and exams will be my own work (except for assignments that explicitly permit collaboration).\n",
    "\n",
    ">I will not make solutions to homework, quizzes or exams available to anyone else. This includes both solutions written by me, as well as any official solutions provided by the course staff.\n",
    "\n",
    ">I will not engage in any other activities that will dishonestly improve my results or dishonestly improve/hurt the results of others.\n",
    "\n",
    "<img src='link to your selfie'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Book Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 1.1\n",
    "\n",
    "<table width=50%>\n",
    "<tr> <td>forecasts</td><td>1</td> </tr>\n",
    "<tr> <td>home</td>     <td>1,2,3,4</td> </tr>\n",
    "<tr> <td>new</td>      <td>1,4</td> </tr>\n",
    "<tr> <td>sales</td>    <td>1,2,3,4</td> </tr>\n",
    "<tr> <td>top</td>      <td>1</td> </tr>\n",
    "<tr> <td>rise</td>     <td>2,4</td> </tr>\n",
    "<tr> <td>in</td>       <td>2,3</td> </tr>\n",
    "<tr> <td>july</td>     <td>2,3,4</td> </tr>\n",
    "<tr> <td>increase</td> <td>3</td> </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 1.2\n",
    "\n",
    "#### A\n",
    "\n",
    "<table width=50%>\n",
    "<tr>  <td></td>             <th>Doc 1</th> <th>Doc 2</th> <th>Doc3</th> <th>Doc 4</th>   </tr>\n",
    "<tr>  <td>breakthrough</td> <td>1</td> <td>0</td> <td>0</td> <td>0</td>   </tr>\n",
    "<tr>  <td>drug</td>         <td>1</td> <td>1</td> <td>0</td> <td>0</td>   </tr>\n",
    "<tr>  <td>for</td>          <td>1</td> <td>0</td> <td>1</td> <td>1</td>   </tr>\n",
    "<tr>  <td>schizophrenia</td><td>1</td> <td>1</td> <td>1</td> <td>1</td>   </tr>\n",
    "<tr>  <td>new</td>          <td>0</td> <td>1</td> <td>1</td> <td>1</td>   </tr>\n",
    "<tr>  <td>approach</td>     <td>0</td> <td>0</td> <td>1</td> <td>0</td>   </tr>\n",
    "<tr>  <td>treatment</td>    <td>0</td> <td>0</td> <td>1</td> <td>0</td>   </tr>\n",
    "<tr>  <td>of</td>           <td>0</td> <td>0</td> <td>1</td> <td>0</td>   </tr>\n",
    "<tr>  <td>hopes</td>        <td>0</td> <td>0</td> <td>0</td> <td>1</td>   </tr>\n",
    "<tr>  <td>patients</td>     <td>0</td> <td>0</td> <td>0</td> <td>1</td>   </tr>\n",
    "</table>\n",
    "\n",
    "#### B\n",
    "\n",
    "<table width=50%>\n",
    "<tr> <td>breakthrough</td> <td>1</td> </tr>\n",
    "<tr> <td>drug</td>         <td>1,2</td> </tr>\n",
    "<tr> <td>for</td>          <td>1,3,4</td> </tr>\n",
    "<tr> <td>schizophrenia</td><td>1,2,3,4</td> </tr>\n",
    "<tr> <td>new</td>          <td>2,3,4</td> </tr>\n",
    "<tr> <td>approach</td>     <td>3</td> </tr>\n",
    "<tr> <td>treatment</td>    <td>3</td> </tr>\n",
    "<tr> <td>of</td>           <td>3</td> </tr>\n",
    "<tr> <td>hopes</td>        <td>4</td> </tr>\n",
    "<tr> <td>patients</td>     <td>4</td> </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3\n",
    "\n",
    "#### A\n",
    "\n",
    "schizophrenia AND drug returns documents 1 and 2.\n",
    "\n",
    "#### B\n",
    "\n",
    "for AND NOT(drug OR approach) returns document 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4\n",
    "\n",
    "#### A\n",
    "\n",
    "By looping through caesar and removing from brutus the ids that appear in caesar the algorithm remains in O(x+y)\n",
    "\n",
    "#### B\n",
    "\n",
    "In one way or another we need to work with a list of size N - y where N is the number of documents, so it's not necessarily in O(x+y). We are still in O(N)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5\n",
    "\n",
    "A merge can always be completed in linear time, specifically O(N). Extending this to a full query, we can complete the algorithm in O(N*T) where T is the amount of terms in the query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6\n",
    "\n",
    "#### A\n",
    "\n",
    "(Brutus OR Caesar) AND NOT (Antony OR Cleopatra)\n",
    "\n",
    "(Brutus OR Caesar) AND (NOT Antony AND NOT Cleopatra)\n",
    "\n",
    "((NOT Antony AND NOT Cleopatra) AND Brutus) OR ((NOT Antony AND NOT Cleopatra) AND Caesar)\n",
    "\n",
    "(NOT Antony AND NOT Cleopatra AND Brutus) OR (NOT Antony AND NOT Cleopatra AND Caesar)\n",
    "\n",
    "\n",
    "#### B\n",
    "\n",
    "In disjunctive normal form we can keep the intermediate result small because we start by evaluating ANDs. For example, Brutus AND NOT Cleopatra would be smaller than Brutus, which would become even smaller after intersecting with NOT Antony. After doing the same thing on the other side, we only have to apply OR at the very end to two relatively small lists. In its original form, we start off with Brutus OR Caesar which is already larger than Brutus.\n",
    "\n",
    "On the other hand, in the normal form, more operations are necessary.\n",
    "\n",
    "Thus, which form can be evaluated faster depends on the exact contents, although it's likely disjunctive normal form is faster.\n",
    "\n",
    "#### C\n",
    "\n",
    "Conjunctive normal form will almost always have longer queries with more operations on shorter lists. It will be faster when the lists get much smaller, and slower when the lists stay roughly the same size. For example, if Brutus and Caesar have for a large part the same documents in which they appear, Brutus OR Caesar will be not much larger than Caesar, so the original query (1.6.a is taken as an example here) could be faster.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7\n",
    "\n",
    "We want to start with the smallest set after conjunction. The sum of the postings sizes of eyes and kaleidoscope is smaller than the other sums, and there should also be some overlap between kaleidoscope and eyes (documents that include kaleidoscope will often include eyes as well). So, we start with kaleidoscope OR eyes. Trees and tangerines have some overlap, and their postings size sum is smaller than the sum of the sizes of skies and marmelade, so tree OR tangerine should be the second smallest. Thus, we evaluate in this order: ((eyes OR kaleidoscope) AND (trees OR tangerine)) AND (skies OR marmelade)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8\n",
    "\n",
    "If the frequency of countrymen is very high, to the point where most documents include the term, we want to use it early in our evaluation to shrink the size of the intermediate result. If the frequency is low, we can shrink the list more by starting with friends AND romans.\n",
    "\n",
    "In general, the higher the frequency of a negated term in a conjunction, the earlier we want to evaluate it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.9\n",
    "\n",
    "It isn't guaranteed to be optimal. Here's a silly example where it doesn't work:\n",
    "\n",
    "Doc1: one\n",
    "\n",
    "Doc2: one\n",
    "\n",
    "Doc3: one\n",
    "\n",
    "Doc4: two three\n",
    "\n",
    "Doc5: two three four\n",
    "\n",
    "query: one AND two AND three AND four\n",
    "\n",
    "Going by size first evaluates four AND three which leaves Doc5, it then adds two which still leaves Doc5, and only when AND one is evaluated is it clear that no documents match the query. If we'd started with one AND four, we'd seen that immediately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.10\n",
    "\n",
    "UNITE(p1, p2)\n",
    "1  answer ← []\n",
    "2  while p1 =\\= NIL and p2 =\\= NIL\n",
    "3  do if docID(p1) = docID(p2)\n",
    "4    then ADD(answer, docID(p1))\n",
    "5      p1 ← next(p1)\n",
    "6      p2 ← next(p2)\n",
    "7    else if docID(p1) < docID(p2)\n",
    "8      then \n",
    "9        ADD(answer, docID(p1))\n",
    "10       p1 ← next(p1)\n",
    "11     else \n",
    "12       ADD(answer, docID(p2))\n",
    "13       p2 ← next(p2)\n",
    "14  return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.11\n",
    "\n",
    "In this case, p2 is the postings list of y in the query 'x AND NOT y'. p1 is as usual the postings list of x.\n",
    "\n",
    "INTERSECT_WITH_NEGATION(p1, p2)\n",
    "1  answer ← []\n",
    "2  while p1 =\\= NIL and p2 =\\= NIL\n",
    "3  do if docID(p1) = docID(p2)\n",
    "4    then\n",
    "5      p1 ← next(p1)\n",
    "6      p2 ← next(p2)\n",
    "7    else if docID(p1) < docID(p2)\n",
    "8      then \n",
    "9        ADD(answer, docID(p1))\n",
    "10       p1 ← next(p1)\n",
    "11     else \n",
    "12       p2 ← next(p2)\n",
    "13  return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.12\n",
    "\n",
    "(professor teacher lecturer) /s explain!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.13\n",
    "\n",
    "|                     \t| Google      \t| Yahoo      \t| Bing       \t|\n",
    "|---------------------\t|-------------\t|------------\t|------------\t|\n",
    "| burglar             \t| 36.400.000  \t| 8,240,000  \t| 8.270.000  \t|\n",
    "| burglar AND burglar \t| 22.900.000  \t| 11,800,000 \t| 9.980.000  \t|\n",
    "| burglar OR burglar  \t| 36.400.000  \t| 8,280,000  \t| 8.280.000  \t|\n",
    "| knight              \t| 359.000.000 \t| 15,500,000 \t| 16.200.000 \t|\n",
    "| conquer             \t| 66.900.000  \t| 12,600,000 \t| 12.700.000 \t|\n",
    "| knight OR conquer   \t| 394.000.000 \t| 16,200,000 \t| 16.900.000 \t|\n",
    "\n",
    "Of course, burglar AND burglar should give the same hits as burglar, which should give the same answer as burglar OR burglar. None of the engines are able to succesfully do that. They all do fairly well with OR, but they're off by a lot for AND. Google has fewer hits, and both Yahoo and Bing have more hits when using AND. Perhaps they use some heuristic for deciding which documents to even consider in the first place, which does not stay the same when using AND and OR.\n",
    "\n",
    "knight OR conquer should return an amount of hits that is higher than or equal to the highest number of hits from either knight or conquer, and lower than or equal to the sum of the number of hits of knight and conquer. All three search engines satisfy this bound."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming exercises\n",
    "\n",
    "* [MakeInvertedIndex](MakeInvertedIndex1516.html): vragen 1-7 | Best done in iPython notebook\n",
    "\n",
    "## Preparing the data\n",
    "\n",
    "### Step 1: Get a corpus\n",
    "The Skakespeare corpus used here was downloaded from http://xml.coverpages.org/bosakShakespeare200.html.\n",
    "\n",
    "### Step 2: Remove XML tags\n",
    "In the code shown below the XML are removed by using the BeautifulSoup module.\n",
    "\n",
    "### Step 3: Tokenize\n",
    "The data is whitespace normalized, therefore we can tokenize the data simply by applying a WhitespaceTokenizer from the nltk library.\n",
    "\n",
    "### Step 4: Normalize\n",
    "The data is normalized by changing capital letters into lowercase letters, by deleting non alphabetical characters and empty lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from collections import Counter, defaultdict\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "CORPUS = \"Shakespeare\"\n",
    "\n",
    "def normalize(word, keep_capitals):\n",
    "    \"\"\"\n",
    "    Normalize capital letters and punctuation\n",
    "    \"\"\"\n",
    "    if (keep_capitals and not word == word.upper()) or not keep_capitals:\n",
    "        word = word.lower()\n",
    "    word = re.sub(r'[^\\w\\s]', '', word)\n",
    "    return word\n",
    "\n",
    "def prepare_data(folder, new_folder, keep_capitals, byte):\n",
    "    \"\"\"\n",
    "    Prepare a text corpus for the process of\n",
    "    making an inverted index.\n",
    "    \"\"\"\n",
    "    # Create a directory for the processed files\n",
    "    if not os.path.exists(new_folder):\n",
    "        os.makedirs(new_folder)\n",
    "\n",
    "    # Prepare each file one by one\n",
    "    for filename in os.listdir(folder):\n",
    "        if not os.path.exists(os.path.join(new_folder, filename[:-4] + \".pkl\")):\n",
    "            # Strip xml tags\n",
    "            if byte:\n",
    "                xmlfile = BeautifulSoup(open(os.path.join(folder, filename), 'rb'),\"xml\")\n",
    "            else:\n",
    "                xmlfile = BeautifulSoup(open(os.path.join(folder, filename), 'r'),\"xml\")\n",
    "            textfile = xmlfile.get_text()\n",
    "\n",
    "            # Split the text on whitespaces\n",
    "            beautifile = WhitespaceTokenizer().tokenize(textfile)\n",
    "\n",
    "            # Normalize text for capital letters, non-alphabetical tokens and whitespace\n",
    "            finalfile = [ normalize(word, keep_capitals)\n",
    "                          for word \n",
    "                          in beautifile \n",
    "                         ]\n",
    "            new_filename = os.path.join(new_folder, filename[:-4] + \".pkl\")\n",
    "            savefile = open(new_filename, 'wb')\n",
    "            pickle.dump(finalfile, savefile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the Inverted Index\n",
    "\n",
    "### Step 1: Counting tokens\n",
    "\n",
    "### Step 2: Inverting the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def index_collection(folder):\n",
    "    \"\"\"\n",
    "    Create an inverted index given a folder \n",
    "    containig a text corpus.\n",
    "    \"\"\"\n",
    "    index= defaultdict(Counter) # initialize MyIndex\n",
    "    for filename in os.listdir(folder):\n",
    "        text = pickle.load(open(os.path.join(folder,filename),'rb'))\n",
    "        \n",
    "        # Update the index with each token\n",
    "        for w in text:    \n",
    "            index[w][filename[:-4]]+=1   \n",
    "    if '' in index.keys():\n",
    "        del index['']\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prepare_data(CORPUS, CORPUS + \"New\", 0, 0)\n",
    "index_org = index_collection(CORPUS + \"New\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "#### 1. Count the total number of tokens in the works of Shakespeare and the total number of unique tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 27867\n",
      "Total number of tokens: 887609\n"
     ]
    }
   ],
   "source": [
    "# total number of unique tokens\n",
    "print(\"Number of unique tokens: \" + str(len(index_org)))\n",
    "\n",
    "# total number of tokens\n",
    "total = 0\n",
    "\n",
    "for word in index_org:\n",
    "    for work in index_org[word]:\n",
    "        total += index_org[word][work]\n",
    "        \n",
    "print(\"Total number of tokens: \" + str(total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Look at the vocabulary. Is it OK? \n",
    "The first 100 words are shown below. At first glance one notices that there are nouns in plural form (like scarfs) and verbs in more than one tense (like wrenching and commanded). That is OK, but not ideal, since a hit for scarfs is a hit for scarf too. Ideally, we could apply some lemmatization.\n",
    "\n",
    "#### Propose further normalization, or better tokenization. Also see Chapter 2. \n",
    "Normalization for Shakespeare's work\n",
    "* Remove so called stop words from the vocabulary. Words like 'the' and 'in' are not very descriptive when you search for some work of Shakespeare or a web page.\n",
    "* We could use lists of synonyms to improve the recall for queries. For example 'plane' and 'airbus' are synonyms. Returning the results for 'airbus' when someone searches for 'plane', can improve the recall.\n",
    "* Applying lemmatization or stemming the words. This can be done roughly or more precise. Some (very rough) rules can be removing '-ing' and '-ed' from words.\n",
    "\n",
    "More ideas for normalization in general\n",
    "* We can correct for spelling mistakes by removing diacritics. Then 'cafe' would return the same hits as 'café'.\n",
    "* But also the differences between words like 'colour' and 'color' can be removed by mapping these to the same results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ariels', 'oppressing', 'smoothpates', 'noddle', 'overjoyd', 'disgraceful', 'taint', 'robbed', 'outright', 'robber', 'amending', 'niobes', 'grossly', 'courageous', 'enfeebles', 'stillclosing', 'mallows', 'bareheaded', 'dulzura', 'certes', 'image', 'uncontemnd', 'paradoxes', 'enfeebled', 'masses', 'zanies', 'reeking', 'familiars', 'buzzed', 'blesses', 'sundaycitizens', 'brainpan', 'traitor', 'dwellingbriefly', 'polecat', 'blessed', 'experimental', 'advancing', 'fellowtribune', 'syllables', 'irrevocable', 'ate', 'unmitigated', 'profits', 'customers', 'halt', 'rends', 'hovelpost', 'railer', 'hall', 'contends', 'depths', 'half', 'hale', 'abandon', 'withstand', 'hideously', 'dromio', 'earthlier', 'dares', 'blister', 'facility', 'lena', 'boult', 'pompeius', 'dared', 'nobleness', 'souls', 'replenished', 'agent', 'ashy', 'sould', 'gavest', 'fallen', 'tongues', 'marred', 'cocks', 'engaged', 'praetors', 'dears', 'cockscomb', 'youoften', 'procreant', 'commoner', 'unction', 'graceful', 'ravishing', 'meditation', 'thirdly', 'parties', 'festivity', 'adventuring', 'chastity', 'cruelty', 'furnival', 'merriment', 'trovato', 'horsemen', 'overflow', 'dulcet']\n"
     ]
    }
   ],
   "source": [
    "vocabulary = list(index_org.keys())\n",
    "print(vocabulary[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Make a new index where you lower case all words except if each letter in a word is a capital. So 'Caesar' becomes 'caesar', but CAESAR remains the same.\n",
    "\n",
    "We adjusted the normalize function. You can give the function a 1 to keep the words like CAESAR, and a 0 to turn them into lower case words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prepare_data(CORPUS, CORPUS + \"Capitals\", 1, 0)\n",
    "index_capitals = index_collection(CORPUS + \"Capitals\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['oppressing', 'smoothpates', 'noddle', 'overjoyd', 'disgraceful', 'taint', 'robbed', 'outright', 'robber', 'amending', 'niobes', 'grossly', 'courageous', 'enfeebles', 'stillclosing', 'mallows', 'bareheaded', 'dulzura', 'certes', 'image', 'uncontemnd', 'paradoxes', 'enfeebled', 'masses', 'zanies', 'reeking', 'familiars', 'buzzed', 'blesses', 'sundaycitizens', 'brainpan', 'traitor', 'dwellingbriefly', 'polecat', 'blessed', 'experimental', 'advancing', 'fellowtribune', 'syllables', 'irrevocable', 'ate', 'unmitigated', 'profits', 'customers', 'halt', 'rends', 'hovelpost', 'railer', 'hall', 'contends', 'depths', 'half', 'hale', 'abandon', 'withstand', 'hideously', 'dromio', 'earthlier', 'dares', 'blister', 'facility', 'lena', 'boult', 'pompeius', 'dared', 'nobleness', 'souls', 'replenished', 'agent', 'ashy', 'sould', 'gavest', 'fallen', 'tongues', 'marred', 'cocks', 'engaged', 'praetors', 'dears', 'cockscomb', 'youoften', 'procreant', 'commoner', 'unction', 'graceful', 'ravishing', 'meditation', 'thirdly', 'parties', 'festivity', 'adventuring', 'chastity', 'cruelty', 'furnival', 'merriment', 'SELEUCUS', 'creates', 'trovato', 'horsemen', 'overflow']\n"
     ]
    }
   ],
   "source": [
    "vocabulary = list(index_capitals.keys())\n",
    "print(vocabulary[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Calculate the document frequency (in how many documents occurs the term) for each term and the corpus frequency (how often does the term occur in the corpus) for each term. Add these two values as new values in your inverted index. You can use several data-structures for this: a \"JSON like dict\", or a \"CSV-like\" triple. Choose what you like best. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_doc_freq(index, word):\n",
    "    \"\"\"\n",
    "    Calculate in how many documents a given \n",
    "    term occurs, based on the inverted index.\n",
    "    \"\"\"\n",
    "    return len(index[word])\n",
    "    \n",
    "def calculate_corp_freq(index, word):\n",
    "    \"\"\"\n",
    "    Calculate how often a term occurs in a \n",
    "    corpus, based on the inverted index.\n",
    "    \"\"\"\n",
    "    corpus_frequency = 0\n",
    "    \n",
    "    for document in index[word]:\n",
    "        corpus_frequency += index[word][document]\n",
    "        \n",
    "    return corpus_frequency\n",
    "\n",
    "def add_frequencies(index):\n",
    "    for word in index:\n",
    "        original_counter = index[word]\n",
    "        document_frequency = calculate_doc_freq(index, word)\n",
    "        corpus_frequency = calculate_corp_freq(index, word)\n",
    "        index[word] = {\"counter\" : original_counter, \n",
    "                       \"doc_freq\" : document_frequency, \n",
    "                       \"corp_freq\" : corpus_frequency\n",
    "                      }\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "index_with_freq = add_frequencies(index_org)\n",
    "index_capitals_with_freq = add_frequencies(index_capitals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Create a script which produces a table with the following information: \n",
    "\n",
    "| size of the vocabulary                                                                      |\n",
    "|---------------------------------------------------------------------------------------------|\n",
    "| nr of terms with corpus frequency 1                                                         |\n",
    "| nr of terms with document frequency 1                                                       |\n",
    "| nr of terms with document frequency equal to the number of documents in the corpus          |\n",
    "| nr of terms with document frequency half or more than the number of documents in the corpus |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_table(index):\n",
    "    \"\"\"\n",
    "    Create a small table with information about\n",
    "    a given inverted index.\n",
    "    \"\"\"\n",
    "    # append the size\n",
    "    size = len(index)\n",
    "    \n",
    "    table = [size, 0,0,0,0]\n",
    "    \n",
    "    for word in index:\n",
    "        corp = index[word][\"corp_freq\"]\n",
    "        doc = index[word][\"doc_freq\"]\n",
    "        if corp == 1:\n",
    "            table[1] += 1\n",
    "        if doc == 1:\n",
    "            table[2] += 1\n",
    "        if doc == size:\n",
    "            table[3] += 1\n",
    "        if doc >= (0.5 * size):\n",
    "            table[4] += 1\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[27867, 12235, 13614, 0, 0]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_table(index_org)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Zipf law Look up what Zipf's law means in Wikipedia or in the book. Now show that it holds or not for our corpus. Do that as follows: (instead of a csv file you can of course use a Python datastructure and use Python's pyplot to do the plotting.) \n",
    "\n",
    "Wikipedia: Zipf's law states that given some corpus of natural language utterances, the frequency of any word is inversely proportional to its rank in the frequency table.\n",
    "\n",
    "Book: Zipf's law states that, if $t_1$ is the most common term in the collection, $t_2$ is the next most common,  and so on,  then the collection frequency $cf_i$ of the $i$th most common term is proportional to $\\frac{1}{i}$: \n",
    "\n",
    "$$cf_i \\propto \\frac{1}{i}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot  as pyplot\n",
    "\n",
    "def gather_zipf_data(index):\n",
    "    \"\"\"\n",
    "    Create two lists from an inverted index\n",
    "    that can be used to illustrate Zipf's law \n",
    "    in a plot\n",
    "    \"\"\"\n",
    "    corp_frequencies = []\n",
    "    row_numbers = []\n",
    "\n",
    "    for word in index:\n",
    "        corp_frequencies.append(index[word][\"corp_freq\"])\n",
    "\n",
    "    corp_frequencies.sort(reverse=True)\n",
    "\n",
    "    for i in range(len(corp_frequencies)):\n",
    "        row_numbers.append(i + 1)\n",
    "        \n",
    "    return(corp_frequencies, row_numbers)\n",
    "        \n",
    "def create_log_plot(x, y, xlabel, ylabel):\n",
    "    \"\"\"\n",
    "    Create a logarithmic plot with the data given\n",
    "    \"\"\"\n",
    "    fig = pyplot.figure()\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "\n",
    "    # Create a line with the data calculated above\n",
    "    line, = ax.plot(row_numbers, corp_frequencies, color='red', lw=2)\n",
    "\n",
    "    # Name axes\n",
    "    pyplot.ylabel(ylabel)\n",
    "    pyplot.xlabel(xlabel)\n",
    "\n",
    "    # Log axes\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_yscale('log')\n",
    "    \n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corp_frequencies, row_numbers = gather_zipf_data(index_org)\n",
    "\n",
    "create_log_plot(corp_frequencies, row_numbers, \"Row number\",\"Corpus frequencies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting picture does look like the images shown on Wikipedia. Though the line is not as steep as it is in a perfect example, it approximates the theory pretty well!\n",
    "\n",
    "<img width = 50% src='http://oi63.tinypic.com/qoj9ci.jpg'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 7. Now with moties from politicalmashup\n",
    "\n",
    "Data taken from http://data.politicalmashup.nl/moties/PoliDocs_Moties.zip, with an uncompressed size of 0.103 GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "POLITICAL_CORPUS = \"Moties/XML/MOT\" # the folder with the data\n",
    "\n",
    "start = time.time()\n",
    "prepare_data(POLITICAL_CORPUS, POLITICAL_CORPUS + \"New\", 0, 1)\n",
    "end = time.time()\n",
    "processing_time = end - start\n",
    "\n",
    "print('Time it took to process the files (in seconds): ', processing_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time it took to create the index (in seconds):  10.549068212509155\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "index_large = index_collection(POLITICAL_CORPUS + \"New\")\n",
    "end = time.time()\n",
    "indexing_time = end-start\n",
    "\n",
    "print('Time it took to create the index (in seconds): ', indexing_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After processing, the files are only good for a total of NEW_TOTAL, that's a decrease of DECREASE.\n",
    "\n",
    "Now, let's look at how many unique tokens we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 111341\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of unique tokens: \" + str(len(index_org.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 111341\n",
      "Total number of tokens: 5394753\n"
     ]
    }
   ],
   "source": [
    "# total number of unique tokens\n",
    "print(\"Number of unique tokens: \" + str(len(index_org)))\n",
    "\n",
    "# total number of tokens\n",
    "total = 0\n",
    "\n",
    "for word in index_org:\n",
    "    for work in index_org[word]:\n",
    "        total += index_org[word][work]\n",
    "        \n",
    "print(\"Total number of tokens: \" + str(total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.3.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
